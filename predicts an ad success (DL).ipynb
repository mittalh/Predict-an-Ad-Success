{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of Batch size on Training loss and Accuracy\n",
    "# I have tried the Batch Sizes as 1,32 and 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Data Set and Preprocessing it\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "os.chdir(r'D:\\HackerEarth Comp-2\\Dataset')\n",
    "train_df=pd.read_csv(\"train.csv\")\n",
    "train_df.drop(['id'],axis=1,inplace=True)\n",
    "df=train_df.drop(['netgain'],axis=1)\n",
    "y=train_df[['netgain']]\n",
    "# Data Preprocessing\n",
    "#\n",
    "def labelencoding(feature):             # Function to label the Categorical data\n",
    "    l=len(df[feature].value_counts())\n",
    "    name=df[feature].unique()\n",
    "    for i in range(l):\n",
    "        df[feature].replace({name[i]:i},inplace=True)\n",
    "df_relationship_status=pd.get_dummies(df['realtionship_status'])\n",
    "df_relationship_status.drop(['Widowed'],axis=1,inplace=True)\n",
    "\n",
    "df_industry=pd.get_dummies(df['industry'])\n",
    "df_industry.drop(['Political'],axis=1,inplace=True)\n",
    "\n",
    "df_genre=pd.get_dummies(df['genre'])\n",
    "df_genre.drop(['Other'],axis=1,inplace=True)\n",
    "\n",
    "df_targeted_sex=pd.get_dummies(df['targeted_sex'])\n",
    "df_targeted_sex.drop(['Male'],axis=1,inplace=True)\n",
    "\n",
    "df_airtime=pd.get_dummies(df['airtime'])\n",
    "df_airtime.drop(['Primetime'],axis=1,inplace=True)\n",
    "\n",
    "df_airlocation=pd.get_dummies(df['airlocation'])\n",
    "df_airlocation.drop(['Yugoslavia'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "df_money_back_guarantee\t=pd.get_dummies(df['money_back_guarantee'])\n",
    "df_money_back_guarantee.drop(['Yes'],axis=1,inplace=True)\n",
    "\n",
    "X=pd.concat([df,df_airlocation,df_airtime,df_genre,df_industry,df_money_back_guarantee,df_relationship_status,df_targeted_sex],axis=1)\n",
    "X.drop(['realtionship_status','industry','genre','targeted_sex','airtime','airlocation','money_back_guarantee'],axis=1,inplace=True)\n",
    "X['expensive']=X['expensive'].replace({\"Low\":0,\"Medium\":1,\"High\":2})\n",
    "# Scaling the 'average_runtime(minutes_per_week)', 'ratings'\n",
    "train=X\n",
    "train['ratings']=(train['ratings']-train['ratings'].min())/(train['ratings'].max()-train['ratings'].min())\n",
    "train['average_runtime(minutes_per_week)']=(train['average_runtime(minutes_per_week)']-train['average_runtime(minutes_per_week)'].min())/(train['average_runtime(minutes_per_week)'].max()-train['average_runtime(minutes_per_week)'].min())\n",
    "\n",
    "y=y.replace({False:0,True:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_runtime(minutes_per_week)</th>\n",
       "      <th>ratings</th>\n",
       "      <th>expensive</th>\n",
       "      <th>Cambodia</th>\n",
       "      <th>Canada</th>\n",
       "      <th>China</th>\n",
       "      <th>Columbia</th>\n",
       "      <th>Cuba</th>\n",
       "      <th>Dominican-Republic</th>\n",
       "      <th>Ecuador</th>\n",
       "      <th>...</th>\n",
       "      <th>Other</th>\n",
       "      <th>Pharma</th>\n",
       "      <th>No</th>\n",
       "      <th>Divorced</th>\n",
       "      <th>Married-AF-spouse</th>\n",
       "      <th>Married-civ-spouse</th>\n",
       "      <th>Married-spouse-absent</th>\n",
       "      <th>Never-married</th>\n",
       "      <th>Separated</th>\n",
       "      <th>Female</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.44898</td>\n",
       "      <td>0.027465</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.44898</td>\n",
       "      <td>0.027465</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   average_runtime(minutes_per_week)   ratings  expensive  Cambodia  Canada  \\\n",
       "0                            0.44898  0.027465          2         0       0   \n",
       "1                            0.44898  0.027465          0         0       0   \n",
       "\n",
       "   China  Columbia  Cuba  Dominican-Republic  Ecuador  ...  Other  Pharma  No  \\\n",
       "0      0         0     0                   0        0  ...      0       0   1   \n",
       "1      0         0     0                   0        0  ...      0       1   1   \n",
       "\n",
       "   Divorced  Married-AF-spouse  Married-civ-spouse  Married-spouse-absent  \\\n",
       "0         0                  0                   0                      1   \n",
       "1         0                  0                   1                      0   \n",
       "\n",
       "   Never-married  Separated  Female  \n",
       "0              0          0       0  \n",
       "1              0          0       0  \n",
       "\n",
       "[2 rows x 63 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Data set after data Preprocessing\n",
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>netgain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   netgain\n",
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_shape: (25787, 63)\n",
      "y_train_shape: (25787, 1)\n",
      "X_test_shape: (261, 63)\n",
      "y_test_shape: (261, 1)\n"
     ]
    }
   ],
   "source": [
    "# Lets split our  data set in test and Train\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(train,y,test_size=0.01,random_state=12,stratify=y)\n",
    "print(\"X_train_shape: \"+str(X_train.shape))\n",
    "print(\"y_train_shape: \"+str(y_train.shape))\n",
    "print(\"X_test_shape: \"+str(X_test.shape))\n",
    "print(\"y_test_shape: \"+str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\HackerEarth Comp-2\\Dataset\n"
     ]
    }
   ],
   "source": [
    "# Current Directory\n",
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 23208 samples, validate on 2579 samples\n",
      "Epoch 1/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.4343 - accuracy: 0.7688\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.78519, saving model to weights-improvement-01-0.79.hdf5\n",
      "23208/23208 [==============================] - 3s 144us/sample - loss: 0.4341 - accuracy: 0.7689 - val_loss: 0.3967 - val_accuracy: 0.7852\n",
      "Epoch 2/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.4012 - accuracy: 0.7922\n",
      "Epoch 00002: val_accuracy improved from 0.78519 to 0.80729, saving model to weights-improvement-02-0.81.hdf5\n",
      "23208/23208 [==============================] - 3s 133us/sample - loss: 0.4009 - accuracy: 0.7924 - val_loss: 0.3768 - val_accuracy: 0.8073\n",
      "Epoch 3/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.3858 - accuracy: 0.7997\n",
      "Epoch 00003: val_accuracy did not improve from 0.80729\n",
      "23208/23208 [==============================] - 3s 131us/sample - loss: 0.3859 - accuracy: 0.7998 - val_loss: 0.3775 - val_accuracy: 0.8046\n",
      "Epoch 4/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3790 - accuracy: 0.8018\n",
      "Epoch 00004: val_accuracy did not improve from 0.80729\n",
      "23208/23208 [==============================] - 3s 132us/sample - loss: 0.3794 - accuracy: 0.8017 - val_loss: 0.3751 - val_accuracy: 0.8054\n",
      "Epoch 5/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.3763 - accuracy: 0.8029\n",
      "Epoch 00005: val_accuracy did not improve from 0.80729\n",
      "23208/23208 [==============================] - 3s 137us/sample - loss: 0.3767 - accuracy: 0.8025 - val_loss: 0.3739 - val_accuracy: 0.8034\n",
      "Epoch 6/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3735 - accuracy: 0.8057\n",
      "Epoch 00006: val_accuracy did not improve from 0.80729\n",
      "23208/23208 [==============================] - 3s 144us/sample - loss: 0.3735 - accuracy: 0.8056 - val_loss: 0.3747 - val_accuracy: 0.8050\n",
      "Epoch 7/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.3706 - accuracy: 0.8060\n",
      "Epoch 00007: val_accuracy did not improve from 0.80729\n",
      "23208/23208 [==============================] - 3s 139us/sample - loss: 0.3706 - accuracy: 0.8060 - val_loss: 0.3823 - val_accuracy: 0.8026\n",
      "Epoch 8/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.3708 - accuracy: 0.8044\n",
      "Epoch 00008: val_accuracy did not improve from 0.80729\n",
      "23208/23208 [==============================] - 3s 130us/sample - loss: 0.3706 - accuracy: 0.8045 - val_loss: 0.3760 - val_accuracy: 0.8061\n",
      "Epoch 9/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.3678 - accuracy: 0.8087\n",
      "Epoch 00009: val_accuracy did not improve from 0.80729\n",
      "23208/23208 [==============================] - 3s 125us/sample - loss: 0.3678 - accuracy: 0.8088 - val_loss: 0.3751 - val_accuracy: 0.7980\n",
      "Epoch 10/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.3651 - accuracy: 0.8082\n",
      "Epoch 00010: val_accuracy did not improve from 0.80729\n",
      "23208/23208 [==============================] - 3s 119us/sample - loss: 0.3655 - accuracy: 0.8083 - val_loss: 0.3900 - val_accuracy: 0.8065\n",
      "Epoch 11/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.3643 - accuracy: 0.8086\n",
      "Epoch 00011: val_accuracy did not improve from 0.80729\n",
      "23208/23208 [==============================] - 3s 118us/sample - loss: 0.3642 - accuracy: 0.8085 - val_loss: 0.3869 - val_accuracy: 0.8019\n",
      "Epoch 12/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3637 - accuracy: 0.8073\n",
      "Epoch 00012: val_accuracy did not improve from 0.80729\n",
      "23208/23208 [==============================] - 3s 112us/sample - loss: 0.3636 - accuracy: 0.8073 - val_loss: 0.3823 - val_accuracy: 0.8061\n",
      "Epoch 13/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.3610 - accuracy: 0.8088\n",
      "Epoch 00013: val_accuracy did not improve from 0.80729\n",
      "23208/23208 [==============================] - 3s 113us/sample - loss: 0.3614 - accuracy: 0.8088 - val_loss: 0.3799 - val_accuracy: 0.8054\n",
      "Epoch 14/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.3605 - accuracy: 0.8095\n",
      "Epoch 00014: val_accuracy did not improve from 0.80729\n",
      "23208/23208 [==============================] - 3s 112us/sample - loss: 0.3610 - accuracy: 0.8090 - val_loss: 0.3868 - val_accuracy: 0.7968\n",
      "Epoch 15/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.3589 - accuracy: 0.8101\n",
      "Epoch 00015: val_accuracy did not improve from 0.80729\n",
      "23208/23208 [==============================] - 3s 122us/sample - loss: 0.3594 - accuracy: 0.8102 - val_loss: 0.3888 - val_accuracy: 0.8046\n",
      "Epoch 16/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.3572 - accuracy: 0.8094\n",
      "Epoch 00016: val_accuracy improved from 0.80729 to 0.81039, saving model to weights-improvement-16-0.81.hdf5\n",
      "23208/23208 [==============================] - 3s 148us/sample - loss: 0.3569 - accuracy: 0.8096 - val_loss: 0.3834 - val_accuracy: 0.8104\n",
      "Epoch 17/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.3576 - accuracy: 0.8116\n",
      "Epoch 00017: val_accuracy did not improve from 0.81039\n",
      "23208/23208 [==============================] - 3s 124us/sample - loss: 0.3566 - accuracy: 0.8119 - val_loss: 0.3955 - val_accuracy: 0.8057\n",
      "Epoch 18/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.3563 - accuracy: 0.8111\n",
      "Epoch 00018: val_accuracy improved from 0.81039 to 0.81155, saving model to weights-improvement-18-0.81.hdf5\n",
      "23208/23208 [==============================] - 3s 113us/sample - loss: 0.3561 - accuracy: 0.8112 - val_loss: 0.3923 - val_accuracy: 0.8116\n",
      "Epoch 19/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.3532 - accuracy: 0.8128\n",
      "Epoch 00019: val_accuracy improved from 0.81155 to 0.81233, saving model to weights-improvement-19-0.81.hdf5\n",
      "23208/23208 [==============================] - 3s 114us/sample - loss: 0.3533 - accuracy: 0.8127 - val_loss: 0.3834 - val_accuracy: 0.8123\n",
      "Epoch 20/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.3527 - accuracy: 0.8150\n",
      "Epoch 00020: val_accuracy improved from 0.81233 to 0.81543, saving model to weights-improvement-20-0.82.hdf5\n",
      "23208/23208 [==============================] - 3s 115us/sample - loss: 0.3538 - accuracy: 0.8143 - val_loss: 0.3896 - val_accuracy: 0.8154\n",
      "Epoch 21/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3521 - accuracy: 0.8137\n",
      "Epoch 00021: val_accuracy improved from 0.81543 to 0.82319, saving model to weights-improvement-21-0.82.hdf5\n",
      "23208/23208 [==============================] - 3s 109us/sample - loss: 0.3522 - accuracy: 0.8138 - val_loss: 0.3881 - val_accuracy: 0.8232\n",
      "Epoch 22/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.3499 - accuracy: 0.8145\n",
      "Epoch 00022: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 108us/sample - loss: 0.3502 - accuracy: 0.8143 - val_loss: 0.3964 - val_accuracy: 0.8181\n",
      "Epoch 23/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.3498 - accuracy: 0.8164\n",
      "Epoch 00023: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.3498 - accuracy: 0.8162 - val_loss: 0.3883 - val_accuracy: 0.8197\n",
      "Epoch 24/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.3489 - accuracy: 0.8166\n",
      "Epoch 00024: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 112us/sample - loss: 0.3487 - accuracy: 0.8164 - val_loss: 0.3939 - val_accuracy: 0.8112\n",
      "Epoch 25/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.3475 - accuracy: 0.8170\n",
      "Epoch 00025: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 112us/sample - loss: 0.3478 - accuracy: 0.8167 - val_loss: 0.3912 - val_accuracy: 0.8201\n",
      "Epoch 26/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.3475 - accuracy: 0.8164\n",
      "Epoch 00026: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 113us/sample - loss: 0.3472 - accuracy: 0.8168 - val_loss: 0.3984 - val_accuracy: 0.8178\n",
      "Epoch 27/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3460 - accuracy: 0.8185\n",
      "Epoch 00027: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 113us/sample - loss: 0.3463 - accuracy: 0.8185 - val_loss: 0.3913 - val_accuracy: 0.8116\n",
      "Epoch 28/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.3437 - accuracy: 0.8206\n",
      "Epoch 00028: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 109us/sample - loss: 0.3442 - accuracy: 0.8205 - val_loss: 0.3979 - val_accuracy: 0.8143\n",
      "Epoch 29/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.3449 - accuracy: 0.8187\n",
      "Epoch 00029: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 107us/sample - loss: 0.3449 - accuracy: 0.8186 - val_loss: 0.4100 - val_accuracy: 0.8147\n",
      "Epoch 30/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.3441 - accuracy: 0.8183\n",
      "Epoch 00030: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 116us/sample - loss: 0.3442 - accuracy: 0.8183 - val_loss: 0.4029 - val_accuracy: 0.8189\n",
      "Epoch 31/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.3432 - accuracy: 0.8206\n",
      "Epoch 00031: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 127us/sample - loss: 0.3430 - accuracy: 0.8208 - val_loss: 0.4032 - val_accuracy: 0.8185\n",
      "Epoch 32/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.3419 - accuracy: 0.8194\n",
      "Epoch 00032: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 131us/sample - loss: 0.3421 - accuracy: 0.8193 - val_loss: 0.4231 - val_accuracy: 0.8108\n",
      "Epoch 33/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.3414 - accuracy: 0.8206\n",
      "Epoch 00033: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 130us/sample - loss: 0.3411 - accuracy: 0.8208 - val_loss: 0.4177 - val_accuracy: 0.8166\n",
      "Epoch 34/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.3410 - accuracy: 0.8192\n",
      "Epoch 00034: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 123us/sample - loss: 0.3411 - accuracy: 0.8191 - val_loss: 0.4338 - val_accuracy: 0.8185\n",
      "Epoch 35/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.3399 - accuracy: 0.8213\n",
      "Epoch 00035: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 125us/sample - loss: 0.3400 - accuracy: 0.8213 - val_loss: 0.4210 - val_accuracy: 0.8046\n",
      "Epoch 36/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.3388 - accuracy: 0.8204\n",
      "Epoch 00036: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 116us/sample - loss: 0.3392 - accuracy: 0.8203 - val_loss: 0.4195 - val_accuracy: 0.8189\n",
      "Epoch 37/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.3393 - accuracy: 0.8214\n",
      "Epoch 00037: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 113us/sample - loss: 0.3391 - accuracy: 0.8217 - val_loss: 0.4172 - val_accuracy: 0.8166\n",
      "Epoch 38/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3387 - accuracy: 0.8217\n",
      "Epoch 00038: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 110us/sample - loss: 0.3389 - accuracy: 0.8217 - val_loss: 0.4023 - val_accuracy: 0.8174\n",
      "Epoch 39/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.3385 - accuracy: 0.8222\n",
      "Epoch 00039: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 111us/sample - loss: 0.3385 - accuracy: 0.8222 - val_loss: 0.4055 - val_accuracy: 0.8181\n",
      "Epoch 40/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.3367 - accuracy: 0.8236\n",
      "Epoch 00040: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 111us/sample - loss: 0.3375 - accuracy: 0.8234 - val_loss: 0.4187 - val_accuracy: 0.8073\n",
      "Epoch 41/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.3377 - accuracy: 0.8236\n",
      "Epoch 00041: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 112us/sample - loss: 0.3378 - accuracy: 0.8235 - val_loss: 0.4180 - val_accuracy: 0.8162\n",
      "Epoch 42/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.3357 - accuracy: 0.8252\n",
      "Epoch 00042: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 113us/sample - loss: 0.3357 - accuracy: 0.8251 - val_loss: 0.4216 - val_accuracy: 0.8209\n",
      "Epoch 43/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.3341 - accuracy: 0.8236\n",
      "Epoch 00043: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 119us/sample - loss: 0.3347 - accuracy: 0.8232 - val_loss: 0.4358 - val_accuracy: 0.8174\n",
      "Epoch 44/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.3339 - accuracy: 0.8238\n",
      "Epoch 00044: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 117us/sample - loss: 0.3337 - accuracy: 0.8239 - val_loss: 0.4281 - val_accuracy: 0.8189\n",
      "Epoch 45/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.3362 - accuracy: 0.8219\n",
      "Epoch 00045: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 117us/sample - loss: 0.3361 - accuracy: 0.8219 - val_loss: 0.4285 - val_accuracy: 0.8170\n",
      "Epoch 46/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.3355 - accuracy: 0.8249\n",
      "Epoch 00046: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 114us/sample - loss: 0.3356 - accuracy: 0.8246 - val_loss: 0.4380 - val_accuracy: 0.8189\n",
      "Epoch 47/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.3340 - accuracy: 0.8252\n",
      "Epoch 00047: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 112us/sample - loss: 0.3341 - accuracy: 0.8250 - val_loss: 0.4345 - val_accuracy: 0.8057\n",
      "Epoch 48/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.3336 - accuracy: 0.8253\n",
      "Epoch 00048: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 110us/sample - loss: 0.3336 - accuracy: 0.8253 - val_loss: 0.4336 - val_accuracy: 0.8185\n",
      "Epoch 49/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3336 - accuracy: 0.8240\n",
      "Epoch 00049: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 112us/sample - loss: 0.3339 - accuracy: 0.8239 - val_loss: 0.4268 - val_accuracy: 0.8096\n",
      "Epoch 50/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.3323 - accuracy: 0.8258\n",
      "Epoch 00050: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 111us/sample - loss: 0.3321 - accuracy: 0.8261 - val_loss: 0.4436 - val_accuracy: 0.8224\n",
      "Epoch 51/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.3340 - accuracy: 0.8238\n",
      "Epoch 00051: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 114us/sample - loss: 0.3331 - accuracy: 0.8242 - val_loss: 0.4326 - val_accuracy: 0.8193\n",
      "Epoch 52/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.3320 - accuracy: 0.8248\n",
      "Epoch 00052: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 115us/sample - loss: 0.3321 - accuracy: 0.8248 - val_loss: 0.4395 - val_accuracy: 0.8181\n",
      "Epoch 53/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.3310 - accuracy: 0.8257\n",
      "Epoch 00053: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 119us/sample - loss: 0.3309 - accuracy: 0.8256 - val_loss: 0.4381 - val_accuracy: 0.8189\n",
      "Epoch 54/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.3324 - accuracy: 0.8248\n",
      "Epoch 00054: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 119us/sample - loss: 0.3329 - accuracy: 0.8247 - val_loss: 0.4347 - val_accuracy: 0.8166\n",
      "Epoch 55/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.3286 - accuracy: 0.8261\n",
      "Epoch 00055: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 113us/sample - loss: 0.3290 - accuracy: 0.8261 - val_loss: 0.4517 - val_accuracy: 0.8205\n",
      "Epoch 56/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3298 - accuracy: 0.8261\n",
      "Epoch 00056: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 112us/sample - loss: 0.3299 - accuracy: 0.8262 - val_loss: 0.4440 - val_accuracy: 0.8209\n",
      "Epoch 57/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.3291 - accuracy: 0.8277\n",
      "Epoch 00057: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 131us/sample - loss: 0.3290 - accuracy: 0.8275 - val_loss: 0.4459 - val_accuracy: 0.8170\n",
      "Epoch 58/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3297 - accuracy: 0.8251\n",
      "Epoch 00058: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 132us/sample - loss: 0.3296 - accuracy: 0.8252 - val_loss: 0.4537 - val_accuracy: 0.8166\n",
      "Epoch 59/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.3289 - accuracy: 0.8258\n",
      "Epoch 00059: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 126us/sample - loss: 0.3289 - accuracy: 0.8258 - val_loss: 0.4374 - val_accuracy: 0.8197\n",
      "Epoch 60/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.3292 - accuracy: 0.8260\n",
      "Epoch 00060: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 116us/sample - loss: 0.3296 - accuracy: 0.8263 - val_loss: 0.4555 - val_accuracy: 0.8197\n",
      "Epoch 61/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.3297 - accuracy: 0.8248\n",
      "Epoch 00061: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 115us/sample - loss: 0.3299 - accuracy: 0.8247 - val_loss: 0.4422 - val_accuracy: 0.8185\n",
      "Epoch 62/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.3294 - accuracy: 0.8283\n",
      "Epoch 00062: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 114us/sample - loss: 0.3293 - accuracy: 0.8281 - val_loss: 0.4538 - val_accuracy: 0.8193\n",
      "Epoch 63/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.3258 - accuracy: 0.8269\n",
      "Epoch 00063: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 115us/sample - loss: 0.3259 - accuracy: 0.8267 - val_loss: 0.4652 - val_accuracy: 0.8147\n",
      "Epoch 64/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.3289 - accuracy: 0.8271\n",
      "Epoch 00064: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 114us/sample - loss: 0.3297 - accuracy: 0.8264 - val_loss: 0.4503 - val_accuracy: 0.8178\n",
      "Epoch 65/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.3244 - accuracy: 0.8276\n",
      "Epoch 00065: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 110us/sample - loss: 0.3247 - accuracy: 0.8274 - val_loss: 0.4986 - val_accuracy: 0.8224\n",
      "Epoch 66/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.3265 - accuracy: 0.8282\n",
      "Epoch 00066: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 109us/sample - loss: 0.3265 - accuracy: 0.8283 - val_loss: 0.4795 - val_accuracy: 0.8224\n",
      "Epoch 67/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.3270 - accuracy: 0.8264\n",
      "Epoch 00067: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 108us/sample - loss: 0.3274 - accuracy: 0.8263 - val_loss: 0.4507 - val_accuracy: 0.8189\n",
      "Epoch 68/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.3254 - accuracy: 0.8274\n",
      "Epoch 00068: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 108us/sample - loss: 0.3264 - accuracy: 0.8265 - val_loss: 0.4469 - val_accuracy: 0.8216\n",
      "Epoch 69/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.3244 - accuracy: 0.8282\n",
      "Epoch 00069: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 108us/sample - loss: 0.3246 - accuracy: 0.8286 - val_loss: 0.4637 - val_accuracy: 0.8150\n",
      "Epoch 70/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.3246 - accuracy: 0.8284\n",
      "Epoch 00070: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 109us/sample - loss: 0.3246 - accuracy: 0.8283 - val_loss: 0.5019 - val_accuracy: 0.8147\n",
      "Epoch 71/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.3257 - accuracy: 0.8289\n",
      "Epoch 00071: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 115us/sample - loss: 0.3262 - accuracy: 0.8288 - val_loss: 0.4784 - val_accuracy: 0.8119\n",
      "Epoch 72/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.3259 - accuracy: 0.8280\n",
      "Epoch 00072: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 116us/sample - loss: 0.3263 - accuracy: 0.8279 - val_loss: 0.4502 - val_accuracy: 0.8212\n",
      "Epoch 73/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3259 - accuracy: 0.8258\n",
      "Epoch 00073: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 115us/sample - loss: 0.3258 - accuracy: 0.8259 - val_loss: 0.4753 - val_accuracy: 0.8178\n",
      "Epoch 74/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.3251 - accuracy: 0.8305\n",
      "Epoch 00074: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 114us/sample - loss: 0.3246 - accuracy: 0.8309 - val_loss: 0.4779 - val_accuracy: 0.8181\n",
      "Epoch 75/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.3224 - accuracy: 0.8304\n",
      "Epoch 00075: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 120us/sample - loss: 0.3224 - accuracy: 0.8304 - val_loss: 0.4580 - val_accuracy: 0.8150\n",
      "Epoch 76/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.3228 - accuracy: 0.8288\n",
      "Epoch 00076: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 112us/sample - loss: 0.3230 - accuracy: 0.8284 - val_loss: 0.4797 - val_accuracy: 0.8193\n",
      "Epoch 77/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.3234 - accuracy: 0.8281\n",
      "Epoch 00077: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 114us/sample - loss: 0.3234 - accuracy: 0.8281 - val_loss: 0.4838 - val_accuracy: 0.8170\n",
      "Epoch 78/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.3221 - accuracy: 0.8288\n",
      "Epoch 00078: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 127us/sample - loss: 0.3220 - accuracy: 0.8288 - val_loss: 0.4637 - val_accuracy: 0.8212\n",
      "Epoch 79/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.3216 - accuracy: 0.8308\n",
      "Epoch 00079: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 122us/sample - loss: 0.3217 - accuracy: 0.8307 - val_loss: 0.4613 - val_accuracy: 0.8224\n",
      "Epoch 80/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.3227 - accuracy: 0.8317\n",
      "Epoch 00080: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 119us/sample - loss: 0.3229 - accuracy: 0.8317 - val_loss: 0.4608 - val_accuracy: 0.8092\n",
      "Epoch 81/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.3231 - accuracy: 0.8313\n",
      "Epoch 00081: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 116us/sample - loss: 0.3236 - accuracy: 0.8308 - val_loss: 0.4638 - val_accuracy: 0.8178\n",
      "Epoch 82/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.3226 - accuracy: 0.8298\n",
      "Epoch 00082: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 113us/sample - loss: 0.3225 - accuracy: 0.8298 - val_loss: 0.4759 - val_accuracy: 0.8189\n",
      "Epoch 83/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.3207 - accuracy: 0.8303\n",
      "Epoch 00083: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 118us/sample - loss: 0.3207 - accuracy: 0.8303 - val_loss: 0.4848 - val_accuracy: 0.8150\n",
      "Epoch 84/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.3203 - accuracy: 0.8280\n",
      "Epoch 00084: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 119us/sample - loss: 0.3202 - accuracy: 0.8278 - val_loss: 0.4888 - val_accuracy: 0.8189\n",
      "Epoch 85/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.3212 - accuracy: 0.8288\n",
      "Epoch 00085: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 130us/sample - loss: 0.3210 - accuracy: 0.8290 - val_loss: 0.5085 - val_accuracy: 0.8185\n",
      "Epoch 86/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3203 - accuracy: 0.8312\n",
      "Epoch 00086: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 127us/sample - loss: 0.3203 - accuracy: 0.8312 - val_loss: 0.4852 - val_accuracy: 0.8139\n",
      "Epoch 87/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.3205 - accuracy: 0.8286\n",
      "Epoch 00087: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 110us/sample - loss: 0.3205 - accuracy: 0.8289 - val_loss: 0.5034 - val_accuracy: 0.8150\n",
      "Epoch 88/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3208 - accuracy: 0.8289\n",
      "Epoch 00088: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 108us/sample - loss: 0.3212 - accuracy: 0.8286 - val_loss: 0.4967 - val_accuracy: 0.8201\n",
      "Epoch 89/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.3197 - accuracy: 0.8303\n",
      "Epoch 00089: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 108us/sample - loss: 0.3204 - accuracy: 0.8296 - val_loss: 0.4626 - val_accuracy: 0.8197\n",
      "Epoch 90/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3210 - accuracy: 0.8289\n",
      "Epoch 00090: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 110us/sample - loss: 0.3210 - accuracy: 0.8289 - val_loss: 0.4975 - val_accuracy: 0.8205\n",
      "Epoch 91/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.3180 - accuracy: 0.8308\n",
      "Epoch 00091: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 115us/sample - loss: 0.3181 - accuracy: 0.8307 - val_loss: 0.5013 - val_accuracy: 0.8170\n",
      "Epoch 92/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3209 - accuracy: 0.8286\n",
      "Epoch 00092: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 108us/sample - loss: 0.3203 - accuracy: 0.8293 - val_loss: 0.5055 - val_accuracy: 0.8139\n",
      "Epoch 93/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.3210 - accuracy: 0.8296\n",
      "Epoch 00093: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 121us/sample - loss: 0.3216 - accuracy: 0.8293 - val_loss: 0.4768 - val_accuracy: 0.8209\n",
      "Epoch 94/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.3185 - accuracy: 0.8305\n",
      "Epoch 00094: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 109us/sample - loss: 0.3187 - accuracy: 0.8307 - val_loss: 0.4958 - val_accuracy: 0.8189\n",
      "Epoch 95/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3183 - accuracy: 0.8311\n",
      "Epoch 00095: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 114us/sample - loss: 0.3181 - accuracy: 0.8312 - val_loss: 0.4974 - val_accuracy: 0.8092\n",
      "Epoch 96/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3167 - accuracy: 0.8312\n",
      "Epoch 00096: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 113us/sample - loss: 0.3173 - accuracy: 0.8308 - val_loss: 0.4778 - val_accuracy: 0.8170\n",
      "Epoch 97/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.3182 - accuracy: 0.8314\n",
      "Epoch 00097: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 115us/sample - loss: 0.3181 - accuracy: 0.8315 - val_loss: 0.4922 - val_accuracy: 0.8216\n",
      "Epoch 98/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.3175 - accuracy: 0.8305\n",
      "Epoch 00098: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 115us/sample - loss: 0.3177 - accuracy: 0.8305 - val_loss: 0.5158 - val_accuracy: 0.8158\n",
      "Epoch 99/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3168 - accuracy: 0.8307\n",
      "Epoch 00099: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 112us/sample - loss: 0.3170 - accuracy: 0.8306 - val_loss: 0.4771 - val_accuracy: 0.8224\n",
      "Epoch 100/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.3184 - accuracy: 0.8321\n",
      "Epoch 00100: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 109us/sample - loss: 0.3183 - accuracy: 0.8321 - val_loss: 0.4958 - val_accuracy: 0.8224\n",
      "Epoch 101/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.3179 - accuracy: 0.8319\n",
      "Epoch 00101: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 113us/sample - loss: 0.3176 - accuracy: 0.8318 - val_loss: 0.4891 - val_accuracy: 0.8143\n",
      "Epoch 102/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.3182 - accuracy: 0.8329\n",
      "Epoch 00102: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 112us/sample - loss: 0.3183 - accuracy: 0.8328 - val_loss: 0.4795 - val_accuracy: 0.8201\n",
      "Epoch 103/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.3154 - accuracy: 0.8295\n",
      "Epoch 00103: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 112us/sample - loss: 0.3160 - accuracy: 0.8289 - val_loss: 0.4951 - val_accuracy: 0.8170\n",
      "Epoch 104/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.3158 - accuracy: 0.8323\n",
      "Epoch 00104: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 127us/sample - loss: 0.3151 - accuracy: 0.8329 - val_loss: 0.5015 - val_accuracy: 0.8127\n",
      "Epoch 105/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.3186 - accuracy: 0.8305\n",
      "Epoch 00105: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 123us/sample - loss: 0.3183 - accuracy: 0.8307 - val_loss: 0.4704 - val_accuracy: 0.8216\n",
      "Epoch 106/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.3165 - accuracy: 0.8321\n",
      "Epoch 00106: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 143us/sample - loss: 0.3165 - accuracy: 0.8321 - val_loss: 0.4973 - val_accuracy: 0.8174\n",
      "Epoch 107/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.3172 - accuracy: 0.8303\n",
      "Epoch 00107: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 137us/sample - loss: 0.3169 - accuracy: 0.8304 - val_loss: 0.5007 - val_accuracy: 0.8112\n",
      "Epoch 108/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.3179 - accuracy: 0.8320\n",
      "Epoch 00108: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 147us/sample - loss: 0.3178 - accuracy: 0.8323 - val_loss: 0.4979 - val_accuracy: 0.8170\n",
      "Epoch 109/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3151 - accuracy: 0.8310\n",
      "Epoch 00109: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 4s 154us/sample - loss: 0.3146 - accuracy: 0.8314 - val_loss: 0.5046 - val_accuracy: 0.8216\n",
      "Epoch 110/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3188 - accuracy: 0.8295\n",
      "Epoch 00110: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 4s 162us/sample - loss: 0.3189 - accuracy: 0.8294 - val_loss: 0.4847 - val_accuracy: 0.8216\n",
      "Epoch 111/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.3161 - accuracy: 0.8293\n",
      "Epoch 00111: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 139us/sample - loss: 0.3161 - accuracy: 0.8292 - val_loss: 0.5085 - val_accuracy: 0.8127\n",
      "Epoch 112/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3179 - accuracy: 0.8300\n",
      "Epoch 00112: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 126us/sample - loss: 0.3180 - accuracy: 0.8299 - val_loss: 0.4801 - val_accuracy: 0.8112\n",
      "Epoch 113/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.3132 - accuracy: 0.8333\n",
      "Epoch 00113: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 122us/sample - loss: 0.3135 - accuracy: 0.8331 - val_loss: 0.5256 - val_accuracy: 0.8096\n",
      "Epoch 114/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3140 - accuracy: 0.8327\n",
      "Epoch 00114: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 135us/sample - loss: 0.3140 - accuracy: 0.8327 - val_loss: 0.5079 - val_accuracy: 0.8193\n",
      "Epoch 115/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3145 - accuracy: 0.8330\n",
      "Epoch 00115: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 115us/sample - loss: 0.3151 - accuracy: 0.8326 - val_loss: 0.4983 - val_accuracy: 0.8174\n",
      "Epoch 116/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.3144 - accuracy: 0.8315\n",
      "Epoch 00116: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 121us/sample - loss: 0.3148 - accuracy: 0.8314 - val_loss: 0.5140 - val_accuracy: 0.8162\n",
      "Epoch 117/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.3197 - accuracy: 0.8318\n",
      "Epoch 00117: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 120us/sample - loss: 0.3197 - accuracy: 0.8316 - val_loss: 0.4929 - val_accuracy: 0.7991\n",
      "Epoch 118/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.3140 - accuracy: 0.8313\n",
      "Epoch 00118: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 117us/sample - loss: 0.3146 - accuracy: 0.8312 - val_loss: 0.5062 - val_accuracy: 0.8135\n",
      "Epoch 119/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.3135 - accuracy: 0.8312\n",
      "Epoch 00119: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 120us/sample - loss: 0.3133 - accuracy: 0.8314 - val_loss: 0.5152 - val_accuracy: 0.8174\n",
      "Epoch 120/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.3121 - accuracy: 0.8335\n",
      "Epoch 00120: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 135us/sample - loss: 0.3121 - accuracy: 0.8335 - val_loss: 0.5157 - val_accuracy: 0.8193\n",
      "Epoch 121/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.3167 - accuracy: 0.8303\n",
      "Epoch 00121: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 130us/sample - loss: 0.3169 - accuracy: 0.8299 - val_loss: 0.5224 - val_accuracy: 0.8181\n",
      "Epoch 122/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.3152 - accuracy: 0.8336\n",
      "Epoch 00122: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 110us/sample - loss: 0.3157 - accuracy: 0.8332 - val_loss: 0.4914 - val_accuracy: 0.8220\n",
      "Epoch 123/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.3155 - accuracy: 0.8321\n",
      "Epoch 00123: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 116us/sample - loss: 0.3152 - accuracy: 0.8320 - val_loss: 0.4986 - val_accuracy: 0.8224\n",
      "Epoch 124/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.3135 - accuracy: 0.8339\n",
      "Epoch 00124: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 116us/sample - loss: 0.3137 - accuracy: 0.8339 - val_loss: 0.5240 - val_accuracy: 0.8131\n",
      "Epoch 125/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.3141 - accuracy: 0.8315\n",
      "Epoch 00125: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 120us/sample - loss: 0.3140 - accuracy: 0.8315 - val_loss: 0.5266 - val_accuracy: 0.8077\n",
      "Epoch 126/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.3127 - accuracy: 0.8324\n",
      "Epoch 00126: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 112us/sample - loss: 0.3127 - accuracy: 0.8324 - val_loss: 0.5123 - val_accuracy: 0.8158\n",
      "Epoch 127/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.3129 - accuracy: 0.8322\n",
      "Epoch 00127: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 114us/sample - loss: 0.3131 - accuracy: 0.8319 - val_loss: 0.5476 - val_accuracy: 0.7953\n",
      "Epoch 128/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.3135 - accuracy: 0.8327\n",
      "Epoch 00128: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 125us/sample - loss: 0.3137 - accuracy: 0.8330 - val_loss: 0.5177 - val_accuracy: 0.8166\n",
      "Epoch 129/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.3135 - accuracy: 0.8329\n",
      "Epoch 00129: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 131us/sample - loss: 0.3131 - accuracy: 0.8330 - val_loss: 0.5290 - val_accuracy: 0.8092\n",
      "Epoch 130/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.3120 - accuracy: 0.8315\n",
      "Epoch 00130: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 131us/sample - loss: 0.3117 - accuracy: 0.8317 - val_loss: 0.5152 - val_accuracy: 0.8189\n",
      "Epoch 131/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3129 - accuracy: 0.8338\n",
      "Epoch 00131: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 124us/sample - loss: 0.3130 - accuracy: 0.8337 - val_loss: 0.5271 - val_accuracy: 0.8185\n",
      "Epoch 132/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.3116 - accuracy: 0.8313\n",
      "Epoch 00132: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 117us/sample - loss: 0.3115 - accuracy: 0.8316 - val_loss: 0.5152 - val_accuracy: 0.8119\n",
      "Epoch 133/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.3124 - accuracy: 0.8354\n",
      "Epoch 00133: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 117us/sample - loss: 0.3127 - accuracy: 0.8351 - val_loss: 0.5078 - val_accuracy: 0.8178\n",
      "Epoch 134/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.3127 - accuracy: 0.8325\n",
      "Epoch 00134: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 124us/sample - loss: 0.3130 - accuracy: 0.8327 - val_loss: 0.5168 - val_accuracy: 0.8174\n",
      "Epoch 135/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.3151 - accuracy: 0.8319\n",
      "Epoch 00135: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 120us/sample - loss: 0.3150 - accuracy: 0.8320 - val_loss: 0.5253 - val_accuracy: 0.8162\n",
      "Epoch 136/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.3111 - accuracy: 0.8343\n",
      "Epoch 00136: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 127us/sample - loss: 0.3121 - accuracy: 0.8339 - val_loss: 0.5091 - val_accuracy: 0.8178\n",
      "Epoch 137/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3117 - accuracy: 0.8320\n",
      "Epoch 00137: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 131us/sample - loss: 0.3117 - accuracy: 0.8320 - val_loss: 0.5173 - val_accuracy: 0.8212\n",
      "Epoch 138/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.3126 - accuracy: 0.8332\n",
      "Epoch 00138: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 119us/sample - loss: 0.3122 - accuracy: 0.8335 - val_loss: 0.5041 - val_accuracy: 0.8189\n",
      "Epoch 139/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.3111 - accuracy: 0.8321\n",
      "Epoch 00139: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 113us/sample - loss: 0.3109 - accuracy: 0.8321 - val_loss: 0.5170 - val_accuracy: 0.8193\n",
      "Epoch 140/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.3134 - accuracy: 0.8333\n",
      "Epoch 00140: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 110us/sample - loss: 0.3134 - accuracy: 0.8332 - val_loss: 0.5148 - val_accuracy: 0.8162\n",
      "Epoch 141/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.3133 - accuracy: 0.8309\n",
      "Epoch 00141: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 109us/sample - loss: 0.3132 - accuracy: 0.8310 - val_loss: 0.5000 - val_accuracy: 0.8185\n",
      "Epoch 142/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.3112 - accuracy: 0.8306\n",
      "Epoch 00142: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 108us/sample - loss: 0.3108 - accuracy: 0.8310 - val_loss: 0.5316 - val_accuracy: 0.8193\n",
      "Epoch 143/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.3096 - accuracy: 0.8325\n",
      "Epoch 00143: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 120us/sample - loss: 0.3097 - accuracy: 0.8325 - val_loss: 0.5494 - val_accuracy: 0.8170\n",
      "Epoch 144/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.3117 - accuracy: 0.8330\n",
      "Epoch 00144: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 123us/sample - loss: 0.3114 - accuracy: 0.8331 - val_loss: 0.5540 - val_accuracy: 0.8193\n",
      "Epoch 145/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.3137 - accuracy: 0.8322\n",
      "Epoch 00145: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 126us/sample - loss: 0.3137 - accuracy: 0.8322 - val_loss: 0.5347 - val_accuracy: 0.8220\n",
      "Epoch 146/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.3129 - accuracy: 0.8328\n",
      "Epoch 00146: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 121us/sample - loss: 0.3129 - accuracy: 0.8330 - val_loss: 0.5318 - val_accuracy: 0.8170\n",
      "Epoch 147/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.3122 - accuracy: 0.8310\n",
      "Epoch 00147: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 4s 153us/sample - loss: 0.3116 - accuracy: 0.8315 - val_loss: 0.5503 - val_accuracy: 0.8135\n",
      "Epoch 148/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.3107 - accuracy: 0.8335\n",
      "Epoch 00148: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 126us/sample - loss: 0.3112 - accuracy: 0.8332 - val_loss: 0.5257 - val_accuracy: 0.8162\n",
      "Epoch 149/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.3139 - accuracy: 0.8327\n",
      "Epoch 00149: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 138us/sample - loss: 0.3139 - accuracy: 0.8328 - val_loss: 0.5160 - val_accuracy: 0.8174\n",
      "Epoch 150/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.3105 - accuracy: 0.8340\n",
      "Epoch 00150: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 147us/sample - loss: 0.3105 - accuracy: 0.8340 - val_loss: 0.5182 - val_accuracy: 0.8174\n",
      "Epoch 151/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.3104 - accuracy: 0.8329\n",
      "Epoch 00151: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 125us/sample - loss: 0.3100 - accuracy: 0.8328 - val_loss: 0.5237 - val_accuracy: 0.8158\n",
      "Epoch 152/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3092 - accuracy: 0.8338\n",
      "Epoch 00152: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 116us/sample - loss: 0.3094 - accuracy: 0.8337 - val_loss: 0.5309 - val_accuracy: 0.8193\n",
      "Epoch 153/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3094 - accuracy: 0.8348\n",
      "Epoch 00153: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 151us/sample - loss: 0.3100 - accuracy: 0.8345 - val_loss: 0.5279 - val_accuracy: 0.8147\n",
      "Epoch 154/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.3091 - accuracy: 0.8360\n",
      "Epoch 00154: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 116us/sample - loss: 0.3091 - accuracy: 0.8359 - val_loss: 0.5173 - val_accuracy: 0.8123\n",
      "Epoch 155/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.3082 - accuracy: 0.8328\n",
      "Epoch 00155: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 150us/sample - loss: 0.3083 - accuracy: 0.8327 - val_loss: 0.5310 - val_accuracy: 0.8197\n",
      "Epoch 156/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.3099 - accuracy: 0.8343\n",
      "Epoch 00156: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 149us/sample - loss: 0.3097 - accuracy: 0.8344 - val_loss: 0.5317 - val_accuracy: 0.8209\n",
      "Epoch 157/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.3107 - accuracy: 0.8337\n",
      "Epoch 00157: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 121us/sample - loss: 0.3104 - accuracy: 0.8342 - val_loss: 0.5337 - val_accuracy: 0.8178\n",
      "Epoch 158/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.3083 - accuracy: 0.8360\n",
      "Epoch 00158: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 118us/sample - loss: 0.3087 - accuracy: 0.8356 - val_loss: 0.5411 - val_accuracy: 0.8181\n",
      "Epoch 159/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.3101 - accuracy: 0.8342\n",
      "Epoch 00159: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 135us/sample - loss: 0.3103 - accuracy: 0.8341 - val_loss: 0.5466 - val_accuracy: 0.8189\n",
      "Epoch 160/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.3095 - accuracy: 0.8338\n",
      "Epoch 00160: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 136us/sample - loss: 0.3093 - accuracy: 0.8340 - val_loss: 0.5210 - val_accuracy: 0.8224\n",
      "Epoch 161/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.3115 - accuracy: 0.8329\n",
      "Epoch 00161: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 136us/sample - loss: 0.3115 - accuracy: 0.8329 - val_loss: 0.5358 - val_accuracy: 0.8166\n",
      "Epoch 162/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.3079 - accuracy: 0.8340\n",
      "Epoch 00162: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 133us/sample - loss: 0.3080 - accuracy: 0.8340 - val_loss: 0.5397 - val_accuracy: 0.8212\n",
      "Epoch 163/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.3118 - accuracy: 0.8341\n",
      "Epoch 00163: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 149us/sample - loss: 0.3114 - accuracy: 0.8345 - val_loss: 0.5030 - val_accuracy: 0.8147\n",
      "Epoch 164/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.3074 - accuracy: 0.8353\n",
      "Epoch 00164: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 122us/sample - loss: 0.3076 - accuracy: 0.8352 - val_loss: 0.5368 - val_accuracy: 0.8209\n",
      "Epoch 165/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.3089 - accuracy: 0.8353\n",
      "Epoch 00165: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 110us/sample - loss: 0.3092 - accuracy: 0.8353 - val_loss: 0.5395 - val_accuracy: 0.8181\n",
      "Epoch 166/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.3091 - accuracy: 0.8356\n",
      "Epoch 00166: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 128us/sample - loss: 0.3091 - accuracy: 0.8354 - val_loss: 0.5346 - val_accuracy: 0.8181\n",
      "Epoch 167/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.3100 - accuracy: 0.8347\n",
      "Epoch 00167: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 4s 155us/sample - loss: 0.3100 - accuracy: 0.8347 - val_loss: 0.5285 - val_accuracy: 0.8181\n",
      "Epoch 168/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3094 - accuracy: 0.8351\n",
      "Epoch 00168: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 129us/sample - loss: 0.3095 - accuracy: 0.8350 - val_loss: 0.5354 - val_accuracy: 0.8189\n",
      "Epoch 169/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3089 - accuracy: 0.8337\n",
      "Epoch 00169: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 144us/sample - loss: 0.3090 - accuracy: 0.8336 - val_loss: 0.5607 - val_accuracy: 0.8185\n",
      "Epoch 170/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.3079 - accuracy: 0.8361\n",
      "Epoch 00170: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 4s 165us/sample - loss: 0.3078 - accuracy: 0.8360 - val_loss: 0.5665 - val_accuracy: 0.8123\n",
      "Epoch 171/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.3108 - accuracy: 0.8345\n",
      "Epoch 00171: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 140us/sample - loss: 0.3106 - accuracy: 0.8345 - val_loss: 0.5379 - val_accuracy: 0.8162\n",
      "Epoch 172/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.3094 - accuracy: 0.8341\n",
      "Epoch 00172: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 118us/sample - loss: 0.3090 - accuracy: 0.8344 - val_loss: 0.5759 - val_accuracy: 0.8154\n",
      "Epoch 173/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.3104 - accuracy: 0.8330\n",
      "Epoch 00173: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 111us/sample - loss: 0.3100 - accuracy: 0.8334 - val_loss: 0.5503 - val_accuracy: 0.8174\n",
      "Epoch 174/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.3085 - accuracy: 0.8325\n",
      "Epoch 00174: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 111us/sample - loss: 0.3085 - accuracy: 0.8329 - val_loss: 0.5500 - val_accuracy: 0.8166\n",
      "Epoch 175/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.3055 - accuracy: 0.8337\n",
      "Epoch 00175: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 115us/sample - loss: 0.3062 - accuracy: 0.8334 - val_loss: 0.5682 - val_accuracy: 0.8174\n",
      "Epoch 176/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.3074 - accuracy: 0.8343\n",
      "Epoch 00176: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 111us/sample - loss: 0.3079 - accuracy: 0.8343 - val_loss: 0.5441 - val_accuracy: 0.8154\n",
      "Epoch 177/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.3072 - accuracy: 0.8330\n",
      "Epoch 00177: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 109us/sample - loss: 0.3080 - accuracy: 0.8328 - val_loss: 0.5447 - val_accuracy: 0.8135\n",
      "Epoch 178/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3072 - accuracy: 0.8342\n",
      "Epoch 00178: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 110us/sample - loss: 0.3069 - accuracy: 0.8345 - val_loss: 0.5739 - val_accuracy: 0.8092\n",
      "Epoch 179/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.3100 - accuracy: 0.8333\n",
      "Epoch 00179: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 115us/sample - loss: 0.3101 - accuracy: 0.8332 - val_loss: 0.5597 - val_accuracy: 0.8077\n",
      "Epoch 180/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.3083 - accuracy: 0.8355\n",
      "Epoch 00180: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 115us/sample - loss: 0.3082 - accuracy: 0.8355 - val_loss: 0.5711 - val_accuracy: 0.8123\n",
      "Epoch 181/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.3084 - accuracy: 0.8335\n",
      "Epoch 00181: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 116us/sample - loss: 0.3080 - accuracy: 0.8338 - val_loss: 0.5641 - val_accuracy: 0.8150\n",
      "Epoch 182/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.3081 - accuracy: 0.8348\n",
      "Epoch 00182: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 125us/sample - loss: 0.3079 - accuracy: 0.8350 - val_loss: 0.5573 - val_accuracy: 0.8174\n",
      "Epoch 183/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.3074 - accuracy: 0.8362\n",
      "Epoch 00183: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 120us/sample - loss: 0.3074 - accuracy: 0.8360 - val_loss: 0.5571 - val_accuracy: 0.8174\n",
      "Epoch 184/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.3062 - accuracy: 0.8360\n",
      "Epoch 00184: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 120us/sample - loss: 0.3062 - accuracy: 0.8362 - val_loss: 0.5601 - val_accuracy: 0.8123\n",
      "Epoch 185/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.3077 - accuracy: 0.8351\n",
      "Epoch 00185: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 143us/sample - loss: 0.3078 - accuracy: 0.8349 - val_loss: 0.5651 - val_accuracy: 0.8104\n",
      "Epoch 186/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.3065 - accuracy: 0.8364\n",
      "Epoch 00186: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 115us/sample - loss: 0.3061 - accuracy: 0.8367 - val_loss: 0.5492 - val_accuracy: 0.8174\n",
      "Epoch 187/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3067 - accuracy: 0.8361\n",
      "Epoch 00187: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 149us/sample - loss: 0.3065 - accuracy: 0.8363 - val_loss: 0.5599 - val_accuracy: 0.8181\n",
      "Epoch 188/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.3063 - accuracy: 0.8359\n",
      "Epoch 00188: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 150us/sample - loss: 0.3067 - accuracy: 0.8353 - val_loss: 0.5781 - val_accuracy: 0.8119\n",
      "Epoch 189/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.3070 - accuracy: 0.8355\n",
      "Epoch 00189: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 125us/sample - loss: 0.3077 - accuracy: 0.8348 - val_loss: 0.5706 - val_accuracy: 0.8170\n",
      "Epoch 190/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.3067 - accuracy: 0.8367\n",
      "Epoch 00190: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 114us/sample - loss: 0.3070 - accuracy: 0.8366 - val_loss: 0.5547 - val_accuracy: 0.8181\n",
      "Epoch 191/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.3066 - accuracy: 0.8353\n",
      "Epoch 00191: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 113us/sample - loss: 0.3071 - accuracy: 0.8351 - val_loss: 0.5560 - val_accuracy: 0.8174\n",
      "Epoch 192/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.3058 - accuracy: 0.8351\n",
      "Epoch 00192: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 108us/sample - loss: 0.3052 - accuracy: 0.8355 - val_loss: 0.6045 - val_accuracy: 0.8131\n",
      "Epoch 193/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.3086 - accuracy: 0.8353\n",
      "Epoch 00193: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 109us/sample - loss: 0.3086 - accuracy: 0.8356 - val_loss: 0.5930 - val_accuracy: 0.8135\n",
      "Epoch 194/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3065 - accuracy: 0.8355\n",
      "Epoch 00194: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 112us/sample - loss: 0.3064 - accuracy: 0.8355 - val_loss: 0.5469 - val_accuracy: 0.8185\n",
      "Epoch 195/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.3058 - accuracy: 0.8352\n",
      "Epoch 00195: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 113us/sample - loss: 0.3058 - accuracy: 0.8353 - val_loss: 0.5483 - val_accuracy: 0.8174\n",
      "Epoch 196/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.3063 - accuracy: 0.8358\n",
      "Epoch 00196: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 112us/sample - loss: 0.3068 - accuracy: 0.8356 - val_loss: 0.5810 - val_accuracy: 0.8154\n",
      "Epoch 197/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.3070 - accuracy: 0.8347\n",
      "Epoch 00197: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 115us/sample - loss: 0.3072 - accuracy: 0.8346 - val_loss: 0.5688 - val_accuracy: 0.8166\n",
      "Epoch 198/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.3057 - accuracy: 0.8361\n",
      "Epoch 00198: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 116us/sample - loss: 0.3067 - accuracy: 0.8357 - val_loss: 0.5633 - val_accuracy: 0.8100\n",
      "Epoch 199/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.3069 - accuracy: 0.8346\n",
      "Epoch 00199: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 128us/sample - loss: 0.3072 - accuracy: 0.8343 - val_loss: 0.5720 - val_accuracy: 0.8154\n",
      "Epoch 200/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.3075 - accuracy: 0.8345\n",
      "Epoch 00200: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 4s 175us/sample - loss: 0.3078 - accuracy: 0.8343 - val_loss: 0.5481 - val_accuracy: 0.8170\n",
      "Epoch 201/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.3063 - accuracy: 0.8353\n",
      "Epoch 00201: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 4s 163us/sample - loss: 0.3061 - accuracy: 0.8353 - val_loss: 0.5892 - val_accuracy: 0.8104\n",
      "Epoch 202/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.3050 - accuracy: 0.8339\n",
      "Epoch 00202: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 140us/sample - loss: 0.3043 - accuracy: 0.8342 - val_loss: 0.5707 - val_accuracy: 0.8166\n",
      "Epoch 203/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3070 - accuracy: 0.8357\n",
      "Epoch 00203: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 116us/sample - loss: 0.3069 - accuracy: 0.8357 - val_loss: 0.5506 - val_accuracy: 0.8193\n",
      "Epoch 204/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.3073 - accuracy: 0.8354\n",
      "Epoch 00204: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 110us/sample - loss: 0.3077 - accuracy: 0.8352 - val_loss: 0.5905 - val_accuracy: 0.8158\n",
      "Epoch 205/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.3058 - accuracy: 0.8352\n",
      "Epoch 00205: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 115us/sample - loss: 0.3058 - accuracy: 0.8352 - val_loss: 0.5508 - val_accuracy: 0.8209\n",
      "Epoch 206/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.3056 - accuracy: 0.8358\n",
      "Epoch 00206: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 116us/sample - loss: 0.3058 - accuracy: 0.8358 - val_loss: 0.5860 - val_accuracy: 0.8135\n",
      "Epoch 207/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.3078 - accuracy: 0.8356\n",
      "Epoch 00207: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 116us/sample - loss: 0.3076 - accuracy: 0.8357 - val_loss: 0.5763 - val_accuracy: 0.8162\n",
      "Epoch 208/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.3049 - accuracy: 0.8357\n",
      "Epoch 00208: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 113us/sample - loss: 0.3052 - accuracy: 0.8355 - val_loss: 0.6045 - val_accuracy: 0.8189\n",
      "Epoch 209/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.3047 - accuracy: 0.8373\n",
      "Epoch 00209: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 109us/sample - loss: 0.3046 - accuracy: 0.8373 - val_loss: 0.5859 - val_accuracy: 0.8224\n",
      "Epoch 210/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.3063 - accuracy: 0.8358\n",
      "Epoch 00210: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 108us/sample - loss: 0.3060 - accuracy: 0.8358 - val_loss: 0.5626 - val_accuracy: 0.8228\n",
      "Epoch 211/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.3073 - accuracy: 0.8359\n",
      "Epoch 00211: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 112us/sample - loss: 0.3074 - accuracy: 0.8358 - val_loss: 0.5792 - val_accuracy: 0.8123\n",
      "Epoch 212/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.3052 - accuracy: 0.8352\n",
      "Epoch 00212: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 112us/sample - loss: 0.3052 - accuracy: 0.8351 - val_loss: 0.5739 - val_accuracy: 0.8174\n",
      "Epoch 213/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.3052 - accuracy: 0.8349\n",
      "Epoch 00213: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 110us/sample - loss: 0.3056 - accuracy: 0.8347 - val_loss: 0.5726 - val_accuracy: 0.8162\n",
      "Epoch 214/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.3062 - accuracy: 0.8360\n",
      "Epoch 00214: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 116us/sample - loss: 0.3061 - accuracy: 0.8363 - val_loss: 0.5610 - val_accuracy: 0.8201\n",
      "Epoch 215/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.3054 - accuracy: 0.8335\n",
      "Epoch 00215: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 114us/sample - loss: 0.3053 - accuracy: 0.8335 - val_loss: 0.5637 - val_accuracy: 0.8193\n",
      "Epoch 216/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.3053 - accuracy: 0.8373\n",
      "Epoch 00216: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 113us/sample - loss: 0.3057 - accuracy: 0.8371 - val_loss: 0.5366 - val_accuracy: 0.8158\n",
      "Epoch 217/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.3050 - accuracy: 0.8360\n",
      "Epoch 00217: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 112us/sample - loss: 0.3050 - accuracy: 0.8359 - val_loss: 0.5702 - val_accuracy: 0.8154\n",
      "Epoch 218/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.3040 - accuracy: 0.8357\n",
      "Epoch 00218: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 113us/sample - loss: 0.3034 - accuracy: 0.8361 - val_loss: 0.5611 - val_accuracy: 0.8224\n",
      "Epoch 219/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.3059 - accuracy: 0.8337\n",
      "Epoch 00219: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 111us/sample - loss: 0.3059 - accuracy: 0.8336 - val_loss: 0.5743 - val_accuracy: 0.8170\n",
      "Epoch 220/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.3061 - accuracy: 0.8359\n",
      "Epoch 00220: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 113us/sample - loss: 0.3061 - accuracy: 0.8359 - val_loss: 0.5762 - val_accuracy: 0.8069\n",
      "Epoch 221/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.3051 - accuracy: 0.8360\n",
      "Epoch 00221: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 114us/sample - loss: 0.3052 - accuracy: 0.8358 - val_loss: 0.5672 - val_accuracy: 0.8197\n",
      "Epoch 222/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.3061 - accuracy: 0.8369\n",
      "Epoch 00222: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 111us/sample - loss: 0.3063 - accuracy: 0.8366 - val_loss: 0.5701 - val_accuracy: 0.8197\n",
      "Epoch 223/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.3057 - accuracy: 0.8362\n",
      "Epoch 00223: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 114us/sample - loss: 0.3058 - accuracy: 0.8362 - val_loss: 0.5414 - val_accuracy: 0.8224\n",
      "Epoch 224/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.3067 - accuracy: 0.8354\n",
      "Epoch 00224: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 117us/sample - loss: 0.3066 - accuracy: 0.8354 - val_loss: 0.5582 - val_accuracy: 0.8166\n",
      "Epoch 225/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.3041 - accuracy: 0.8361\n",
      "Epoch 00225: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 116us/sample - loss: 0.3041 - accuracy: 0.8360 - val_loss: 0.5735 - val_accuracy: 0.8088\n",
      "Epoch 226/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3046 - accuracy: 0.8367\n",
      "Epoch 00226: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 118us/sample - loss: 0.3046 - accuracy: 0.8368 - val_loss: 0.5552 - val_accuracy: 0.8189\n",
      "Epoch 227/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.3032 - accuracy: 0.8373\n",
      "Epoch 00227: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 110us/sample - loss: 0.3031 - accuracy: 0.8374 - val_loss: 0.6036 - val_accuracy: 0.8092\n",
      "Epoch 228/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.3049 - accuracy: 0.8362\n",
      "Epoch 00228: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 113us/sample - loss: 0.3049 - accuracy: 0.8362 - val_loss: 0.5776 - val_accuracy: 0.8181\n",
      "Epoch 229/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.3055 - accuracy: 0.8368\n",
      "Epoch 00229: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 110us/sample - loss: 0.3057 - accuracy: 0.8371 - val_loss: 0.5466 - val_accuracy: 0.8193\n",
      "Epoch 230/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.3056 - accuracy: 0.8351\n",
      "Epoch 00230: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.3057 - accuracy: 0.8350 - val_loss: 0.5862 - val_accuracy: 0.8147\n",
      "Epoch 231/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.3028 - accuracy: 0.8367\n",
      "Epoch 00231: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.3027 - accuracy: 0.8367 - val_loss: 0.5841 - val_accuracy: 0.8158\n",
      "Epoch 232/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.3073 - accuracy: 0.8355\n",
      "Epoch 00232: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.3068 - accuracy: 0.8355 - val_loss: 0.5852 - val_accuracy: 0.8189\n",
      "Epoch 233/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.3050 - accuracy: 0.8374\n",
      "Epoch 00233: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.3049 - accuracy: 0.8375 - val_loss: 0.5604 - val_accuracy: 0.8112\n",
      "Epoch 234/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.3036 - accuracy: 0.8369\n",
      "Epoch 00234: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.3036 - accuracy: 0.8373 - val_loss: 0.5748 - val_accuracy: 0.8135\n",
      "Epoch 235/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3023 - accuracy: 0.8381\n",
      "Epoch 00235: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.3025 - accuracy: 0.8382 - val_loss: 0.5562 - val_accuracy: 0.8197\n",
      "Epoch 236/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.3038 - accuracy: 0.8373\n",
      "Epoch 00236: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 107us/sample - loss: 0.3038 - accuracy: 0.8375 - val_loss: 0.5997 - val_accuracy: 0.8100\n",
      "Epoch 237/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.3037 - accuracy: 0.8368\n",
      "Epoch 00237: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.3038 - accuracy: 0.8366 - val_loss: 0.5734 - val_accuracy: 0.8162\n",
      "Epoch 238/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.3059 - accuracy: 0.8360\n",
      "Epoch 00238: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.3058 - accuracy: 0.8361 - val_loss: 0.5754 - val_accuracy: 0.8154\n",
      "Epoch 239/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.3052 - accuracy: 0.8361\n",
      "Epoch 00239: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.3055 - accuracy: 0.8361 - val_loss: 0.5806 - val_accuracy: 0.8123\n",
      "Epoch 240/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3063 - accuracy: 0.8386\n",
      "Epoch 00240: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.3063 - accuracy: 0.8383 - val_loss: 0.5564 - val_accuracy: 0.8216\n",
      "Epoch 241/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.3017 - accuracy: 0.8366\n",
      "Epoch 00241: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.3018 - accuracy: 0.8369 - val_loss: 0.5525 - val_accuracy: 0.8197\n",
      "Epoch 242/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.3048 - accuracy: 0.8376\n",
      "Epoch 00242: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.3049 - accuracy: 0.8376 - val_loss: 0.5783 - val_accuracy: 0.8178\n",
      "Epoch 243/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.3043 - accuracy: 0.8368\n",
      "Epoch 00243: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.3041 - accuracy: 0.8370 - val_loss: 0.5946 - val_accuracy: 0.8162\n",
      "Epoch 244/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.3031 - accuracy: 0.8370\n",
      "Epoch 00244: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.3032 - accuracy: 0.8371 - val_loss: 0.5986 - val_accuracy: 0.8174\n",
      "Epoch 245/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.3037 - accuracy: 0.8390\n",
      "Epoch 00245: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.3036 - accuracy: 0.8392 - val_loss: 0.5887 - val_accuracy: 0.8158\n",
      "Epoch 246/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.3029 - accuracy: 0.8356\n",
      "Epoch 00246: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 109us/sample - loss: 0.3036 - accuracy: 0.8352 - val_loss: 0.5869 - val_accuracy: 0.8189\n",
      "Epoch 247/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.3039 - accuracy: 0.8347\n",
      "Epoch 00247: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 112us/sample - loss: 0.3037 - accuracy: 0.8347 - val_loss: 0.5780 - val_accuracy: 0.8216\n",
      "Epoch 248/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.3027 - accuracy: 0.8372\n",
      "Epoch 00248: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 114us/sample - loss: 0.3029 - accuracy: 0.8369 - val_loss: 0.5754 - val_accuracy: 0.8189\n",
      "Epoch 249/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.3037 - accuracy: 0.8366\n",
      "Epoch 00249: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 112us/sample - loss: 0.3036 - accuracy: 0.8367 - val_loss: 0.5925 - val_accuracy: 0.8212\n",
      "Epoch 250/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.3051 - accuracy: 0.8361\n",
      "Epoch 00250: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 109us/sample - loss: 0.3046 - accuracy: 0.8363 - val_loss: 0.5676 - val_accuracy: 0.8166\n",
      "Epoch 251/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.3044 - accuracy: 0.8384\n",
      "Epoch 00251: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.3044 - accuracy: 0.8383 - val_loss: 0.5807 - val_accuracy: 0.8174\n",
      "Epoch 252/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.3047 - accuracy: 0.8354\n",
      "Epoch 00252: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.3044 - accuracy: 0.8355 - val_loss: 0.5804 - val_accuracy: 0.8189\n",
      "Epoch 253/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.3021 - accuracy: 0.8383\n",
      "Epoch 00253: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.3019 - accuracy: 0.8383 - val_loss: 0.6013 - val_accuracy: 0.8181\n",
      "Epoch 254/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.3037 - accuracy: 0.8372\n",
      "Epoch 00254: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.3048 - accuracy: 0.8370 - val_loss: 0.5519 - val_accuracy: 0.8150\n",
      "Epoch 255/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.3039 - accuracy: 0.8368\n",
      "Epoch 00255: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.3039 - accuracy: 0.8368 - val_loss: 0.5982 - val_accuracy: 0.8150\n",
      "Epoch 256/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.3030 - accuracy: 0.8367\n",
      "Epoch 00256: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.3035 - accuracy: 0.8364 - val_loss: 0.5806 - val_accuracy: 0.8150\n",
      "Epoch 257/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.3011 - accuracy: 0.8384\n",
      "Epoch 00257: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.3015 - accuracy: 0.8382 - val_loss: 0.5862 - val_accuracy: 0.8205\n",
      "Epoch 258/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.3037 - accuracy: 0.8364\n",
      "Epoch 00258: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.3033 - accuracy: 0.8366 - val_loss: 0.5782 - val_accuracy: 0.8131\n",
      "Epoch 259/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.3030 - accuracy: 0.8364\n",
      "Epoch 00259: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.3029 - accuracy: 0.8369 - val_loss: 0.6102 - val_accuracy: 0.8185\n",
      "Epoch 260/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3043 - accuracy: 0.8369\n",
      "Epoch 00260: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.3041 - accuracy: 0.8369 - val_loss: 0.5850 - val_accuracy: 0.8197\n",
      "Epoch 261/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.3038 - accuracy: 0.8361\n",
      "Epoch 00261: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.3039 - accuracy: 0.8362 - val_loss: 0.5685 - val_accuracy: 0.8181\n",
      "Epoch 262/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.3038 - accuracy: 0.8361\n",
      "Epoch 00262: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.3039 - accuracy: 0.8361 - val_loss: 0.5737 - val_accuracy: 0.8123\n",
      "Epoch 263/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.3035 - accuracy: 0.8379\n",
      "Epoch 00263: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.3035 - accuracy: 0.8380 - val_loss: 0.5642 - val_accuracy: 0.8147\n",
      "Epoch 264/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.3067 - accuracy: 0.8373\n",
      "Epoch 00264: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.3061 - accuracy: 0.8376 - val_loss: 0.5831 - val_accuracy: 0.8104\n",
      "Epoch 265/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.3018 - accuracy: 0.8370\n",
      "Epoch 00265: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.3017 - accuracy: 0.8369 - val_loss: 0.6013 - val_accuracy: 0.8181\n",
      "Epoch 266/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.3042 - accuracy: 0.8364\n",
      "Epoch 00266: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.3041 - accuracy: 0.8361 - val_loss: 0.6092 - val_accuracy: 0.8112\n",
      "Epoch 267/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.3031 - accuracy: 0.8379\n",
      "Epoch 00267: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.3031 - accuracy: 0.8379 - val_loss: 0.5949 - val_accuracy: 0.8193\n",
      "Epoch 268/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.3017 - accuracy: 0.8389\n",
      "Epoch 00268: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.3020 - accuracy: 0.8390 - val_loss: 0.5942 - val_accuracy: 0.8162\n",
      "Epoch 269/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.3030 - accuracy: 0.8374\n",
      "Epoch 00269: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.3026 - accuracy: 0.8376 - val_loss: 0.5872 - val_accuracy: 0.8205\n",
      "Epoch 270/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3025 - accuracy: 0.8364\n",
      "Epoch 00270: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.3025 - accuracy: 0.8364 - val_loss: 0.5779 - val_accuracy: 0.8174\n",
      "Epoch 271/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3020 - accuracy: 0.8359\n",
      "Epoch 00271: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.3019 - accuracy: 0.8360 - val_loss: 0.6059 - val_accuracy: 0.8174\n",
      "Epoch 272/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.3018 - accuracy: 0.8371\n",
      "Epoch 00272: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.3013 - accuracy: 0.8377 - val_loss: 0.5836 - val_accuracy: 0.8181\n",
      "Epoch 273/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3007 - accuracy: 0.8393\n",
      "Epoch 00273: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.3011 - accuracy: 0.8387 - val_loss: 0.5872 - val_accuracy: 0.8201\n",
      "Epoch 274/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3039 - accuracy: 0.8372\n",
      "Epoch 00274: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.3038 - accuracy: 0.8372 - val_loss: 0.5831 - val_accuracy: 0.8174\n",
      "Epoch 275/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.3031 - accuracy: 0.8389\n",
      "Epoch 00275: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.3031 - accuracy: 0.8388 - val_loss: 0.5788 - val_accuracy: 0.8054\n",
      "Epoch 276/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.3012 - accuracy: 0.8377\n",
      "Epoch 00276: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.3018 - accuracy: 0.8373 - val_loss: 0.5983 - val_accuracy: 0.8185\n",
      "Epoch 277/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.3032 - accuracy: 0.8371\n",
      "Epoch 00277: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.3032 - accuracy: 0.8370 - val_loss: 0.5822 - val_accuracy: 0.8077\n",
      "Epoch 278/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.3032 - accuracy: 0.8356\n",
      "Epoch 00278: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.3029 - accuracy: 0.8359 - val_loss: 0.5874 - val_accuracy: 0.8139\n",
      "Epoch 279/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.3019 - accuracy: 0.8382\n",
      "Epoch 00279: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.3022 - accuracy: 0.8382 - val_loss: 0.5829 - val_accuracy: 0.8170\n",
      "Epoch 280/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.3040 - accuracy: 0.8363\n",
      "Epoch 00280: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.3028 - accuracy: 0.8371 - val_loss: 0.5701 - val_accuracy: 0.8197\n",
      "Epoch 281/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.3017 - accuracy: 0.8371\n",
      "Epoch 00281: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.3014 - accuracy: 0.8376 - val_loss: 0.5864 - val_accuracy: 0.8042\n",
      "Epoch 282/1000\n",
      "22688/23208 [============================>.] - ETA: 0s - loss: 0.3019 - accuracy: 0.8368\n",
      "Epoch 00282: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.3020 - accuracy: 0.8365 - val_loss: 0.5881 - val_accuracy: 0.8185\n",
      "Epoch 283/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.3005 - accuracy: 0.8375\n",
      "Epoch 00283: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.3006 - accuracy: 0.8373 - val_loss: 0.5967 - val_accuracy: 0.8158\n",
      "Epoch 284/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.3016 - accuracy: 0.8395\n",
      "Epoch 00284: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.3020 - accuracy: 0.8393 - val_loss: 0.5952 - val_accuracy: 0.8166\n",
      "Epoch 285/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.3013 - accuracy: 0.8377\n",
      "Epoch 00285: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.3019 - accuracy: 0.8377 - val_loss: 0.6131 - val_accuracy: 0.8174\n",
      "Epoch 286/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.2989 - accuracy: 0.8396\n",
      "Epoch 00286: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2991 - accuracy: 0.8395 - val_loss: 0.5995 - val_accuracy: 0.8131\n",
      "Epoch 287/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.3046 - accuracy: 0.8361\n",
      "Epoch 00287: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.3047 - accuracy: 0.8361 - val_loss: 0.5867 - val_accuracy: 0.8162\n",
      "Epoch 288/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.3049 - accuracy: 0.8383\n",
      "Epoch 00288: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.3048 - accuracy: 0.8384 - val_loss: 0.5812 - val_accuracy: 0.8189\n",
      "Epoch 289/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3025 - accuracy: 0.8382\n",
      "Epoch 00289: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.3027 - accuracy: 0.8381 - val_loss: 0.5855 - val_accuracy: 0.8185\n",
      "Epoch 290/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3013 - accuracy: 0.8377\n",
      "Epoch 00290: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.3015 - accuracy: 0.8377 - val_loss: 0.5946 - val_accuracy: 0.8116\n",
      "Epoch 291/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.3039 - accuracy: 0.8365\n",
      "Epoch 00291: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 122us/sample - loss: 0.3044 - accuracy: 0.8367 - val_loss: 0.5607 - val_accuracy: 0.8220\n",
      "Epoch 292/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.3027 - accuracy: 0.8372\n",
      "Epoch 00292: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 141us/sample - loss: 0.3024 - accuracy: 0.8372 - val_loss: 0.6007 - val_accuracy: 0.8166\n",
      "Epoch 293/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.3003 - accuracy: 0.8370\n",
      "Epoch 00293: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 133us/sample - loss: 0.3000 - accuracy: 0.8372 - val_loss: 0.5933 - val_accuracy: 0.8185\n",
      "Epoch 294/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.3034 - accuracy: 0.8361\n",
      "Epoch 00294: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 4s 152us/sample - loss: 0.3034 - accuracy: 0.8361 - val_loss: 0.5718 - val_accuracy: 0.8193\n",
      "Epoch 295/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.3003 - accuracy: 0.8385\n",
      "Epoch 00295: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 4s 153us/sample - loss: 0.3002 - accuracy: 0.8388 - val_loss: 0.5984 - val_accuracy: 0.8108\n",
      "Epoch 296/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.3024 - accuracy: 0.8368\n",
      "Epoch 00296: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 144us/sample - loss: 0.3023 - accuracy: 0.8370 - val_loss: 0.6041 - val_accuracy: 0.8205\n",
      "Epoch 297/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.3015 - accuracy: 0.8387\n",
      "Epoch 00297: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 142us/sample - loss: 0.3009 - accuracy: 0.8389 - val_loss: 0.5867 - val_accuracy: 0.8185\n",
      "Epoch 298/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.3018 - accuracy: 0.8381\n",
      "Epoch 00298: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 130us/sample - loss: 0.3015 - accuracy: 0.8382 - val_loss: 0.6027 - val_accuracy: 0.8147\n",
      "Epoch 299/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.3027 - accuracy: 0.8364\n",
      "Epoch 00299: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 145us/sample - loss: 0.3027 - accuracy: 0.8365 - val_loss: 0.5934 - val_accuracy: 0.8123\n",
      "Epoch 300/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.3002 - accuracy: 0.8399\n",
      "Epoch 00300: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 150us/sample - loss: 0.3004 - accuracy: 0.8398 - val_loss: 0.6161 - val_accuracy: 0.8127\n",
      "Epoch 301/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.3029 - accuracy: 0.8376\n",
      "Epoch 00301: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 4s 155us/sample - loss: 0.3026 - accuracy: 0.8377 - val_loss: 0.5832 - val_accuracy: 0.8127\n",
      "Epoch 302/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2999 - accuracy: 0.8386\n",
      "Epoch 00302: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 146us/sample - loss: 0.2997 - accuracy: 0.8392 - val_loss: 0.6087 - val_accuracy: 0.8065\n",
      "Epoch 303/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.3049 - accuracy: 0.8354\n",
      "Epoch 00303: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 115us/sample - loss: 0.3050 - accuracy: 0.8354 - val_loss: 0.5592 - val_accuracy: 0.8135\n",
      "Epoch 304/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3020 - accuracy: 0.8375\n",
      "Epoch 00304: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.3019 - accuracy: 0.8376 - val_loss: 0.6095 - val_accuracy: 0.8181\n",
      "Epoch 305/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.3003 - accuracy: 0.8386\n",
      "Epoch 00305: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 107us/sample - loss: 0.3003 - accuracy: 0.8386 - val_loss: 0.5944 - val_accuracy: 0.8147\n",
      "Epoch 306/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.3018 - accuracy: 0.8391\n",
      "Epoch 00306: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.3013 - accuracy: 0.8388 - val_loss: 0.5872 - val_accuracy: 0.8150\n",
      "Epoch 307/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.3013 - accuracy: 0.8390\n",
      "Epoch 00307: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.3013 - accuracy: 0.8388 - val_loss: 0.5915 - val_accuracy: 0.8143\n",
      "Epoch 308/1000\n",
      "22688/23208 [============================>.] - ETA: 0s - loss: 0.3020 - accuracy: 0.8378\n",
      "Epoch 00308: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.3018 - accuracy: 0.8379 - val_loss: 0.5739 - val_accuracy: 0.8119\n",
      "Epoch 309/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.3026 - accuracy: 0.8384\n",
      "Epoch 00309: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 140us/sample - loss: 0.3025 - accuracy: 0.8385 - val_loss: 0.5825 - val_accuracy: 0.8135\n",
      "Epoch 310/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.3008 - accuracy: 0.8380\n",
      "Epoch 00310: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 149us/sample - loss: 0.3009 - accuracy: 0.8378 - val_loss: 0.5695 - val_accuracy: 0.8216\n",
      "Epoch 311/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2993 - accuracy: 0.8393\n",
      "Epoch 00311: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 3s 127us/sample - loss: 0.2987 - accuracy: 0.8397 - val_loss: 0.5862 - val_accuracy: 0.8189\n",
      "Epoch 312/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.3011 - accuracy: 0.8368\n",
      "Epoch 00312: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.3014 - accuracy: 0.8365 - val_loss: 0.6141 - val_accuracy: 0.8073\n",
      "Epoch 313/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.3012 - accuracy: 0.8384\n",
      "Epoch 00313: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.3011 - accuracy: 0.8385 - val_loss: 0.5850 - val_accuracy: 0.8123\n",
      "Epoch 314/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.3017 - accuracy: 0.8359\n",
      "Epoch 00314: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.3012 - accuracy: 0.8363 - val_loss: 0.5542 - val_accuracy: 0.8162\n",
      "Epoch 315/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.3001 - accuracy: 0.8368\n",
      "Epoch 00315: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.3001 - accuracy: 0.8370 - val_loss: 0.5946 - val_accuracy: 0.8147\n",
      "Epoch 316/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.3015 - accuracy: 0.8372\n",
      "Epoch 00316: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.3017 - accuracy: 0.8372 - val_loss: 0.5969 - val_accuracy: 0.8162\n",
      "Epoch 317/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.3005 - accuracy: 0.8383\n",
      "Epoch 00317: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 107us/sample - loss: 0.3010 - accuracy: 0.8379 - val_loss: 0.5982 - val_accuracy: 0.8166\n",
      "Epoch 318/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.3037 - accuracy: 0.8383\n",
      "Epoch 00318: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.3036 - accuracy: 0.8385 - val_loss: 0.5699 - val_accuracy: 0.8170\n",
      "Epoch 319/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.3008 - accuracy: 0.8387\n",
      "Epoch 00319: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.3006 - accuracy: 0.8387 - val_loss: 0.6049 - val_accuracy: 0.8131\n",
      "Epoch 320/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.2993 - accuracy: 0.8390\n",
      "Epoch 00320: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2992 - accuracy: 0.8389 - val_loss: 0.6130 - val_accuracy: 0.8158\n",
      "Epoch 321/1000\n",
      "22688/23208 [============================>.] - ETA: 0s - loss: 0.3017 - accuracy: 0.8384\n",
      "Epoch 00321: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.3011 - accuracy: 0.8385 - val_loss: 0.5987 - val_accuracy: 0.8147\n",
      "Epoch 322/1000\n",
      "22688/23208 [============================>.] - ETA: 0s - loss: 0.3022 - accuracy: 0.8378\n",
      "Epoch 00322: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.3019 - accuracy: 0.8380 - val_loss: 0.5771 - val_accuracy: 0.8170\n",
      "Epoch 323/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.3021 - accuracy: 0.8363\n",
      "Epoch 00323: val_accuracy did not improve from 0.82319\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.3021 - accuracy: 0.8361 - val_loss: 0.6020 - val_accuracy: 0.8131\n",
      "Epoch 324/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.3007 - accuracy: 0.8384\n",
      "Epoch 00324: val_accuracy improved from 0.82319 to 0.82513, saving model to weights-improvement-324-0.83.hdf5\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2997 - accuracy: 0.8388 - val_loss: 0.6120 - val_accuracy: 0.8251\n",
      "Epoch 325/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.3008 - accuracy: 0.8387\n",
      "Epoch 00325: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 108us/sample - loss: 0.3005 - accuracy: 0.8388 - val_loss: 0.6162 - val_accuracy: 0.8077\n",
      "Epoch 326/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.3008 - accuracy: 0.8383\n",
      "Epoch 00326: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.3013 - accuracy: 0.8381 - val_loss: 0.5846 - val_accuracy: 0.8116\n",
      "Epoch 327/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.3000 - accuracy: 0.8375\n",
      "Epoch 00327: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.3003 - accuracy: 0.8373 - val_loss: 0.5750 - val_accuracy: 0.8181\n",
      "Epoch 328/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.3008 - accuracy: 0.8372\n",
      "Epoch 00328: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.3007 - accuracy: 0.8373 - val_loss: 0.5899 - val_accuracy: 0.8170\n",
      "Epoch 329/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.2986 - accuracy: 0.8382\n",
      "Epoch 00329: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2990 - accuracy: 0.8379 - val_loss: 0.6149 - val_accuracy: 0.8197\n",
      "Epoch 330/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2986 - accuracy: 0.8387\n",
      "Epoch 00330: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2986 - accuracy: 0.8387 - val_loss: 0.6015 - val_accuracy: 0.8150\n",
      "Epoch 331/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.2993 - accuracy: 0.8383\n",
      "Epoch 00331: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 4s 152us/sample - loss: 0.2992 - accuracy: 0.8385 - val_loss: 0.5889 - val_accuracy: 0.8174\n",
      "Epoch 332/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.3007 - accuracy: 0.8372\n",
      "Epoch 00332: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 4s 166us/sample - loss: 0.3005 - accuracy: 0.8376 - val_loss: 0.6228 - val_accuracy: 0.8085\n",
      "Epoch 333/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.3012 - accuracy: 0.8380\n",
      "Epoch 00333: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 4s 155us/sample - loss: 0.3010 - accuracy: 0.8385 - val_loss: 0.6131 - val_accuracy: 0.8197\n",
      "Epoch 334/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.3018 - accuracy: 0.8385\n",
      "Epoch 00334: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 150us/sample - loss: 0.3013 - accuracy: 0.8389 - val_loss: 0.6018 - val_accuracy: 0.8150\n",
      "Epoch 335/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.3028 - accuracy: 0.8370\n",
      "Epoch 00335: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 118us/sample - loss: 0.3028 - accuracy: 0.8369 - val_loss: 0.5965 - val_accuracy: 0.8166\n",
      "Epoch 336/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.3004 - accuracy: 0.8392\n",
      "Epoch 00336: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 120us/sample - loss: 0.3005 - accuracy: 0.8391 - val_loss: 0.6051 - val_accuracy: 0.8061\n",
      "Epoch 337/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2990 - accuracy: 0.8375\n",
      "Epoch 00337: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 112us/sample - loss: 0.2991 - accuracy: 0.8375 - val_loss: 0.6162 - val_accuracy: 0.8174\n",
      "Epoch 338/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.3002 - accuracy: 0.8376\n",
      "Epoch 00338: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 107us/sample - loss: 0.3002 - accuracy: 0.8378 - val_loss: 0.5873 - val_accuracy: 0.8150\n",
      "Epoch 339/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2998 - accuracy: 0.8385\n",
      "Epoch 00339: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2995 - accuracy: 0.8386 - val_loss: 0.5963 - val_accuracy: 0.8127\n",
      "Epoch 340/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.3013 - accuracy: 0.8384\n",
      "Epoch 00340: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.3015 - accuracy: 0.8379 - val_loss: 0.5684 - val_accuracy: 0.8108\n",
      "Epoch 341/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.3007 - accuracy: 0.8384\n",
      "Epoch 00341: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.3007 - accuracy: 0.8382 - val_loss: 0.5985 - val_accuracy: 0.8143\n",
      "Epoch 342/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.2998 - accuracy: 0.8395\n",
      "Epoch 00342: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2999 - accuracy: 0.8396 - val_loss: 0.6277 - val_accuracy: 0.8034\n",
      "Epoch 343/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.3003 - accuracy: 0.8388\n",
      "Epoch 00343: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 107us/sample - loss: 0.3007 - accuracy: 0.8390 - val_loss: 0.5991 - val_accuracy: 0.8147\n",
      "Epoch 344/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.3003 - accuracy: 0.8380\n",
      "Epoch 00344: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.3003 - accuracy: 0.8379 - val_loss: 0.6239 - val_accuracy: 0.8178\n",
      "Epoch 345/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.2995 - accuracy: 0.8392\n",
      "Epoch 00345: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2992 - accuracy: 0.8393 - val_loss: 0.6203 - val_accuracy: 0.8154\n",
      "Epoch 346/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.2995 - accuracy: 0.8394\n",
      "Epoch 00346: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2996 - accuracy: 0.8393 - val_loss: 0.6259 - val_accuracy: 0.8216\n",
      "Epoch 347/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.3032 - accuracy: 0.8380\n",
      "Epoch 00347: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.3031 - accuracy: 0.8382 - val_loss: 0.5945 - val_accuracy: 0.8197\n",
      "Epoch 348/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.2992 - accuracy: 0.8406\n",
      "Epoch 00348: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 107us/sample - loss: 0.2992 - accuracy: 0.8405 - val_loss: 0.6210 - val_accuracy: 0.8212\n",
      "Epoch 349/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3004 - accuracy: 0.8361\n",
      "Epoch 00349: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 147us/sample - loss: 0.3000 - accuracy: 0.8367 - val_loss: 0.6021 - val_accuracy: 0.8150\n",
      "Epoch 350/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2998 - accuracy: 0.8391\n",
      "Epoch 00350: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 139us/sample - loss: 0.2999 - accuracy: 0.8390 - val_loss: 0.6284 - val_accuracy: 0.8135\n",
      "Epoch 351/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.3013 - accuracy: 0.8384\n",
      "Epoch 00351: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 107us/sample - loss: 0.3013 - accuracy: 0.8385 - val_loss: 0.5887 - val_accuracy: 0.8170\n",
      "Epoch 352/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3000 - accuracy: 0.8392\n",
      "Epoch 00352: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 118us/sample - loss: 0.3001 - accuracy: 0.8390 - val_loss: 0.5853 - val_accuracy: 0.8081\n",
      "Epoch 353/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.2982 - accuracy: 0.8399\n",
      "Epoch 00353: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 139us/sample - loss: 0.2981 - accuracy: 0.8398 - val_loss: 0.6044 - val_accuracy: 0.8127\n",
      "Epoch 354/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.3001 - accuracy: 0.8381\n",
      "Epoch 00354: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2999 - accuracy: 0.8382 - val_loss: 0.6128 - val_accuracy: 0.8104\n",
      "Epoch 355/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.2995 - accuracy: 0.8392\n",
      "Epoch 00355: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2994 - accuracy: 0.8391 - val_loss: 0.5955 - val_accuracy: 0.8104\n",
      "Epoch 356/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.3005 - accuracy: 0.8390\n",
      "Epoch 00356: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.3003 - accuracy: 0.8392 - val_loss: 0.6042 - val_accuracy: 0.8154\n",
      "Epoch 357/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3023 - accuracy: 0.8393\n",
      "Epoch 00357: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.3022 - accuracy: 0.8393 - val_loss: 0.5919 - val_accuracy: 0.8131\n",
      "Epoch 358/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.2992 - accuracy: 0.8390\n",
      "Epoch 00358: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2991 - accuracy: 0.8394 - val_loss: 0.6029 - val_accuracy: 0.8185\n",
      "Epoch 359/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2993 - accuracy: 0.8395\n",
      "Epoch 00359: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2994 - accuracy: 0.8395 - val_loss: 0.5880 - val_accuracy: 0.8174\n",
      "Epoch 360/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.3001 - accuracy: 0.8387\n",
      "Epoch 00360: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.2998 - accuracy: 0.8388 - val_loss: 0.5825 - val_accuracy: 0.8178\n",
      "Epoch 361/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.3009 - accuracy: 0.8390\n",
      "Epoch 00361: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.3000 - accuracy: 0.8395 - val_loss: 0.6018 - val_accuracy: 0.8166\n",
      "Epoch 362/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2983 - accuracy: 0.8403\n",
      "Epoch 00362: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2984 - accuracy: 0.8401 - val_loss: 0.6109 - val_accuracy: 0.8166\n",
      "Epoch 363/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.3006 - accuracy: 0.8390\n",
      "Epoch 00363: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.3006 - accuracy: 0.8388 - val_loss: 0.6153 - val_accuracy: 0.8174\n",
      "Epoch 364/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.3024 - accuracy: 0.8373\n",
      "Epoch 00364: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.3022 - accuracy: 0.8376 - val_loss: 0.5917 - val_accuracy: 0.8158\n",
      "Epoch 365/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.3002 - accuracy: 0.8383\n",
      "Epoch 00365: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 115us/sample - loss: 0.3007 - accuracy: 0.8378 - val_loss: 0.5852 - val_accuracy: 0.8154\n",
      "Epoch 366/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2987 - accuracy: 0.8382\n",
      "Epoch 00366: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 142us/sample - loss: 0.2989 - accuracy: 0.8382 - val_loss: 0.6233 - val_accuracy: 0.8108\n",
      "Epoch 367/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2985 - accuracy: 0.8389\n",
      "Epoch 00367: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 143us/sample - loss: 0.2982 - accuracy: 0.8392 - val_loss: 0.5842 - val_accuracy: 0.8185\n",
      "Epoch 368/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2982 - accuracy: 0.8387\n",
      "Epoch 00368: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 146us/sample - loss: 0.2983 - accuracy: 0.8388 - val_loss: 0.6104 - val_accuracy: 0.8178\n",
      "Epoch 369/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2983 - accuracy: 0.8396\n",
      "Epoch 00369: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 145us/sample - loss: 0.2982 - accuracy: 0.8398 - val_loss: 0.6224 - val_accuracy: 0.8112\n",
      "Epoch 370/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.2992 - accuracy: 0.8390\n",
      "Epoch 00370: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 139us/sample - loss: 0.2993 - accuracy: 0.8387 - val_loss: 0.5538 - val_accuracy: 0.8131\n",
      "Epoch 371/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2975 - accuracy: 0.8411\n",
      "Epoch 00371: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2975 - accuracy: 0.8410 - val_loss: 0.5801 - val_accuracy: 0.8181\n",
      "Epoch 372/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.2990 - accuracy: 0.8373\n",
      "Epoch 00372: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 141us/sample - loss: 0.2987 - accuracy: 0.8377 - val_loss: 0.6073 - val_accuracy: 0.8119\n",
      "Epoch 373/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2993 - accuracy: 0.8391\n",
      "Epoch 00373: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 147us/sample - loss: 0.2991 - accuracy: 0.8390 - val_loss: 0.5529 - val_accuracy: 0.8034\n",
      "Epoch 374/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.2984 - accuracy: 0.8376\n",
      "Epoch 00374: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 146us/sample - loss: 0.2983 - accuracy: 0.8378 - val_loss: 0.6161 - val_accuracy: 0.8123\n",
      "Epoch 375/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.2993 - accuracy: 0.8358\n",
      "Epoch 00375: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 112us/sample - loss: 0.2992 - accuracy: 0.8360 - val_loss: 0.5980 - val_accuracy: 0.8158\n",
      "Epoch 376/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3000 - accuracy: 0.8389\n",
      "Epoch 00376: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.3002 - accuracy: 0.8388 - val_loss: 0.6264 - val_accuracy: 0.8131\n",
      "Epoch 377/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2991 - accuracy: 0.8382\n",
      "Epoch 00377: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2997 - accuracy: 0.8378 - val_loss: 0.6130 - val_accuracy: 0.8181\n",
      "Epoch 378/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2989 - accuracy: 0.8393\n",
      "Epoch 00378: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2986 - accuracy: 0.8395 - val_loss: 0.6114 - val_accuracy: 0.8135\n",
      "Epoch 379/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.2970 - accuracy: 0.8402\n",
      "Epoch 00379: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 107us/sample - loss: 0.2968 - accuracy: 0.8404 - val_loss: 0.6308 - val_accuracy: 0.8081\n",
      "Epoch 380/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2982 - accuracy: 0.8376\n",
      "Epoch 00380: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 146us/sample - loss: 0.2987 - accuracy: 0.8377 - val_loss: 0.6325 - val_accuracy: 0.8158\n",
      "Epoch 381/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.3008 - accuracy: 0.8389\n",
      "Epoch 00381: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 119us/sample - loss: 0.3004 - accuracy: 0.8389 - val_loss: 0.6054 - val_accuracy: 0.8147\n",
      "Epoch 382/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.2988 - accuracy: 0.8398\n",
      "Epoch 00382: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2998 - accuracy: 0.8391 - val_loss: 0.6267 - val_accuracy: 0.8143\n",
      "Epoch 383/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.3011 - accuracy: 0.8380\n",
      "Epoch 00383: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.3012 - accuracy: 0.8379 - val_loss: 0.6000 - val_accuracy: 0.8143\n",
      "Epoch 384/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2977 - accuracy: 0.8399\n",
      "Epoch 00384: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2978 - accuracy: 0.8399 - val_loss: 0.6173 - val_accuracy: 0.8162\n",
      "Epoch 385/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2983 - accuracy: 0.8400\n",
      "Epoch 00385: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2983 - accuracy: 0.8401 - val_loss: 0.6537 - val_accuracy: 0.8116\n",
      "Epoch 386/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.2986 - accuracy: 0.8390\n",
      "Epoch 00386: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2983 - accuracy: 0.8389 - val_loss: 0.6408 - val_accuracy: 0.8065\n",
      "Epoch 387/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2984 - accuracy: 0.8381\n",
      "Epoch 00387: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2983 - accuracy: 0.8382 - val_loss: 0.6575 - val_accuracy: 0.8108\n",
      "Epoch 388/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2995 - accuracy: 0.8392\n",
      "Epoch 00388: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2997 - accuracy: 0.8393 - val_loss: 0.6030 - val_accuracy: 0.8147\n",
      "Epoch 389/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2998 - accuracy: 0.8382\n",
      "Epoch 00389: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.3000 - accuracy: 0.8381 - val_loss: 0.6061 - val_accuracy: 0.8193\n",
      "Epoch 390/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.2971 - accuracy: 0.8413\n",
      "Epoch 00390: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2975 - accuracy: 0.8407 - val_loss: 0.6214 - val_accuracy: 0.8209\n",
      "Epoch 391/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.2976 - accuracy: 0.8391\n",
      "Epoch 00391: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2977 - accuracy: 0.8390 - val_loss: 0.6367 - val_accuracy: 0.8127\n",
      "Epoch 392/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.2975 - accuracy: 0.8395\n",
      "Epoch 00392: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2976 - accuracy: 0.8396 - val_loss: 0.6236 - val_accuracy: 0.8154\n",
      "Epoch 393/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2985 - accuracy: 0.8394\n",
      "Epoch 00393: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2985 - accuracy: 0.8394 - val_loss: 0.6254 - val_accuracy: 0.8147\n",
      "Epoch 394/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.2993 - accuracy: 0.8393\n",
      "Epoch 00394: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2997 - accuracy: 0.8392 - val_loss: 0.5775 - val_accuracy: 0.8189\n",
      "Epoch 395/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2993 - accuracy: 0.8399\n",
      "Epoch 00395: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2995 - accuracy: 0.8396 - val_loss: 0.6431 - val_accuracy: 0.8104\n",
      "Epoch 396/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.3024 - accuracy: 0.8387\n",
      "Epoch 00396: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.3025 - accuracy: 0.8386 - val_loss: 0.6162 - val_accuracy: 0.8158\n",
      "Epoch 397/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.2977 - accuracy: 0.8393\n",
      "Epoch 00397: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2981 - accuracy: 0.8393 - val_loss: 0.5953 - val_accuracy: 0.8116\n",
      "Epoch 398/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.2965 - accuracy: 0.8396\n",
      "Epoch 00398: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2965 - accuracy: 0.8395 - val_loss: 0.6146 - val_accuracy: 0.8119\n",
      "Epoch 399/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2971 - accuracy: 0.8413\n",
      "Epoch 00399: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2976 - accuracy: 0.8409 - val_loss: 0.6566 - val_accuracy: 0.8178\n",
      "Epoch 400/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.2984 - accuracy: 0.8410\n",
      "Epoch 00400: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2984 - accuracy: 0.8410 - val_loss: 0.6157 - val_accuracy: 0.8104\n",
      "Epoch 401/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.2988 - accuracy: 0.8415\n",
      "Epoch 00401: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2987 - accuracy: 0.8416 - val_loss: 0.5864 - val_accuracy: 0.8158\n",
      "Epoch 402/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.3013 - accuracy: 0.8388\n",
      "Epoch 00402: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 109us/sample - loss: 0.3009 - accuracy: 0.8390 - val_loss: 0.5821 - val_accuracy: 0.8096\n",
      "Epoch 403/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.2983 - accuracy: 0.8395\n",
      "Epoch 00403: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 142us/sample - loss: 0.2985 - accuracy: 0.8395 - val_loss: 0.6290 - val_accuracy: 0.8123\n",
      "Epoch 404/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2977 - accuracy: 0.8381\n",
      "Epoch 00404: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 144us/sample - loss: 0.2980 - accuracy: 0.8382 - val_loss: 0.6153 - val_accuracy: 0.8116\n",
      "Epoch 405/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2983 - accuracy: 0.8391\n",
      "Epoch 00405: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 150us/sample - loss: 0.2980 - accuracy: 0.8391 - val_loss: 0.6038 - val_accuracy: 0.8178\n",
      "Epoch 406/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.2976 - accuracy: 0.8384\n",
      "Epoch 00406: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 119us/sample - loss: 0.2975 - accuracy: 0.8384 - val_loss: 0.6345 - val_accuracy: 0.8104\n",
      "Epoch 407/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3026 - accuracy: 0.8372\n",
      "Epoch 00407: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.3017 - accuracy: 0.8373 - val_loss: 0.6136 - val_accuracy: 0.8131\n",
      "Epoch 408/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2978 - accuracy: 0.8410\n",
      "Epoch 00408: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2984 - accuracy: 0.8407 - val_loss: 0.6223 - val_accuracy: 0.8108\n",
      "Epoch 409/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.2975 - accuracy: 0.8386\n",
      "Epoch 00409: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2974 - accuracy: 0.8388 - val_loss: 0.6024 - val_accuracy: 0.8166\n",
      "Epoch 410/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2994 - accuracy: 0.8381\n",
      "Epoch 00410: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2998 - accuracy: 0.8378 - val_loss: 0.5824 - val_accuracy: 0.8166\n",
      "Epoch 411/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2976 - accuracy: 0.8403\n",
      "Epoch 00411: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2977 - accuracy: 0.8402 - val_loss: 0.5961 - val_accuracy: 0.8065\n",
      "Epoch 412/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.2989 - accuracy: 0.8391\n",
      "Epoch 00412: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2985 - accuracy: 0.8392 - val_loss: 0.5905 - val_accuracy: 0.7995\n",
      "Epoch 413/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2963 - accuracy: 0.8385\n",
      "Epoch 00413: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2962 - accuracy: 0.8385 - val_loss: 0.6376 - val_accuracy: 0.8162\n",
      "Epoch 414/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2985 - accuracy: 0.8387\n",
      "Epoch 00414: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2985 - accuracy: 0.8387 - val_loss: 0.6160 - val_accuracy: 0.8147\n",
      "Epoch 415/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.3010 - accuracy: 0.8373\n",
      "Epoch 00415: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.3009 - accuracy: 0.8373 - val_loss: 0.6204 - val_accuracy: 0.8100\n",
      "Epoch 416/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.2995 - accuracy: 0.8373\n",
      "Epoch 00416: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2993 - accuracy: 0.8374 - val_loss: 0.5701 - val_accuracy: 0.8185\n",
      "Epoch 417/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2943 - accuracy: 0.8412\n",
      "Epoch 00417: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2943 - accuracy: 0.8409 - val_loss: 0.6244 - val_accuracy: 0.8185\n",
      "Epoch 418/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2957 - accuracy: 0.8400\n",
      "Epoch 00418: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2960 - accuracy: 0.8399 - val_loss: 0.6157 - val_accuracy: 0.8166\n",
      "Epoch 419/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2953 - accuracy: 0.8405\n",
      "Epoch 00419: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2956 - accuracy: 0.8405 - val_loss: 0.6187 - val_accuracy: 0.8100\n",
      "Epoch 420/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2973 - accuracy: 0.8395\n",
      "Epoch 00420: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2974 - accuracy: 0.8395 - val_loss: 0.6330 - val_accuracy: 0.8135\n",
      "Epoch 421/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.2998 - accuracy: 0.8391\n",
      "Epoch 00421: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2999 - accuracy: 0.8392 - val_loss: 0.6010 - val_accuracy: 0.8154\n",
      "Epoch 422/1000\n",
      "22688/23208 [============================>.] - ETA: 0s - loss: 0.2959 - accuracy: 0.8388\n",
      "Epoch 00422: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2968 - accuracy: 0.8387 - val_loss: 0.6158 - val_accuracy: 0.8181\n",
      "Epoch 423/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2973 - accuracy: 0.8410\n",
      "Epoch 00423: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2972 - accuracy: 0.8410 - val_loss: 0.6133 - val_accuracy: 0.8174\n",
      "Epoch 424/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2979 - accuracy: 0.8402\n",
      "Epoch 00424: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2981 - accuracy: 0.8401 - val_loss: 0.5866 - val_accuracy: 0.8147\n",
      "Epoch 425/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.2974 - accuracy: 0.8397\n",
      "Epoch 00425: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2976 - accuracy: 0.8395 - val_loss: 0.6042 - val_accuracy: 0.8162\n",
      "Epoch 426/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.2974 - accuracy: 0.8396\n",
      "Epoch 00426: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2974 - accuracy: 0.8398 - val_loss: 0.6154 - val_accuracy: 0.8085\n",
      "Epoch 427/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.2985 - accuracy: 0.8392\n",
      "Epoch 00427: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2986 - accuracy: 0.8395 - val_loss: 0.5942 - val_accuracy: 0.8166\n",
      "Epoch 428/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.2967 - accuracy: 0.8399\n",
      "Epoch 00428: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2967 - accuracy: 0.8398 - val_loss: 0.6402 - val_accuracy: 0.8127\n",
      "Epoch 429/1000\n",
      "22688/23208 [============================>.] - ETA: 0s - loss: 0.3008 - accuracy: 0.8381\n",
      "Epoch 00429: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.3001 - accuracy: 0.8388 - val_loss: 0.6333 - val_accuracy: 0.8162\n",
      "Epoch 430/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.2954 - accuracy: 0.8406\n",
      "Epoch 00430: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2961 - accuracy: 0.8406 - val_loss: 0.6067 - val_accuracy: 0.8154\n",
      "Epoch 431/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.2991 - accuracy: 0.8391\n",
      "Epoch 00431: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2989 - accuracy: 0.8392 - val_loss: 0.6392 - val_accuracy: 0.8050\n",
      "Epoch 432/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2984 - accuracy: 0.8384\n",
      "Epoch 00432: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2985 - accuracy: 0.8380 - val_loss: 0.5875 - val_accuracy: 0.8100\n",
      "Epoch 433/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.2988 - accuracy: 0.8396\n",
      "Epoch 00433: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 112us/sample - loss: 0.2987 - accuracy: 0.8396 - val_loss: 0.6043 - val_accuracy: 0.8127\n",
      "Epoch 434/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.2990 - accuracy: 0.8390\n",
      "Epoch 00434: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2991 - accuracy: 0.8390 - val_loss: 0.6247 - val_accuracy: 0.8085\n",
      "Epoch 435/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.2979 - accuracy: 0.8404\n",
      "Epoch 00435: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2976 - accuracy: 0.8406 - val_loss: 0.6447 - val_accuracy: 0.8166\n",
      "Epoch 436/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.2954 - accuracy: 0.8403\n",
      "Epoch 00436: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2955 - accuracy: 0.8399 - val_loss: 0.6294 - val_accuracy: 0.8127\n",
      "Epoch 437/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.2996 - accuracy: 0.8377\n",
      "Epoch 00437: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2993 - accuracy: 0.8381 - val_loss: 0.6409 - val_accuracy: 0.8150\n",
      "Epoch 438/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2966 - accuracy: 0.8392\n",
      "Epoch 00438: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2966 - accuracy: 0.8392 - val_loss: 0.6345 - val_accuracy: 0.8150\n",
      "Epoch 439/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.2975 - accuracy: 0.8380\n",
      "Epoch 00439: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2974 - accuracy: 0.8380 - val_loss: 0.6102 - val_accuracy: 0.8131\n",
      "Epoch 440/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.2966 - accuracy: 0.8401\n",
      "Epoch 00440: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2966 - accuracy: 0.8402 - val_loss: 0.6442 - val_accuracy: 0.8139\n",
      "Epoch 441/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.2978 - accuracy: 0.8386\n",
      "Epoch 00441: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2980 - accuracy: 0.8387 - val_loss: 0.6630 - val_accuracy: 0.8139\n",
      "Epoch 442/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.3010 - accuracy: 0.8406\n",
      "Epoch 00442: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.3008 - accuracy: 0.8406 - val_loss: 0.6379 - val_accuracy: 0.8166\n",
      "Epoch 443/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.2985 - accuracy: 0.8388\n",
      "Epoch 00443: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2980 - accuracy: 0.8393 - val_loss: 0.6248 - val_accuracy: 0.8135\n",
      "Epoch 444/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.2958 - accuracy: 0.8405\n",
      "Epoch 00444: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2956 - accuracy: 0.8405 - val_loss: 0.6575 - val_accuracy: 0.8127\n",
      "Epoch 445/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2979 - accuracy: 0.8388\n",
      "Epoch 00445: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2997 - accuracy: 0.8383 - val_loss: 0.6144 - val_accuracy: 0.8154\n",
      "Epoch 446/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.2972 - accuracy: 0.8402\n",
      "Epoch 00446: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2974 - accuracy: 0.8401 - val_loss: 0.6522 - val_accuracy: 0.8088\n",
      "Epoch 447/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.2967 - accuracy: 0.8387\n",
      "Epoch 00447: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2964 - accuracy: 0.8392 - val_loss: 0.6355 - val_accuracy: 0.8193\n",
      "Epoch 448/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2946 - accuracy: 0.8412\n",
      "Epoch 00448: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2951 - accuracy: 0.8409 - val_loss: 0.6263 - val_accuracy: 0.8147\n",
      "Epoch 449/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.2981 - accuracy: 0.8388\n",
      "Epoch 00449: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2984 - accuracy: 0.8382 - val_loss: 0.6354 - val_accuracy: 0.8116\n",
      "Epoch 450/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.2971 - accuracy: 0.8402\n",
      "Epoch 00450: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2971 - accuracy: 0.8402 - val_loss: 0.5898 - val_accuracy: 0.8174\n",
      "Epoch 451/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2963 - accuracy: 0.8414\n",
      "Epoch 00451: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2960 - accuracy: 0.8413 - val_loss: 0.6148 - val_accuracy: 0.8085\n",
      "Epoch 452/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.2973 - accuracy: 0.8404\n",
      "Epoch 00452: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2979 - accuracy: 0.8400 - val_loss: 0.6144 - val_accuracy: 0.8189\n",
      "Epoch 453/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2965 - accuracy: 0.8406\n",
      "Epoch 00453: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2966 - accuracy: 0.8405 - val_loss: 0.6161 - val_accuracy: 0.8147\n",
      "Epoch 454/1000\n",
      "22688/23208 [============================>.] - ETA: 0s - loss: 0.2954 - accuracy: 0.8414\n",
      "Epoch 00454: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2957 - accuracy: 0.8410 - val_loss: 0.6146 - val_accuracy: 0.8127\n",
      "Epoch 455/1000\n",
      "22688/23208 [============================>.] - ETA: 0s - loss: 0.2971 - accuracy: 0.8411\n",
      "Epoch 00455: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2965 - accuracy: 0.8413 - val_loss: 0.6278 - val_accuracy: 0.8135\n",
      "Epoch 456/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.2949 - accuracy: 0.8411\n",
      "Epoch 00456: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2949 - accuracy: 0.8409 - val_loss: 0.6649 - val_accuracy: 0.8092\n",
      "Epoch 457/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2979 - accuracy: 0.8398\n",
      "Epoch 00457: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2984 - accuracy: 0.8394 - val_loss: 0.6260 - val_accuracy: 0.8123\n",
      "Epoch 458/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.2961 - accuracy: 0.8402\n",
      "Epoch 00458: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2963 - accuracy: 0.8401 - val_loss: 0.6249 - val_accuracy: 0.8178\n",
      "Epoch 459/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.2988 - accuracy: 0.8374\n",
      "Epoch 00459: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 108us/sample - loss: 0.2986 - accuracy: 0.8373 - val_loss: 0.5819 - val_accuracy: 0.8158\n",
      "Epoch 460/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2956 - accuracy: 0.8406\n",
      "Epoch 00460: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2955 - accuracy: 0.8406 - val_loss: 0.6197 - val_accuracy: 0.8178\n",
      "Epoch 461/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.2968 - accuracy: 0.8394\n",
      "Epoch 00461: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2972 - accuracy: 0.8391 - val_loss: 0.6424 - val_accuracy: 0.8135\n",
      "Epoch 462/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.2986 - accuracy: 0.8367\n",
      "Epoch 00462: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2985 - accuracy: 0.8367 - val_loss: 0.6566 - val_accuracy: 0.8127\n",
      "Epoch 463/1000\n",
      "22688/23208 [============================>.] - ETA: 0s - loss: 0.2955 - accuracy: 0.8407\n",
      "Epoch 00463: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2963 - accuracy: 0.8402 - val_loss: 0.6381 - val_accuracy: 0.8135\n",
      "Epoch 464/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.2966 - accuracy: 0.8398\n",
      "Epoch 00464: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 107us/sample - loss: 0.2963 - accuracy: 0.8400 - val_loss: 0.6430 - val_accuracy: 0.8174\n",
      "Epoch 465/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.2972 - accuracy: 0.8385\n",
      "Epoch 00465: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2969 - accuracy: 0.8383 - val_loss: 0.6447 - val_accuracy: 0.8139\n",
      "Epoch 466/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2969 - accuracy: 0.8395\n",
      "Epoch 00466: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2968 - accuracy: 0.8396 - val_loss: 0.6251 - val_accuracy: 0.8015\n",
      "Epoch 467/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.2979 - accuracy: 0.8378\n",
      "Epoch 00467: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2976 - accuracy: 0.8381 - val_loss: 0.6515 - val_accuracy: 0.8123\n",
      "Epoch 468/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.2967 - accuracy: 0.8381\n",
      "Epoch 00468: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2969 - accuracy: 0.8379 - val_loss: 0.6311 - val_accuracy: 0.8174\n",
      "Epoch 469/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2962 - accuracy: 0.8403\n",
      "Epoch 00469: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2958 - accuracy: 0.8404 - val_loss: 0.6564 - val_accuracy: 0.8112\n",
      "Epoch 470/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.2981 - accuracy: 0.8394\n",
      "Epoch 00470: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2991 - accuracy: 0.8393 - val_loss: 0.6498 - val_accuracy: 0.8104\n",
      "Epoch 471/1000\n",
      "22688/23208 [============================>.] - ETA: 0s - loss: 0.2972 - accuracy: 0.8393\n",
      "Epoch 00471: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2968 - accuracy: 0.8397 - val_loss: 0.6218 - val_accuracy: 0.8170\n",
      "Epoch 472/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2962 - accuracy: 0.8372\n",
      "Epoch 00472: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2962 - accuracy: 0.8372 - val_loss: 0.6290 - val_accuracy: 0.8143\n",
      "Epoch 473/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.2978 - accuracy: 0.8399\n",
      "Epoch 00473: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2977 - accuracy: 0.8399 - val_loss: 0.6352 - val_accuracy: 0.8162\n",
      "Epoch 474/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2966 - accuracy: 0.8394\n",
      "Epoch 00474: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2966 - accuracy: 0.8395 - val_loss: 0.6165 - val_accuracy: 0.8065\n",
      "Epoch 475/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.2963 - accuracy: 0.8408\n",
      "Epoch 00475: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2965 - accuracy: 0.8413 - val_loss: 0.6529 - val_accuracy: 0.8119\n",
      "Epoch 476/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.2973 - accuracy: 0.8404\n",
      "Epoch 00476: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2972 - accuracy: 0.8404 - val_loss: 0.6367 - val_accuracy: 0.8166\n",
      "Epoch 477/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2965 - accuracy: 0.8397\n",
      "Epoch 00477: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2971 - accuracy: 0.8394 - val_loss: 0.6044 - val_accuracy: 0.8057\n",
      "Epoch 478/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.2973 - accuracy: 0.8413\n",
      "Epoch 00478: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2974 - accuracy: 0.8411 - val_loss: 0.6020 - val_accuracy: 0.8069\n",
      "Epoch 479/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2968 - accuracy: 0.8391\n",
      "Epoch 00479: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2968 - accuracy: 0.8392 - val_loss: 0.6324 - val_accuracy: 0.8054\n",
      "Epoch 480/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2966 - accuracy: 0.8423\n",
      "Epoch 00480: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2965 - accuracy: 0.8424 - val_loss: 0.6453 - val_accuracy: 0.8073\n",
      "Epoch 481/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.2964 - accuracy: 0.8395\n",
      "Epoch 00481: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2961 - accuracy: 0.8396 - val_loss: 0.6593 - val_accuracy: 0.8127\n",
      "Epoch 482/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2980 - accuracy: 0.8388\n",
      "Epoch 00482: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2980 - accuracy: 0.8388 - val_loss: 0.6252 - val_accuracy: 0.8092\n",
      "Epoch 483/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.2991 - accuracy: 0.8378\n",
      "Epoch 00483: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2990 - accuracy: 0.8377 - val_loss: 0.6157 - val_accuracy: 0.8170\n",
      "Epoch 484/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2961 - accuracy: 0.8397\n",
      "Epoch 00484: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2962 - accuracy: 0.8397 - val_loss: 0.6302 - val_accuracy: 0.8139\n",
      "Epoch 485/1000\n",
      "22688/23208 [============================>.] - ETA: 0s - loss: 0.2962 - accuracy: 0.8411\n",
      "Epoch 00485: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2956 - accuracy: 0.8419 - val_loss: 0.6279 - val_accuracy: 0.8174\n",
      "Epoch 486/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2957 - accuracy: 0.8400\n",
      "Epoch 00486: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2957 - accuracy: 0.8400 - val_loss: 0.6150 - val_accuracy: 0.8112\n",
      "Epoch 487/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2954 - accuracy: 0.8410\n",
      "Epoch 00487: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2952 - accuracy: 0.8411 - val_loss: 0.6708 - val_accuracy: 0.8143\n",
      "Epoch 488/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.2976 - accuracy: 0.8403\n",
      "Epoch 00488: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2976 - accuracy: 0.8403 - val_loss: 0.6169 - val_accuracy: 0.8139\n",
      "Epoch 489/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.2972 - accuracy: 0.8398\n",
      "Epoch 00489: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 107us/sample - loss: 0.2975 - accuracy: 0.8396 - val_loss: 0.6078 - val_accuracy: 0.8158\n",
      "Epoch 490/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.2966 - accuracy: 0.8383\n",
      "Epoch 00490: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 107us/sample - loss: 0.2958 - accuracy: 0.8391 - val_loss: 0.6492 - val_accuracy: 0.8057\n",
      "Epoch 491/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2962 - accuracy: 0.8396\n",
      "Epoch 00491: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 107us/sample - loss: 0.2963 - accuracy: 0.8395 - val_loss: 0.6402 - val_accuracy: 0.8150\n",
      "Epoch 492/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2948 - accuracy: 0.8395\n",
      "Epoch 00492: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2955 - accuracy: 0.8392 - val_loss: 0.6324 - val_accuracy: 0.8119\n",
      "Epoch 493/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.2986 - accuracy: 0.8422\n",
      "Epoch 00493: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 108us/sample - loss: 0.2988 - accuracy: 0.8420 - val_loss: 0.6241 - val_accuracy: 0.8112\n",
      "Epoch 494/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2968 - accuracy: 0.8417\n",
      "Epoch 00494: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 148us/sample - loss: 0.2967 - accuracy: 0.8419 - val_loss: 0.6348 - val_accuracy: 0.8112\n",
      "Epoch 495/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.2953 - accuracy: 0.8405\n",
      "Epoch 00495: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 119us/sample - loss: 0.2955 - accuracy: 0.8402 - val_loss: 0.5823 - val_accuracy: 0.8100\n",
      "Epoch 496/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2973 - accuracy: 0.8390\n",
      "Epoch 00496: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2971 - accuracy: 0.8390 - val_loss: 0.6322 - val_accuracy: 0.8154\n",
      "Epoch 497/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.2957 - accuracy: 0.8404\n",
      "Epoch 00497: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2957 - accuracy: 0.8404 - val_loss: 0.6455 - val_accuracy: 0.8119\n",
      "Epoch 498/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2947 - accuracy: 0.8405\n",
      "Epoch 00498: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 107us/sample - loss: 0.2946 - accuracy: 0.8407 - val_loss: 0.6023 - val_accuracy: 0.8139\n",
      "Epoch 499/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2991 - accuracy: 0.8378\n",
      "Epoch 00499: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2985 - accuracy: 0.8383 - val_loss: 0.6219 - val_accuracy: 0.8174\n",
      "Epoch 500/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.2952 - accuracy: 0.8398\n",
      "Epoch 00500: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2959 - accuracy: 0.8395 - val_loss: 0.6722 - val_accuracy: 0.8119\n",
      "Epoch 501/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2975 - accuracy: 0.8395\n",
      "Epoch 00501: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2978 - accuracy: 0.8392 - val_loss: 0.6378 - val_accuracy: 0.8147\n",
      "Epoch 502/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.2964 - accuracy: 0.8395\n",
      "Epoch 00502: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2958 - accuracy: 0.8401 - val_loss: 0.6329 - val_accuracy: 0.8108\n",
      "Epoch 503/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.2957 - accuracy: 0.8390\n",
      "Epoch 00503: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2958 - accuracy: 0.8388 - val_loss: 0.6280 - val_accuracy: 0.8147\n",
      "Epoch 504/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.2950 - accuracy: 0.8411\n",
      "Epoch 00504: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2954 - accuracy: 0.8407 - val_loss: 0.6538 - val_accuracy: 0.8119\n",
      "Epoch 505/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.2983 - accuracy: 0.8387\n",
      "Epoch 00505: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2981 - accuracy: 0.8387 - val_loss: 0.6480 - val_accuracy: 0.8030\n",
      "Epoch 506/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.2980 - accuracy: 0.8389\n",
      "Epoch 00506: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2975 - accuracy: 0.8394 - val_loss: 0.6081 - val_accuracy: 0.8131\n",
      "Epoch 507/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.2964 - accuracy: 0.8403\n",
      "Epoch 00507: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2962 - accuracy: 0.8403 - val_loss: 0.6642 - val_accuracy: 0.8081\n",
      "Epoch 508/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.2977 - accuracy: 0.8393\n",
      "Epoch 00508: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2974 - accuracy: 0.8396 - val_loss: 0.6019 - val_accuracy: 0.8135\n",
      "Epoch 509/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2979 - accuracy: 0.8393\n",
      "Epoch 00509: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2984 - accuracy: 0.8388 - val_loss: 0.6183 - val_accuracy: 0.8147\n",
      "Epoch 510/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.2982 - accuracy: 0.8365\n",
      "Epoch 00510: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2982 - accuracy: 0.8365 - val_loss: 0.6249 - val_accuracy: 0.8104\n",
      "Epoch 511/1000\n",
      "22688/23208 [============================>.] - ETA: 0s - loss: 0.2948 - accuracy: 0.8401\n",
      "Epoch 00511: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 107us/sample - loss: 0.2953 - accuracy: 0.8400 - val_loss: 0.6345 - val_accuracy: 0.8116\n",
      "Epoch 512/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2978 - accuracy: 0.8378\n",
      "Epoch 00512: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2976 - accuracy: 0.8379 - val_loss: 0.6043 - val_accuracy: 0.8139\n",
      "Epoch 513/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.2955 - accuracy: 0.8368\n",
      "Epoch 00513: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2963 - accuracy: 0.8362 - val_loss: 0.6402 - val_accuracy: 0.8143\n",
      "Epoch 514/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.2954 - accuracy: 0.8396\n",
      "Epoch 00514: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2952 - accuracy: 0.8397 - val_loss: 0.6838 - val_accuracy: 0.7960\n",
      "Epoch 515/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.2968 - accuracy: 0.8413\n",
      "Epoch 00515: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2967 - accuracy: 0.8414 - val_loss: 0.6608 - val_accuracy: 0.8100\n",
      "Epoch 516/1000\n",
      "22688/23208 [============================>.] - ETA: 0s - loss: 0.2943 - accuracy: 0.8410\n",
      "Epoch 00516: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 107us/sample - loss: 0.2949 - accuracy: 0.8406 - val_loss: 0.6227 - val_accuracy: 0.8135\n",
      "Epoch 517/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2972 - accuracy: 0.8400\n",
      "Epoch 00517: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 107us/sample - loss: 0.2974 - accuracy: 0.8397 - val_loss: 0.5837 - val_accuracy: 0.8038\n",
      "Epoch 518/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.2959 - accuracy: 0.8407\n",
      "Epoch 00518: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2956 - accuracy: 0.8410 - val_loss: 0.6674 - val_accuracy: 0.8092\n",
      "Epoch 519/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.2943 - accuracy: 0.8420\n",
      "Epoch 00519: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2944 - accuracy: 0.8419 - val_loss: 0.6370 - val_accuracy: 0.8154\n",
      "Epoch 520/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2966 - accuracy: 0.8390\n",
      "Epoch 00520: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2967 - accuracy: 0.8389 - val_loss: 0.6478 - val_accuracy: 0.8143\n",
      "Epoch 521/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.2981 - accuracy: 0.8393\n",
      "Epoch 00521: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2981 - accuracy: 0.8393 - val_loss: 0.6469 - val_accuracy: 0.8131\n",
      "Epoch 522/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2973 - accuracy: 0.8389\n",
      "Epoch 00522: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2966 - accuracy: 0.8392 - val_loss: 0.6627 - val_accuracy: 0.8185\n",
      "Epoch 523/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2962 - accuracy: 0.8403\n",
      "Epoch 00523: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2962 - accuracy: 0.8404 - val_loss: 0.6204 - val_accuracy: 0.8150\n",
      "Epoch 524/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.2954 - accuracy: 0.8402\n",
      "Epoch 00524: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2949 - accuracy: 0.8405 - val_loss: 0.6498 - val_accuracy: 0.8162\n",
      "Epoch 525/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.2971 - accuracy: 0.8396\n",
      "Epoch 00525: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2965 - accuracy: 0.8400 - val_loss: 0.6205 - val_accuracy: 0.8127\n",
      "Epoch 526/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.2975 - accuracy: 0.8412\n",
      "Epoch 00526: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 107us/sample - loss: 0.2976 - accuracy: 0.8408 - val_loss: 0.6194 - val_accuracy: 0.8143\n",
      "Epoch 527/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.2963 - accuracy: 0.8406\n",
      "Epoch 00527: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2960 - accuracy: 0.8406 - val_loss: 0.6439 - val_accuracy: 0.8123\n",
      "Epoch 528/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.2978 - accuracy: 0.8399\n",
      "Epoch 00528: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2977 - accuracy: 0.8400 - val_loss: 0.6375 - val_accuracy: 0.8116\n",
      "Epoch 529/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.2972 - accuracy: 0.8396\n",
      "Epoch 00529: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2970 - accuracy: 0.8398 - val_loss: 0.6232 - val_accuracy: 0.8127\n",
      "Epoch 530/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2956 - accuracy: 0.8401\n",
      "Epoch 00530: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2956 - accuracy: 0.8402 - val_loss: 0.6196 - val_accuracy: 0.8077\n",
      "Epoch 531/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2946 - accuracy: 0.8402\n",
      "Epoch 00531: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2947 - accuracy: 0.8402 - val_loss: 0.6387 - val_accuracy: 0.8185\n",
      "Epoch 532/1000\n",
      "22688/23208 [============================>.] - ETA: 0s - loss: 0.2965 - accuracy: 0.8406\n",
      "Epoch 00532: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2965 - accuracy: 0.8408 - val_loss: 0.5975 - val_accuracy: 0.8131\n",
      "Epoch 533/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.2973 - accuracy: 0.8401\n",
      "Epoch 00533: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2973 - accuracy: 0.8404 - val_loss: 0.6109 - val_accuracy: 0.8154\n",
      "Epoch 534/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2955 - accuracy: 0.8394\n",
      "Epoch 00534: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2962 - accuracy: 0.8392 - val_loss: 0.6388 - val_accuracy: 0.8119\n",
      "Epoch 535/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.2964 - accuracy: 0.8399\n",
      "Epoch 00535: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2959 - accuracy: 0.8402 - val_loss: 0.6494 - val_accuracy: 0.8116\n",
      "Epoch 536/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2945 - accuracy: 0.8400\n",
      "Epoch 00536: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2940 - accuracy: 0.8401 - val_loss: 0.6389 - val_accuracy: 0.8108\n",
      "Epoch 537/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.2955 - accuracy: 0.8421\n",
      "Epoch 00537: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2964 - accuracy: 0.8415 - val_loss: 0.6292 - val_accuracy: 0.8158\n",
      "Epoch 538/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2976 - accuracy: 0.8404\n",
      "Epoch 00538: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 107us/sample - loss: 0.2976 - accuracy: 0.8404 - val_loss: 0.6503 - val_accuracy: 0.8166\n",
      "Epoch 539/1000\n",
      "22688/23208 [============================>.] - ETA: 0s - loss: 0.2966 - accuracy: 0.8395\n",
      "Epoch 00539: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2954 - accuracy: 0.8401 - val_loss: 0.6481 - val_accuracy: 0.8143\n",
      "Epoch 540/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.2944 - accuracy: 0.8404\n",
      "Epoch 00540: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2946 - accuracy: 0.8402 - val_loss: 0.6222 - val_accuracy: 0.8147\n",
      "Epoch 541/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2958 - accuracy: 0.8408\n",
      "Epoch 00541: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2959 - accuracy: 0.8410 - val_loss: 0.6700 - val_accuracy: 0.8096\n",
      "Epoch 542/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.2955 - accuracy: 0.8410\n",
      "Epoch 00542: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2949 - accuracy: 0.8413 - val_loss: 0.6593 - val_accuracy: 0.8162\n",
      "Epoch 543/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2958 - accuracy: 0.8426\n",
      "Epoch 00543: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2960 - accuracy: 0.8425 - val_loss: 0.6281 - val_accuracy: 0.8139\n",
      "Epoch 544/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2944 - accuracy: 0.8401\n",
      "Epoch 00544: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2951 - accuracy: 0.8397 - val_loss: 0.6302 - val_accuracy: 0.8127\n",
      "Epoch 545/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.2993 - accuracy: 0.8409\n",
      "Epoch 00545: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2994 - accuracy: 0.8407 - val_loss: 0.5974 - val_accuracy: 0.8185\n",
      "Epoch 546/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.2962 - accuracy: 0.8408\n",
      "Epoch 00546: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2960 - accuracy: 0.8411 - val_loss: 0.6218 - val_accuracy: 0.8100\n",
      "Epoch 547/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2986 - accuracy: 0.8397\n",
      "Epoch 00547: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2986 - accuracy: 0.8397 - val_loss: 0.6517 - val_accuracy: 0.8081\n",
      "Epoch 548/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.2970 - accuracy: 0.8399\n",
      "Epoch 00548: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2970 - accuracy: 0.8399 - val_loss: 0.6662 - val_accuracy: 0.8065\n",
      "Epoch 549/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2954 - accuracy: 0.8416\n",
      "Epoch 00549: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2952 - accuracy: 0.8420 - val_loss: 0.6208 - val_accuracy: 0.8197\n",
      "Epoch 550/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.2959 - accuracy: 0.8398\n",
      "Epoch 00550: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2956 - accuracy: 0.8401 - val_loss: 0.6379 - val_accuracy: 0.8127\n",
      "Epoch 551/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2958 - accuracy: 0.8406\n",
      "Epoch 00551: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2961 - accuracy: 0.8404 - val_loss: 0.6199 - val_accuracy: 0.8081\n",
      "Epoch 552/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.2939 - accuracy: 0.8405\n",
      "Epoch 00552: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2945 - accuracy: 0.8409 - val_loss: 0.6464 - val_accuracy: 0.8127\n",
      "Epoch 553/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.2948 - accuracy: 0.8415\n",
      "Epoch 00553: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2955 - accuracy: 0.8411 - val_loss: 0.6009 - val_accuracy: 0.8127\n",
      "Epoch 554/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.2956 - accuracy: 0.8413\n",
      "Epoch 00554: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2957 - accuracy: 0.8410 - val_loss: 0.6224 - val_accuracy: 0.8116\n",
      "Epoch 555/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2944 - accuracy: 0.8426\n",
      "Epoch 00555: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2947 - accuracy: 0.8425 - val_loss: 0.6234 - val_accuracy: 0.8022\n",
      "Epoch 556/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2950 - accuracy: 0.8388\n",
      "Epoch 00556: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2949 - accuracy: 0.8388 - val_loss: 0.6400 - val_accuracy: 0.8135\n",
      "Epoch 557/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.2968 - accuracy: 0.8385\n",
      "Epoch 00557: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 107us/sample - loss: 0.2968 - accuracy: 0.8385 - val_loss: 0.6317 - val_accuracy: 0.8139\n",
      "Epoch 558/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.2973 - accuracy: 0.8393\n",
      "Epoch 00558: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 108us/sample - loss: 0.2968 - accuracy: 0.8395 - val_loss: 0.6095 - val_accuracy: 0.8123\n",
      "Epoch 559/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.2962 - accuracy: 0.8399\n",
      "Epoch 00559: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2964 - accuracy: 0.8403 - val_loss: 0.5861 - val_accuracy: 0.8085\n",
      "Epoch 560/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.2966 - accuracy: 0.8400\n",
      "Epoch 00560: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2965 - accuracy: 0.8402 - val_loss: 0.6246 - val_accuracy: 0.8108\n",
      "Epoch 561/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.2949 - accuracy: 0.8417\n",
      "Epoch 00561: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2959 - accuracy: 0.8411 - val_loss: 0.6158 - val_accuracy: 0.8139\n",
      "Epoch 562/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2977 - accuracy: 0.8392\n",
      "Epoch 00562: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2976 - accuracy: 0.8393 - val_loss: 0.6478 - val_accuracy: 0.8181\n",
      "Epoch 563/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.2981 - accuracy: 0.8414\n",
      "Epoch 00563: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2982 - accuracy: 0.8413 - val_loss: 0.6251 - val_accuracy: 0.8143\n",
      "Epoch 564/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2947 - accuracy: 0.8416\n",
      "Epoch 00564: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2945 - accuracy: 0.8418 - val_loss: 0.6222 - val_accuracy: 0.8154\n",
      "Epoch 565/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.2946 - accuracy: 0.8402\n",
      "Epoch 00565: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2945 - accuracy: 0.8401 - val_loss: 0.6344 - val_accuracy: 0.8135\n",
      "Epoch 566/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2928 - accuracy: 0.8405\n",
      "Epoch 00566: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2934 - accuracy: 0.8404 - val_loss: 0.6486 - val_accuracy: 0.8185\n",
      "Epoch 567/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2944 - accuracy: 0.8404\n",
      "Epoch 00567: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 107us/sample - loss: 0.2939 - accuracy: 0.8406 - val_loss: 0.6850 - val_accuracy: 0.8123\n",
      "Epoch 568/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.2952 - accuracy: 0.8394\n",
      "Epoch 00568: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2954 - accuracy: 0.8394 - val_loss: 0.6439 - val_accuracy: 0.8127\n",
      "Epoch 569/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2969 - accuracy: 0.8414\n",
      "Epoch 00569: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2966 - accuracy: 0.8417 - val_loss: 0.6763 - val_accuracy: 0.8088\n",
      "Epoch 570/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.2999 - accuracy: 0.8393\n",
      "Epoch 00570: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.3000 - accuracy: 0.8392 - val_loss: 0.6339 - val_accuracy: 0.8150\n",
      "Epoch 571/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.2951 - accuracy: 0.8402\n",
      "Epoch 00571: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2948 - accuracy: 0.8409 - val_loss: 0.6167 - val_accuracy: 0.8100\n",
      "Epoch 572/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.2957 - accuracy: 0.8421\n",
      "Epoch 00572: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2958 - accuracy: 0.8421 - val_loss: 0.6525 - val_accuracy: 0.8123\n",
      "Epoch 573/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.2946 - accuracy: 0.8406\n",
      "Epoch 00573: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2947 - accuracy: 0.8407 - val_loss: 0.6592 - val_accuracy: 0.8026\n",
      "Epoch 574/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.2957 - accuracy: 0.8401\n",
      "Epoch 00574: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2957 - accuracy: 0.8402 - val_loss: 0.6344 - val_accuracy: 0.8116\n",
      "Epoch 575/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2948 - accuracy: 0.8387\n",
      "Epoch 00575: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2952 - accuracy: 0.8386 - val_loss: 0.6266 - val_accuracy: 0.8127\n",
      "Epoch 576/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.2951 - accuracy: 0.8388\n",
      "Epoch 00576: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2943 - accuracy: 0.8393 - val_loss: 0.6655 - val_accuracy: 0.8143\n",
      "Epoch 577/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.2973 - accuracy: 0.8391\n",
      "Epoch 00577: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2972 - accuracy: 0.8397 - val_loss: 0.6152 - val_accuracy: 0.8131\n",
      "Epoch 578/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.2963 - accuracy: 0.8390\n",
      "Epoch 00578: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2960 - accuracy: 0.8391 - val_loss: 0.6301 - val_accuracy: 0.8127\n",
      "Epoch 579/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2968 - accuracy: 0.8392\n",
      "Epoch 00579: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2963 - accuracy: 0.8397 - val_loss: 0.6601 - val_accuracy: 0.8147\n",
      "Epoch 580/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.2952 - accuracy: 0.8404\n",
      "Epoch 00580: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2957 - accuracy: 0.8400 - val_loss: 0.6264 - val_accuracy: 0.8119\n",
      "Epoch 581/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.2965 - accuracy: 0.8412\n",
      "Epoch 00581: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2959 - accuracy: 0.8417 - val_loss: 0.6021 - val_accuracy: 0.8174\n",
      "Epoch 582/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.2984 - accuracy: 0.8392\n",
      "Epoch 00582: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2984 - accuracy: 0.8392 - val_loss: 0.6370 - val_accuracy: 0.8127\n",
      "Epoch 583/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.2944 - accuracy: 0.8399\n",
      "Epoch 00583: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2944 - accuracy: 0.8403 - val_loss: 0.6334 - val_accuracy: 0.8158\n",
      "Epoch 584/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2962 - accuracy: 0.8385\n",
      "Epoch 00584: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2966 - accuracy: 0.8383 - val_loss: 0.6225 - val_accuracy: 0.8100\n",
      "Epoch 585/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.2967 - accuracy: 0.8376\n",
      "Epoch 00585: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2965 - accuracy: 0.8379 - val_loss: 0.6604 - val_accuracy: 0.8139\n",
      "Epoch 586/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.2959 - accuracy: 0.8415\n",
      "Epoch 00586: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2962 - accuracy: 0.8412 - val_loss: 0.6516 - val_accuracy: 0.8131\n",
      "Epoch 587/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.2940 - accuracy: 0.8407\n",
      "Epoch 00587: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 107us/sample - loss: 0.2941 - accuracy: 0.8408 - val_loss: 0.6834 - val_accuracy: 0.8104\n",
      "Epoch 588/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.3003 - accuracy: 0.8387\n",
      "Epoch 00588: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.3006 - accuracy: 0.8384 - val_loss: 0.6398 - val_accuracy: 0.8135\n",
      "Epoch 589/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2936 - accuracy: 0.8395\n",
      "Epoch 00589: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2936 - accuracy: 0.8396 - val_loss: 0.6750 - val_accuracy: 0.8127\n",
      "Epoch 590/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.2928 - accuracy: 0.8415\n",
      "Epoch 00590: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2927 - accuracy: 0.8413 - val_loss: 0.6693 - val_accuracy: 0.8139\n",
      "Epoch 591/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.2943 - accuracy: 0.8413\n",
      "Epoch 00591: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 107us/sample - loss: 0.2947 - accuracy: 0.8416 - val_loss: 0.6459 - val_accuracy: 0.8162\n",
      "Epoch 592/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.2942 - accuracy: 0.8407\n",
      "Epoch 00592: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 113us/sample - loss: 0.2944 - accuracy: 0.8404 - val_loss: 0.6422 - val_accuracy: 0.8131\n",
      "Epoch 593/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.2940 - accuracy: 0.8403\n",
      "Epoch 00593: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 139us/sample - loss: 0.2933 - accuracy: 0.8408 - val_loss: 0.6808 - val_accuracy: 0.8104\n",
      "Epoch 594/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.2973 - accuracy: 0.8403\n",
      "Epoch 00594: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 146us/sample - loss: 0.2971 - accuracy: 0.8403 - val_loss: 0.6079 - val_accuracy: 0.8143\n",
      "Epoch 595/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.2940 - accuracy: 0.8414\n",
      "Epoch 00595: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 121us/sample - loss: 0.2938 - accuracy: 0.8414 - val_loss: 0.6381 - val_accuracy: 0.8170\n",
      "Epoch 596/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2933 - accuracy: 0.8427\n",
      "Epoch 00596: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 126us/sample - loss: 0.2934 - accuracy: 0.8424 - val_loss: 0.6733 - val_accuracy: 0.8135\n",
      "Epoch 597/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.2965 - accuracy: 0.8399\n",
      "Epoch 00597: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 148us/sample - loss: 0.2960 - accuracy: 0.8399 - val_loss: 0.6526 - val_accuracy: 0.8147\n",
      "Epoch 598/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.2962 - accuracy: 0.8396\n",
      "Epoch 00598: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 142us/sample - loss: 0.2964 - accuracy: 0.8394 - val_loss: 0.6739 - val_accuracy: 0.8131\n",
      "Epoch 599/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.2967 - accuracy: 0.8406\n",
      "Epoch 00599: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 141us/sample - loss: 0.2966 - accuracy: 0.8405 - val_loss: 0.6362 - val_accuracy: 0.8085\n",
      "Epoch 600/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2943 - accuracy: 0.8400\n",
      "Epoch 00600: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 112us/sample - loss: 0.2944 - accuracy: 0.8399 - val_loss: 0.6497 - val_accuracy: 0.8127\n",
      "Epoch 601/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2966 - accuracy: 0.8408\n",
      "Epoch 00601: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2966 - accuracy: 0.8408 - val_loss: 0.6071 - val_accuracy: 0.8147\n",
      "Epoch 602/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2966 - accuracy: 0.8417\n",
      "Epoch 00602: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2964 - accuracy: 0.8416 - val_loss: 0.6322 - val_accuracy: 0.8166\n",
      "Epoch 603/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2945 - accuracy: 0.8411\n",
      "Epoch 00603: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2945 - accuracy: 0.8407 - val_loss: 0.6547 - val_accuracy: 0.8185\n",
      "Epoch 604/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2953 - accuracy: 0.8405\n",
      "Epoch 00604: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2951 - accuracy: 0.8405 - val_loss: 0.6201 - val_accuracy: 0.8123\n",
      "Epoch 605/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.2936 - accuracy: 0.8406\n",
      "Epoch 00605: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2937 - accuracy: 0.8407 - val_loss: 0.6333 - val_accuracy: 0.8178\n",
      "Epoch 606/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.2960 - accuracy: 0.8412\n",
      "Epoch 00606: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 107us/sample - loss: 0.2962 - accuracy: 0.8410 - val_loss: 0.6284 - val_accuracy: 0.8143\n",
      "Epoch 607/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.2973 - accuracy: 0.8383\n",
      "Epoch 00607: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2971 - accuracy: 0.8385 - val_loss: 0.6319 - val_accuracy: 0.8174\n",
      "Epoch 608/1000\n",
      "22688/23208 [============================>.] - ETA: 0s - loss: 0.2937 - accuracy: 0.8423\n",
      "Epoch 00608: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2944 - accuracy: 0.8416 - val_loss: 0.6211 - val_accuracy: 0.8092\n",
      "Epoch 609/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2963 - accuracy: 0.8412\n",
      "Epoch 00609: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 107us/sample - loss: 0.2962 - accuracy: 0.8412 - val_loss: 0.6342 - val_accuracy: 0.8193\n",
      "Epoch 610/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2952 - accuracy: 0.8409\n",
      "Epoch 00610: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2951 - accuracy: 0.8412 - val_loss: 0.6160 - val_accuracy: 0.8054\n",
      "Epoch 611/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2956 - accuracy: 0.8399\n",
      "Epoch 00611: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 136us/sample - loss: 0.2956 - accuracy: 0.8399 - val_loss: 0.6345 - val_accuracy: 0.8088\n",
      "Epoch 612/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.2944 - accuracy: 0.8410\n",
      "Epoch 00612: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 147us/sample - loss: 0.2941 - accuracy: 0.8411 - val_loss: 0.6410 - val_accuracy: 0.8143\n",
      "Epoch 613/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2962 - accuracy: 0.8395\n",
      "Epoch 00613: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 146us/sample - loss: 0.2957 - accuracy: 0.8398 - val_loss: 0.6271 - val_accuracy: 0.8131\n",
      "Epoch 614/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.2966 - accuracy: 0.8387\n",
      "Epoch 00614: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 134us/sample - loss: 0.2964 - accuracy: 0.8391 - val_loss: 0.6553 - val_accuracy: 0.8119\n",
      "Epoch 615/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.2954 - accuracy: 0.8418\n",
      "Epoch 00615: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2951 - accuracy: 0.8418 - val_loss: 0.6343 - val_accuracy: 0.8085\n",
      "Epoch 616/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2955 - accuracy: 0.8406\n",
      "Epoch 00616: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 140us/sample - loss: 0.2953 - accuracy: 0.8406 - val_loss: 0.6542 - val_accuracy: 0.8135\n",
      "Epoch 617/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.2956 - accuracy: 0.8410\n",
      "Epoch 00617: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 123us/sample - loss: 0.2954 - accuracy: 0.8411 - val_loss: 0.6165 - val_accuracy: 0.8116\n",
      "Epoch 618/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.2972 - accuracy: 0.8394\n",
      "Epoch 00618: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 143us/sample - loss: 0.2970 - accuracy: 0.8395 - val_loss: 0.6579 - val_accuracy: 0.8081\n",
      "Epoch 619/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.2959 - accuracy: 0.8399\n",
      "Epoch 00619: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2963 - accuracy: 0.8396 - val_loss: 0.6413 - val_accuracy: 0.8162\n",
      "Epoch 620/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2952 - accuracy: 0.8400\n",
      "Epoch 00620: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2946 - accuracy: 0.8406 - val_loss: 0.6693 - val_accuracy: 0.8104\n",
      "Epoch 621/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.2951 - accuracy: 0.8410\n",
      "Epoch 00621: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 135us/sample - loss: 0.2951 - accuracy: 0.8410 - val_loss: 0.6328 - val_accuracy: 0.8135\n",
      "Epoch 622/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.2937 - accuracy: 0.8401\n",
      "Epoch 00622: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 108us/sample - loss: 0.2939 - accuracy: 0.8398 - val_loss: 0.6346 - val_accuracy: 0.8116\n",
      "Epoch 623/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2914 - accuracy: 0.8391\n",
      "Epoch 00623: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2915 - accuracy: 0.8391 - val_loss: 0.6771 - val_accuracy: 0.8123\n",
      "Epoch 624/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2933 - accuracy: 0.8418\n",
      "Epoch 00624: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2937 - accuracy: 0.8420 - val_loss: 0.6453 - val_accuracy: 0.8135\n",
      "Epoch 625/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.2929 - accuracy: 0.8402\n",
      "Epoch 00625: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2934 - accuracy: 0.8398 - val_loss: 0.6685 - val_accuracy: 0.8154\n",
      "Epoch 626/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2953 - accuracy: 0.8400\n",
      "Epoch 00626: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2952 - accuracy: 0.8401 - val_loss: 0.6489 - val_accuracy: 0.8104\n",
      "Epoch 627/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.2934 - accuracy: 0.8397\n",
      "Epoch 00627: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2942 - accuracy: 0.8396 - val_loss: 0.6220 - val_accuracy: 0.8092\n",
      "Epoch 628/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2932 - accuracy: 0.8412\n",
      "Epoch 00628: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2930 - accuracy: 0.8410 - val_loss: 0.6777 - val_accuracy: 0.8119\n",
      "Epoch 629/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2942 - accuracy: 0.8395\n",
      "Epoch 00629: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2947 - accuracy: 0.8395 - val_loss: 0.6567 - val_accuracy: 0.8100\n",
      "Epoch 630/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2981 - accuracy: 0.8403\n",
      "Epoch 00630: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2981 - accuracy: 0.8404 - val_loss: 0.6359 - val_accuracy: 0.8112\n",
      "Epoch 631/1000\n",
      "22688/23208 [============================>.] - ETA: 0s - loss: 0.2955 - accuracy: 0.8405\n",
      "Epoch 00631: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2959 - accuracy: 0.8405 - val_loss: 0.6203 - val_accuracy: 0.8181\n",
      "Epoch 632/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.2956 - accuracy: 0.8391\n",
      "Epoch 00632: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2960 - accuracy: 0.8390 - val_loss: 0.6263 - val_accuracy: 0.8135\n",
      "Epoch 633/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.2950 - accuracy: 0.8388\n",
      "Epoch 00633: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2943 - accuracy: 0.8394 - val_loss: 0.6058 - val_accuracy: 0.8166\n",
      "Epoch 634/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.2946 - accuracy: 0.8401\n",
      "Epoch 00634: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2948 - accuracy: 0.8400 - val_loss: 0.6429 - val_accuracy: 0.8185\n",
      "Epoch 635/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.2938 - accuracy: 0.8400\n",
      "Epoch 00635: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 131us/sample - loss: 0.2939 - accuracy: 0.8401 - val_loss: 0.6231 - val_accuracy: 0.8123\n",
      "Epoch 636/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2972 - accuracy: 0.8384\n",
      "Epoch 00636: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 115us/sample - loss: 0.2968 - accuracy: 0.8387 - val_loss: 0.6354 - val_accuracy: 0.8143\n",
      "Epoch 637/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2946 - accuracy: 0.8401\n",
      "Epoch 00637: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2951 - accuracy: 0.8399 - val_loss: 0.6353 - val_accuracy: 0.8108\n",
      "Epoch 638/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2970 - accuracy: 0.8404\n",
      "Epoch 00638: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 125us/sample - loss: 0.2973 - accuracy: 0.8406 - val_loss: 0.6600 - val_accuracy: 0.8150\n",
      "Epoch 639/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2931 - accuracy: 0.8414\n",
      "Epoch 00639: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 129us/sample - loss: 0.2934 - accuracy: 0.8414 - val_loss: 0.6729 - val_accuracy: 0.8147\n",
      "Epoch 640/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.2936 - accuracy: 0.8409\n",
      "Epoch 00640: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 107us/sample - loss: 0.2936 - accuracy: 0.8410 - val_loss: 0.6758 - val_accuracy: 0.8123\n",
      "Epoch 641/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.2961 - accuracy: 0.8402\n",
      "Epoch 00641: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2966 - accuracy: 0.8403 - val_loss: 0.6208 - val_accuracy: 0.8166\n",
      "Epoch 642/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.2949 - accuracy: 0.8401\n",
      "Epoch 00642: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 125us/sample - loss: 0.2947 - accuracy: 0.8400 - val_loss: 0.6164 - val_accuracy: 0.8170\n",
      "Epoch 643/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2955 - accuracy: 0.8419\n",
      "Epoch 00643: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 4s 151us/sample - loss: 0.2957 - accuracy: 0.8419 - val_loss: 0.6192 - val_accuracy: 0.8112\n",
      "Epoch 644/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2952 - accuracy: 0.8399\n",
      "Epoch 00644: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 149us/sample - loss: 0.2950 - accuracy: 0.8399 - val_loss: 0.5922 - val_accuracy: 0.8181\n",
      "Epoch 645/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2935 - accuracy: 0.8427\n",
      "Epoch 00645: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 128us/sample - loss: 0.2933 - accuracy: 0.8426 - val_loss: 0.6333 - val_accuracy: 0.8127\n",
      "Epoch 646/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.2943 - accuracy: 0.8409\n",
      "Epoch 00646: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 107us/sample - loss: 0.2945 - accuracy: 0.8407 - val_loss: 0.6445 - val_accuracy: 0.8150\n",
      "Epoch 647/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.2962 - accuracy: 0.8407\n",
      "Epoch 00647: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2972 - accuracy: 0.8405 - val_loss: 0.6310 - val_accuracy: 0.8127\n",
      "Epoch 648/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2950 - accuracy: 0.8402\n",
      "Epoch 00648: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2955 - accuracy: 0.8396 - val_loss: 0.6001 - val_accuracy: 0.8143\n",
      "Epoch 649/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2960 - accuracy: 0.8383\n",
      "Epoch 00649: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2958 - accuracy: 0.8386 - val_loss: 0.6323 - val_accuracy: 0.8174\n",
      "Epoch 650/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2950 - accuracy: 0.8401\n",
      "Epoch 00650: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 108us/sample - loss: 0.2950 - accuracy: 0.8400 - val_loss: 0.6092 - val_accuracy: 0.8158\n",
      "Epoch 651/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.2941 - accuracy: 0.8421\n",
      "Epoch 00651: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2941 - accuracy: 0.8420 - val_loss: 0.6978 - val_accuracy: 0.8088\n",
      "Epoch 652/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.2952 - accuracy: 0.8395\n",
      "Epoch 00652: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2953 - accuracy: 0.8397 - val_loss: 0.6319 - val_accuracy: 0.8108\n",
      "Epoch 653/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2955 - accuracy: 0.8417\n",
      "Epoch 00653: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2960 - accuracy: 0.8417 - val_loss: 0.6503 - val_accuracy: 0.8135\n",
      "Epoch 654/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.2976 - accuracy: 0.8403\n",
      "Epoch 00654: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2969 - accuracy: 0.8406 - val_loss: 0.6576 - val_accuracy: 0.8166\n",
      "Epoch 655/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2954 - accuracy: 0.8398\n",
      "Epoch 00655: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2955 - accuracy: 0.8397 - val_loss: 0.6347 - val_accuracy: 0.8150\n",
      "Epoch 656/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2953 - accuracy: 0.8408\n",
      "Epoch 00656: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2955 - accuracy: 0.8407 - val_loss: 0.6540 - val_accuracy: 0.8174\n",
      "Epoch 657/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.2954 - accuracy: 0.8396\n",
      "Epoch 00657: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2954 - accuracy: 0.8398 - val_loss: 0.6170 - val_accuracy: 0.8197\n",
      "Epoch 658/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2925 - accuracy: 0.8405\n",
      "Epoch 00658: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2933 - accuracy: 0.8405 - val_loss: 0.6202 - val_accuracy: 0.8088\n",
      "Epoch 659/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.2933 - accuracy: 0.8412\n",
      "Epoch 00659: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2931 - accuracy: 0.8411 - val_loss: 0.6359 - val_accuracy: 0.8193\n",
      "Epoch 660/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2954 - accuracy: 0.8385\n",
      "Epoch 00660: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2956 - accuracy: 0.8382 - val_loss: 0.6155 - val_accuracy: 0.8170\n",
      "Epoch 661/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2954 - accuracy: 0.8413\n",
      "Epoch 00661: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2954 - accuracy: 0.8413 - val_loss: 0.6301 - val_accuracy: 0.8127\n",
      "Epoch 662/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2949 - accuracy: 0.8404\n",
      "Epoch 00662: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2944 - accuracy: 0.8407 - val_loss: 0.6384 - val_accuracy: 0.8170\n",
      "Epoch 663/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.2929 - accuracy: 0.8419\n",
      "Epoch 00663: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2931 - accuracy: 0.8413 - val_loss: 0.6760 - val_accuracy: 0.8150\n",
      "Epoch 664/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2938 - accuracy: 0.8402\n",
      "Epoch 00664: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2939 - accuracy: 0.8402 - val_loss: 0.6438 - val_accuracy: 0.8135\n",
      "Epoch 665/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2965 - accuracy: 0.8415\n",
      "Epoch 00665: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 107us/sample - loss: 0.2965 - accuracy: 0.8415 - val_loss: 0.6181 - val_accuracy: 0.8112\n",
      "Epoch 666/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.2959 - accuracy: 0.8402\n",
      "Epoch 00666: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2958 - accuracy: 0.8402 - val_loss: 0.6393 - val_accuracy: 0.8150\n",
      "Epoch 667/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2960 - accuracy: 0.8411\n",
      "Epoch 00667: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2956 - accuracy: 0.8415 - val_loss: 0.5904 - val_accuracy: 0.8143\n",
      "Epoch 668/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.2924 - accuracy: 0.8412\n",
      "Epoch 00668: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2920 - accuracy: 0.8414 - val_loss: 0.6324 - val_accuracy: 0.8139\n",
      "Epoch 669/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2927 - accuracy: 0.8410\n",
      "Epoch 00669: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2928 - accuracy: 0.8412 - val_loss: 0.6800 - val_accuracy: 0.8147\n",
      "Epoch 670/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2961 - accuracy: 0.8400\n",
      "Epoch 00670: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2958 - accuracy: 0.8401 - val_loss: 0.6526 - val_accuracy: 0.8166\n",
      "Epoch 671/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2951 - accuracy: 0.8421\n",
      "Epoch 00671: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 107us/sample - loss: 0.2948 - accuracy: 0.8423 - val_loss: 0.6284 - val_accuracy: 0.8096\n",
      "Epoch 672/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2936 - accuracy: 0.8405\n",
      "Epoch 00672: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2935 - accuracy: 0.8409 - val_loss: 0.6360 - val_accuracy: 0.8154\n",
      "Epoch 673/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.2960 - accuracy: 0.8416\n",
      "Epoch 00673: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2963 - accuracy: 0.8414 - val_loss: 0.6262 - val_accuracy: 0.8112\n",
      "Epoch 674/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.2945 - accuracy: 0.8401\n",
      "Epoch 00674: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2943 - accuracy: 0.8400 - val_loss: 0.6257 - val_accuracy: 0.8139\n",
      "Epoch 675/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2947 - accuracy: 0.8400\n",
      "Epoch 00675: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 107us/sample - loss: 0.2941 - accuracy: 0.8409 - val_loss: 0.6747 - val_accuracy: 0.7991\n",
      "Epoch 676/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.2953 - accuracy: 0.8420\n",
      "Epoch 00676: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2951 - accuracy: 0.8421 - val_loss: 0.6576 - val_accuracy: 0.8119\n",
      "Epoch 677/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2965 - accuracy: 0.8393\n",
      "Epoch 00677: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2965 - accuracy: 0.8395 - val_loss: 0.6451 - val_accuracy: 0.8112\n",
      "Epoch 678/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2953 - accuracy: 0.8410\n",
      "Epoch 00678: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2950 - accuracy: 0.8412 - val_loss: 0.6429 - val_accuracy: 0.8174\n",
      "Epoch 679/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.2924 - accuracy: 0.8416\n",
      "Epoch 00679: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2923 - accuracy: 0.8417 - val_loss: 0.6536 - val_accuracy: 0.8135\n",
      "Epoch 680/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.2953 - accuracy: 0.8393\n",
      "Epoch 00680: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2952 - accuracy: 0.8393 - val_loss: 0.6507 - val_accuracy: 0.8162\n",
      "Epoch 681/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.2950 - accuracy: 0.8413\n",
      "Epoch 00681: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2950 - accuracy: 0.8411 - val_loss: 0.6193 - val_accuracy: 0.8154\n",
      "Epoch 682/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2947 - accuracy: 0.8405\n",
      "Epoch 00682: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2945 - accuracy: 0.8407 - val_loss: 0.6499 - val_accuracy: 0.8135\n",
      "Epoch 683/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.2975 - accuracy: 0.8387\n",
      "Epoch 00683: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2970 - accuracy: 0.8388 - val_loss: 0.5968 - val_accuracy: 0.8139\n",
      "Epoch 684/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.2938 - accuracy: 0.8412\n",
      "Epoch 00684: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2938 - accuracy: 0.8411 - val_loss: 0.6960 - val_accuracy: 0.8135\n",
      "Epoch 685/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.2949 - accuracy: 0.8405\n",
      "Epoch 00685: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2944 - accuracy: 0.8407 - val_loss: 0.6518 - val_accuracy: 0.8127\n",
      "Epoch 686/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.2951 - accuracy: 0.8404\n",
      "Epoch 00686: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2959 - accuracy: 0.8396 - val_loss: 0.6120 - val_accuracy: 0.8147\n",
      "Epoch 687/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2948 - accuracy: 0.8415\n",
      "Epoch 00687: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2953 - accuracy: 0.8410 - val_loss: 0.6655 - val_accuracy: 0.8147\n",
      "Epoch 688/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.2940 - accuracy: 0.8400\n",
      "Epoch 00688: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 107us/sample - loss: 0.2946 - accuracy: 0.8399 - val_loss: 0.6564 - val_accuracy: 0.8131\n",
      "Epoch 689/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2955 - accuracy: 0.8406\n",
      "Epoch 00689: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2955 - accuracy: 0.8413 - val_loss: 0.6585 - val_accuracy: 0.8158\n",
      "Epoch 690/1000\n",
      "22688/23208 [============================>.] - ETA: 0s - loss: 0.2932 - accuracy: 0.8411\n",
      "Epoch 00690: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 109us/sample - loss: 0.2928 - accuracy: 0.8412 - val_loss: 0.6621 - val_accuracy: 0.8131\n",
      "Epoch 691/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.2953 - accuracy: 0.8415\n",
      "Epoch 00691: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2952 - accuracy: 0.8415 - val_loss: 0.6486 - val_accuracy: 0.8135\n",
      "Epoch 692/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.2921 - accuracy: 0.8415\n",
      "Epoch 00692: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2922 - accuracy: 0.8414 - val_loss: 0.6979 - val_accuracy: 0.8143\n",
      "Epoch 693/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2966 - accuracy: 0.8415\n",
      "Epoch 00693: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2969 - accuracy: 0.8411 - val_loss: 0.6182 - val_accuracy: 0.8057\n",
      "Epoch 694/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.2961 - accuracy: 0.8391\n",
      "Epoch 00694: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2962 - accuracy: 0.8392 - val_loss: 0.6456 - val_accuracy: 0.8119\n",
      "Epoch 695/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.2938 - accuracy: 0.8413\n",
      "Epoch 00695: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 108us/sample - loss: 0.2937 - accuracy: 0.8413 - val_loss: 0.6723 - val_accuracy: 0.8116\n",
      "Epoch 696/1000\n",
      "22688/23208 [============================>.] - ETA: 0s - loss: 0.2948 - accuracy: 0.8393\n",
      "Epoch 00696: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2946 - accuracy: 0.8393 - val_loss: 0.6436 - val_accuracy: 0.8088\n",
      "Epoch 697/1000\n",
      "22656/23208 [============================>.] - ETA: 0s - loss: 0.2935 - accuracy: 0.8391\n",
      "Epoch 00697: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2937 - accuracy: 0.8396 - val_loss: 0.6546 - val_accuracy: 0.8147\n",
      "Epoch 698/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2966 - accuracy: 0.8404\n",
      "Epoch 00698: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2963 - accuracy: 0.8407 - val_loss: 0.6153 - val_accuracy: 0.8135\n",
      "Epoch 699/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.2932 - accuracy: 0.8423\n",
      "Epoch 00699: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2928 - accuracy: 0.8425 - val_loss: 0.6654 - val_accuracy: 0.8123\n",
      "Epoch 700/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.2932 - accuracy: 0.8406\n",
      "Epoch 00700: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2932 - accuracy: 0.8406 - val_loss: 0.6938 - val_accuracy: 0.8085\n",
      "Epoch 701/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2971 - accuracy: 0.8393\n",
      "Epoch 00701: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2970 - accuracy: 0.8394 - val_loss: 0.6230 - val_accuracy: 0.8158\n",
      "Epoch 702/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2943 - accuracy: 0.8411\n",
      "Epoch 00702: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2942 - accuracy: 0.8411 - val_loss: 0.6812 - val_accuracy: 0.8112\n",
      "Epoch 703/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2953 - accuracy: 0.8412\n",
      "Epoch 00703: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2954 - accuracy: 0.8411 - val_loss: 0.6669 - val_accuracy: 0.8108\n",
      "Epoch 704/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.2947 - accuracy: 0.8410\n",
      "Epoch 00704: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2952 - accuracy: 0.8405 - val_loss: 0.6605 - val_accuracy: 0.8119\n",
      "Epoch 705/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.2942 - accuracy: 0.8421\n",
      "Epoch 00705: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2945 - accuracy: 0.8420 - val_loss: 0.6658 - val_accuracy: 0.8181\n",
      "Epoch 706/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.2961 - accuracy: 0.8406\n",
      "Epoch 00706: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2967 - accuracy: 0.8402 - val_loss: 0.6449 - val_accuracy: 0.8143\n",
      "Epoch 707/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.2937 - accuracy: 0.8405\n",
      "Epoch 00707: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2937 - accuracy: 0.8405 - val_loss: 0.6322 - val_accuracy: 0.8108\n",
      "Epoch 708/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.2946 - accuracy: 0.8403\n",
      "Epoch 00708: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2947 - accuracy: 0.8403 - val_loss: 0.6790 - val_accuracy: 0.8096\n",
      "Epoch 709/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.2964 - accuracy: 0.8387\n",
      "Epoch 00709: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 107us/sample - loss: 0.2962 - accuracy: 0.8389 - val_loss: 0.6346 - val_accuracy: 0.8174\n",
      "Epoch 710/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.2947 - accuracy: 0.8397\n",
      "Epoch 00710: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2948 - accuracy: 0.8397 - val_loss: 0.6395 - val_accuracy: 0.8131\n",
      "Epoch 711/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2952 - accuracy: 0.8408\n",
      "Epoch 00711: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2946 - accuracy: 0.8412 - val_loss: 0.6267 - val_accuracy: 0.8085\n",
      "Epoch 712/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.2958 - accuracy: 0.8402\n",
      "Epoch 00712: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2956 - accuracy: 0.8403 - val_loss: 0.6136 - val_accuracy: 0.8065\n",
      "Epoch 713/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.2934 - accuracy: 0.8412\n",
      "Epoch 00713: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2939 - accuracy: 0.8413 - val_loss: 0.6810 - val_accuracy: 0.8135\n",
      "Epoch 714/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.2939 - accuracy: 0.8416\n",
      "Epoch 00714: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2938 - accuracy: 0.8416 - val_loss: 0.6407 - val_accuracy: 0.8065\n",
      "Epoch 715/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2950 - accuracy: 0.8406\n",
      "Epoch 00715: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2945 - accuracy: 0.8409 - val_loss: 0.6250 - val_accuracy: 0.8154\n",
      "Epoch 716/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2931 - accuracy: 0.8422\n",
      "Epoch 00716: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2931 - accuracy: 0.8422 - val_loss: 0.6380 - val_accuracy: 0.8085\n",
      "Epoch 717/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.2942 - accuracy: 0.8411\n",
      "Epoch 00717: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2945 - accuracy: 0.8410 - val_loss: 0.6487 - val_accuracy: 0.8127\n",
      "Epoch 718/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.2957 - accuracy: 0.8402\n",
      "Epoch 00718: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2958 - accuracy: 0.8403 - val_loss: 0.6408 - val_accuracy: 0.8209\n",
      "Epoch 719/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2964 - accuracy: 0.8407\n",
      "Epoch 00719: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2968 - accuracy: 0.8404 - val_loss: 0.6168 - val_accuracy: 0.8181\n",
      "Epoch 720/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2958 - accuracy: 0.8389\n",
      "Epoch 00720: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2954 - accuracy: 0.8391 - val_loss: 0.6700 - val_accuracy: 0.8112\n",
      "Epoch 721/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2939 - accuracy: 0.8410\n",
      "Epoch 00721: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2939 - accuracy: 0.8410 - val_loss: 0.6137 - val_accuracy: 0.8158\n",
      "Epoch 722/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2928 - accuracy: 0.8412\n",
      "Epoch 00722: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2924 - accuracy: 0.8415 - val_loss: 0.6632 - val_accuracy: 0.8135\n",
      "Epoch 723/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2952 - accuracy: 0.8420\n",
      "Epoch 00723: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2945 - accuracy: 0.8422 - val_loss: 0.6404 - val_accuracy: 0.8174\n",
      "Epoch 724/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2944 - accuracy: 0.8415\n",
      "Epoch 00724: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2945 - accuracy: 0.8415 - val_loss: 0.6015 - val_accuracy: 0.8174\n",
      "Epoch 725/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2919 - accuracy: 0.8412\n",
      "Epoch 00725: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2920 - accuracy: 0.8412 - val_loss: 0.6151 - val_accuracy: 0.8088\n",
      "Epoch 726/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.2962 - accuracy: 0.8416\n",
      "Epoch 00726: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2961 - accuracy: 0.8417 - val_loss: 0.6125 - val_accuracy: 0.8150\n",
      "Epoch 727/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.2922 - accuracy: 0.8422\n",
      "Epoch 00727: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 107us/sample - loss: 0.2924 - accuracy: 0.8419 - val_loss: 0.6518 - val_accuracy: 0.8127\n",
      "Epoch 728/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.2964 - accuracy: 0.8392\n",
      "Epoch 00728: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2957 - accuracy: 0.8396 - val_loss: 0.6390 - val_accuracy: 0.8150\n",
      "Epoch 729/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2920 - accuracy: 0.8409\n",
      "Epoch 00729: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2923 - accuracy: 0.8412 - val_loss: 0.6767 - val_accuracy: 0.8143\n",
      "Epoch 730/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2949 - accuracy: 0.8405\n",
      "Epoch 00730: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2946 - accuracy: 0.8406 - val_loss: 0.6455 - val_accuracy: 0.8158\n",
      "Epoch 731/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2923 - accuracy: 0.8425\n",
      "Epoch 00731: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2923 - accuracy: 0.8426 - val_loss: 0.6441 - val_accuracy: 0.8193\n",
      "Epoch 732/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.2939 - accuracy: 0.8424\n",
      "Epoch 00732: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2933 - accuracy: 0.8430 - val_loss: 0.6537 - val_accuracy: 0.8123\n",
      "Epoch 733/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2953 - accuracy: 0.8416\n",
      "Epoch 00733: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2953 - accuracy: 0.8416 - val_loss: 0.6332 - val_accuracy: 0.8209\n",
      "Epoch 734/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.2953 - accuracy: 0.8405\n",
      "Epoch 00734: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 112us/sample - loss: 0.2955 - accuracy: 0.8407 - val_loss: 0.6145 - val_accuracy: 0.8178\n",
      "Epoch 735/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.2969 - accuracy: 0.8394\n",
      "Epoch 00735: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2964 - accuracy: 0.8399 - val_loss: 0.6317 - val_accuracy: 0.8162\n",
      "Epoch 736/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.2949 - accuracy: 0.8412\n",
      "Epoch 00736: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2951 - accuracy: 0.8407 - val_loss: 0.6216 - val_accuracy: 0.8158\n",
      "Epoch 737/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.2927 - accuracy: 0.8408\n",
      "Epoch 00737: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2920 - accuracy: 0.8410 - val_loss: 0.6648 - val_accuracy: 0.8100\n",
      "Epoch 738/1000\n",
      "22688/23208 [============================>.] - ETA: 0s - loss: 0.2929 - accuracy: 0.8408\n",
      "Epoch 00738: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2934 - accuracy: 0.8407 - val_loss: 0.6333 - val_accuracy: 0.8170\n",
      "Epoch 739/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2927 - accuracy: 0.8414\n",
      "Epoch 00739: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2927 - accuracy: 0.8414 - val_loss: 0.6441 - val_accuracy: 0.8185\n",
      "Epoch 740/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2939 - accuracy: 0.8401\n",
      "Epoch 00740: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2936 - accuracy: 0.8403 - val_loss: 0.6511 - val_accuracy: 0.8147\n",
      "Epoch 741/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2953 - accuracy: 0.8403\n",
      "Epoch 00741: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2953 - accuracy: 0.8402 - val_loss: 0.6621 - val_accuracy: 0.8050\n",
      "Epoch 742/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2965 - accuracy: 0.8381\n",
      "Epoch 00742: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2964 - accuracy: 0.8381 - val_loss: 0.5981 - val_accuracy: 0.8147\n",
      "Epoch 743/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2957 - accuracy: 0.8406\n",
      "Epoch 00743: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2955 - accuracy: 0.8405 - val_loss: 0.6561 - val_accuracy: 0.8170\n",
      "Epoch 744/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.2951 - accuracy: 0.8414\n",
      "Epoch 00744: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2949 - accuracy: 0.8415 - val_loss: 0.6377 - val_accuracy: 0.8154\n",
      "Epoch 745/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2910 - accuracy: 0.8426\n",
      "Epoch 00745: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2910 - accuracy: 0.8426 - val_loss: 0.6867 - val_accuracy: 0.8147\n",
      "Epoch 746/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2923 - accuracy: 0.8399\n",
      "Epoch 00746: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2920 - accuracy: 0.8401 - val_loss: 0.6387 - val_accuracy: 0.8158\n",
      "Epoch 747/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2922 - accuracy: 0.8419\n",
      "Epoch 00747: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2919 - accuracy: 0.8421 - val_loss: 0.6753 - val_accuracy: 0.8174\n",
      "Epoch 748/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2958 - accuracy: 0.8388\n",
      "Epoch 00748: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2966 - accuracy: 0.8388 - val_loss: 0.6287 - val_accuracy: 0.8158\n",
      "Epoch 749/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2957 - accuracy: 0.8416\n",
      "Epoch 00749: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2956 - accuracy: 0.8416 - val_loss: 0.6163 - val_accuracy: 0.8143\n",
      "Epoch 750/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2954 - accuracy: 0.8408\n",
      "Epoch 00750: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2951 - accuracy: 0.8407 - val_loss: 0.6196 - val_accuracy: 0.8147\n",
      "Epoch 751/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.2932 - accuracy: 0.8409\n",
      "Epoch 00751: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2933 - accuracy: 0.8408 - val_loss: 0.6519 - val_accuracy: 0.8166\n",
      "Epoch 752/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2937 - accuracy: 0.8430\n",
      "Epoch 00752: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2936 - accuracy: 0.8430 - val_loss: 0.6225 - val_accuracy: 0.8154\n",
      "Epoch 753/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2919 - accuracy: 0.8415\n",
      "Epoch 00753: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2916 - accuracy: 0.8418 - val_loss: 0.6630 - val_accuracy: 0.8147\n",
      "Epoch 754/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2941 - accuracy: 0.8410\n",
      "Epoch 00754: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2940 - accuracy: 0.8412 - val_loss: 0.6444 - val_accuracy: 0.8127\n",
      "Epoch 755/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2916 - accuracy: 0.8420\n",
      "Epoch 00755: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2915 - accuracy: 0.8416 - val_loss: 0.6891 - val_accuracy: 0.8127\n",
      "Epoch 756/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2957 - accuracy: 0.8412\n",
      "Epoch 00756: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2957 - accuracy: 0.8411 - val_loss: 0.6603 - val_accuracy: 0.8108\n",
      "Epoch 757/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.2932 - accuracy: 0.8410\n",
      "Epoch 00757: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2934 - accuracy: 0.8410 - val_loss: 0.6336 - val_accuracy: 0.8181\n",
      "Epoch 758/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.2949 - accuracy: 0.8404\n",
      "Epoch 00758: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2944 - accuracy: 0.8405 - val_loss: 0.6453 - val_accuracy: 0.8143\n",
      "Epoch 759/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.2944 - accuracy: 0.8398\n",
      "Epoch 00759: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2937 - accuracy: 0.8400 - val_loss: 0.6403 - val_accuracy: 0.8154\n",
      "Epoch 760/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.2939 - accuracy: 0.8420\n",
      "Epoch 00760: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2937 - accuracy: 0.8423 - val_loss: 0.6608 - val_accuracy: 0.8154\n",
      "Epoch 761/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.2975 - accuracy: 0.8407\n",
      "Epoch 00761: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2976 - accuracy: 0.8404 - val_loss: 0.6363 - val_accuracy: 0.8154\n",
      "Epoch 762/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.2969 - accuracy: 0.8409\n",
      "Epoch 00762: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2963 - accuracy: 0.8411 - val_loss: 0.6354 - val_accuracy: 0.8150\n",
      "Epoch 763/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.2979 - accuracy: 0.8417\n",
      "Epoch 00763: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2980 - accuracy: 0.8414 - val_loss: 0.6420 - val_accuracy: 0.8166\n",
      "Epoch 764/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2960 - accuracy: 0.8405\n",
      "Epoch 00764: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2959 - accuracy: 0.8404 - val_loss: 0.6386 - val_accuracy: 0.8166\n",
      "Epoch 765/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.2941 - accuracy: 0.8420\n",
      "Epoch 00765: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2939 - accuracy: 0.8421 - val_loss: 0.6414 - val_accuracy: 0.8112\n",
      "Epoch 766/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.2931 - accuracy: 0.8417\n",
      "Epoch 00766: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2929 - accuracy: 0.8416 - val_loss: 0.6447 - val_accuracy: 0.8131\n",
      "Epoch 767/1000\n",
      "22688/23208 [============================>.] - ETA: 0s - loss: 0.2925 - accuracy: 0.8416\n",
      "Epoch 00767: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2928 - accuracy: 0.8413 - val_loss: 0.6501 - val_accuracy: 0.8065\n",
      "Epoch 768/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.2948 - accuracy: 0.8400\n",
      "Epoch 00768: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2953 - accuracy: 0.8397 - val_loss: 0.6474 - val_accuracy: 0.8116\n",
      "Epoch 769/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2942 - accuracy: 0.8422\n",
      "Epoch 00769: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2947 - accuracy: 0.8420 - val_loss: 0.6271 - val_accuracy: 0.8119\n",
      "Epoch 770/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.2910 - accuracy: 0.8418\n",
      "Epoch 00770: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2912 - accuracy: 0.8415 - val_loss: 0.6776 - val_accuracy: 0.8088\n",
      "Epoch 771/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2959 - accuracy: 0.8401\n",
      "Epoch 00771: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2956 - accuracy: 0.8401 - val_loss: 0.6604 - val_accuracy: 0.8116\n",
      "Epoch 772/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.2956 - accuracy: 0.8410\n",
      "Epoch 00772: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2959 - accuracy: 0.8411 - val_loss: 0.6379 - val_accuracy: 0.8065\n",
      "Epoch 773/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.2948 - accuracy: 0.8415\n",
      "Epoch 00773: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2949 - accuracy: 0.8414 - val_loss: 0.6317 - val_accuracy: 0.8147\n",
      "Epoch 774/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.2932 - accuracy: 0.8425\n",
      "Epoch 00774: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2935 - accuracy: 0.8421 - val_loss: 0.6668 - val_accuracy: 0.8112\n",
      "Epoch 775/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2936 - accuracy: 0.8409\n",
      "Epoch 00775: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2934 - accuracy: 0.8409 - val_loss: 0.6939 - val_accuracy: 0.8150\n",
      "Epoch 776/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2968 - accuracy: 0.8399\n",
      "Epoch 00776: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2965 - accuracy: 0.8400 - val_loss: 0.6239 - val_accuracy: 0.8185\n",
      "Epoch 777/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.2942 - accuracy: 0.8427\n",
      "Epoch 00777: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2940 - accuracy: 0.8432 - val_loss: 0.6158 - val_accuracy: 0.8178\n",
      "Epoch 778/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2937 - accuracy: 0.8402\n",
      "Epoch 00778: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2939 - accuracy: 0.8400 - val_loss: 0.6419 - val_accuracy: 0.8131\n",
      "Epoch 779/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.2935 - accuracy: 0.8408\n",
      "Epoch 00779: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2937 - accuracy: 0.8406 - val_loss: 0.6326 - val_accuracy: 0.8131\n",
      "Epoch 780/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2930 - accuracy: 0.8403\n",
      "Epoch 00780: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2928 - accuracy: 0.8402 - val_loss: 0.6337 - val_accuracy: 0.8139\n",
      "Epoch 781/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2921 - accuracy: 0.8423\n",
      "Epoch 00781: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2923 - accuracy: 0.8422 - val_loss: 0.6352 - val_accuracy: 0.8123\n",
      "Epoch 782/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.2924 - accuracy: 0.8431\n",
      "Epoch 00782: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2918 - accuracy: 0.8432 - val_loss: 0.6264 - val_accuracy: 0.8131\n",
      "Epoch 783/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.2936 - accuracy: 0.8413\n",
      "Epoch 00783: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2930 - accuracy: 0.8416 - val_loss: 0.6370 - val_accuracy: 0.8154\n",
      "Epoch 784/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2949 - accuracy: 0.8402\n",
      "Epoch 00784: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2951 - accuracy: 0.8401 - val_loss: 0.6586 - val_accuracy: 0.8147\n",
      "Epoch 785/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.2928 - accuracy: 0.8410\n",
      "Epoch 00785: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2933 - accuracy: 0.8408 - val_loss: 0.6272 - val_accuracy: 0.8143\n",
      "Epoch 786/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.2920 - accuracy: 0.8401\n",
      "Epoch 00786: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2922 - accuracy: 0.8402 - val_loss: 0.6863 - val_accuracy: 0.8104\n",
      "Epoch 787/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.2926 - accuracy: 0.8435\n",
      "Epoch 00787: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2926 - accuracy: 0.8435 - val_loss: 0.6407 - val_accuracy: 0.8127\n",
      "Epoch 788/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2951 - accuracy: 0.8402\n",
      "Epoch 00788: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2950 - accuracy: 0.8398 - val_loss: 0.6197 - val_accuracy: 0.8135\n",
      "Epoch 789/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2950 - accuracy: 0.8414\n",
      "Epoch 00789: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2951 - accuracy: 0.8415 - val_loss: 0.6069 - val_accuracy: 0.8174\n",
      "Epoch 790/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2925 - accuracy: 0.8431\n",
      "Epoch 00790: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2927 - accuracy: 0.8429 - val_loss: 0.6253 - val_accuracy: 0.8170\n",
      "Epoch 791/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2923 - accuracy: 0.8404\n",
      "Epoch 00791: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 117us/sample - loss: 0.2922 - accuracy: 0.8405 - val_loss: 0.6581 - val_accuracy: 0.8088\n",
      "Epoch 792/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2940 - accuracy: 0.8397\n",
      "Epoch 00792: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2944 - accuracy: 0.8395 - val_loss: 0.6253 - val_accuracy: 0.8150\n",
      "Epoch 793/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.2955 - accuracy: 0.8408\n",
      "Epoch 00793: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2957 - accuracy: 0.8405 - val_loss: 0.6156 - val_accuracy: 0.8197\n",
      "Epoch 794/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.2946 - accuracy: 0.8406\n",
      "Epoch 00794: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2947 - accuracy: 0.8406 - val_loss: 0.6292 - val_accuracy: 0.8112\n",
      "Epoch 795/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.2928 - accuracy: 0.8425\n",
      "Epoch 00795: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2924 - accuracy: 0.8429 - val_loss: 0.6666 - val_accuracy: 0.8088\n",
      "Epoch 796/1000\n",
      "22688/23208 [============================>.] - ETA: 0s - loss: 0.2934 - accuracy: 0.8399\n",
      "Epoch 00796: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.2936 - accuracy: 0.8395 - val_loss: 0.6605 - val_accuracy: 0.8073\n",
      "Epoch 797/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2928 - accuracy: 0.8401\n",
      "Epoch 00797: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.2927 - accuracy: 0.8402 - val_loss: 0.6645 - val_accuracy: 0.8158\n",
      "Epoch 798/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2958 - accuracy: 0.8412\n",
      "Epoch 00798: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2958 - accuracy: 0.8412 - val_loss: 0.6458 - val_accuracy: 0.8119\n",
      "Epoch 799/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2940 - accuracy: 0.8413\n",
      "Epoch 00799: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2937 - accuracy: 0.8416 - val_loss: 0.6463 - val_accuracy: 0.8147\n",
      "Epoch 800/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.2953 - accuracy: 0.8414\n",
      "Epoch 00800: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 99us/sample - loss: 0.2948 - accuracy: 0.8417 - val_loss: 0.6255 - val_accuracy: 0.8154\n",
      "Epoch 801/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2966 - accuracy: 0.8411\n",
      "Epoch 00801: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.2966 - accuracy: 0.8408 - val_loss: 0.6567 - val_accuracy: 0.8170\n",
      "Epoch 802/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.2953 - accuracy: 0.8412\n",
      "Epoch 00802: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.2955 - accuracy: 0.8412 - val_loss: 0.6308 - val_accuracy: 0.8170\n",
      "Epoch 803/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2929 - accuracy: 0.8415\n",
      "Epoch 00803: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.2929 - accuracy: 0.8415 - val_loss: 0.6292 - val_accuracy: 0.8143\n",
      "Epoch 804/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.2950 - accuracy: 0.8413\n",
      "Epoch 00804: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 99us/sample - loss: 0.2954 - accuracy: 0.8413 - val_loss: 0.6619 - val_accuracy: 0.8046\n",
      "Epoch 805/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2932 - accuracy: 0.8409\n",
      "Epoch 00805: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 99us/sample - loss: 0.2931 - accuracy: 0.8410 - val_loss: 0.6586 - val_accuracy: 0.8131\n",
      "Epoch 806/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2923 - accuracy: 0.8399\n",
      "Epoch 00806: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 99us/sample - loss: 0.2927 - accuracy: 0.8396 - val_loss: 0.6251 - val_accuracy: 0.8077\n",
      "Epoch 807/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2910 - accuracy: 0.8436\n",
      "Epoch 00807: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2920 - accuracy: 0.8429 - val_loss: 0.6476 - val_accuracy: 0.8189\n",
      "Epoch 808/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.2938 - accuracy: 0.8401\n",
      "Epoch 00808: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2929 - accuracy: 0.8409 - val_loss: 0.6754 - val_accuracy: 0.8011\n",
      "Epoch 809/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2921 - accuracy: 0.8411\n",
      "Epoch 00809: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2921 - accuracy: 0.8411 - val_loss: 0.6593 - val_accuracy: 0.8139\n",
      "Epoch 810/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2942 - accuracy: 0.8402\n",
      "Epoch 00810: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2940 - accuracy: 0.8403 - val_loss: 0.6482 - val_accuracy: 0.8205\n",
      "Epoch 811/1000\n",
      "22688/23208 [============================>.] - ETA: 0s - loss: 0.2935 - accuracy: 0.8404\n",
      "Epoch 00811: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.2940 - accuracy: 0.8403 - val_loss: 0.6573 - val_accuracy: 0.8116\n",
      "Epoch 812/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2941 - accuracy: 0.8407\n",
      "Epoch 00812: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2942 - accuracy: 0.8406 - val_loss: 0.6491 - val_accuracy: 0.8147\n",
      "Epoch 813/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2937 - accuracy: 0.8409\n",
      "Epoch 00813: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2941 - accuracy: 0.8406 - val_loss: 0.6095 - val_accuracy: 0.8154\n",
      "Epoch 814/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2960 - accuracy: 0.8414\n",
      "Epoch 00814: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 99us/sample - loss: 0.2963 - accuracy: 0.8412 - val_loss: 0.6010 - val_accuracy: 0.7995\n",
      "Epoch 815/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.2933 - accuracy: 0.8402\n",
      "Epoch 00815: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2934 - accuracy: 0.8401 - val_loss: 0.6862 - val_accuracy: 0.8143\n",
      "Epoch 816/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.2956 - accuracy: 0.8413\n",
      "Epoch 00816: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2961 - accuracy: 0.8407 - val_loss: 0.6096 - val_accuracy: 0.8181\n",
      "Epoch 817/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2960 - accuracy: 0.8409\n",
      "Epoch 00817: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2964 - accuracy: 0.8407 - val_loss: 0.6279 - val_accuracy: 0.8178\n",
      "Epoch 818/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.2932 - accuracy: 0.8422\n",
      "Epoch 00818: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2941 - accuracy: 0.8412 - val_loss: 0.6483 - val_accuracy: 0.8154\n",
      "Epoch 819/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2947 - accuracy: 0.8401\n",
      "Epoch 00819: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2944 - accuracy: 0.8403 - val_loss: 0.6771 - val_accuracy: 0.8158\n",
      "Epoch 820/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.2922 - accuracy: 0.8403\n",
      "Epoch 00820: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2927 - accuracy: 0.8400 - val_loss: 0.6627 - val_accuracy: 0.8139\n",
      "Epoch 821/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2936 - accuracy: 0.8415\n",
      "Epoch 00821: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.2937 - accuracy: 0.8414 - val_loss: 0.6770 - val_accuracy: 0.8135\n",
      "Epoch 822/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2926 - accuracy: 0.8432\n",
      "Epoch 00822: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 99us/sample - loss: 0.2929 - accuracy: 0.8429 - val_loss: 0.6669 - val_accuracy: 0.8119\n",
      "Epoch 823/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.2925 - accuracy: 0.8427\n",
      "Epoch 00823: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2927 - accuracy: 0.8427 - val_loss: 0.6373 - val_accuracy: 0.8150\n",
      "Epoch 824/1000\n",
      "22688/23208 [============================>.] - ETA: 0s - loss: 0.2954 - accuracy: 0.8397\n",
      "Epoch 00824: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2951 - accuracy: 0.8399 - val_loss: 0.6211 - val_accuracy: 0.8181\n",
      "Epoch 825/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.2972 - accuracy: 0.8397\n",
      "Epoch 00825: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 99us/sample - loss: 0.2973 - accuracy: 0.8398 - val_loss: 0.6041 - val_accuracy: 0.8170\n",
      "Epoch 826/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.2954 - accuracy: 0.8421\n",
      "Epoch 00826: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2952 - accuracy: 0.8416 - val_loss: 0.6530 - val_accuracy: 0.8112\n",
      "Epoch 827/1000\n",
      "22688/23208 [============================>.] - ETA: 0s - loss: 0.2943 - accuracy: 0.8437\n",
      "Epoch 00827: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.2944 - accuracy: 0.8436 - val_loss: 0.6188 - val_accuracy: 0.8189\n",
      "Epoch 828/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.2933 - accuracy: 0.8425\n",
      "Epoch 00828: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 99us/sample - loss: 0.2934 - accuracy: 0.8426 - val_loss: 0.6421 - val_accuracy: 0.8131\n",
      "Epoch 829/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.2934 - accuracy: 0.8422\n",
      "Epoch 00829: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2933 - accuracy: 0.8421 - val_loss: 0.6242 - val_accuracy: 0.8166\n",
      "Epoch 830/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.2927 - accuracy: 0.8438\n",
      "Epoch 00830: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2928 - accuracy: 0.8438 - val_loss: 0.6481 - val_accuracy: 0.8135\n",
      "Epoch 831/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.2944 - accuracy: 0.8414\n",
      "Epoch 00831: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.2944 - accuracy: 0.8413 - val_loss: 0.6253 - val_accuracy: 0.8162\n",
      "Epoch 832/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2924 - accuracy: 0.8411\n",
      "Epoch 00832: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.2921 - accuracy: 0.8413 - val_loss: 0.6709 - val_accuracy: 0.8170\n",
      "Epoch 833/1000\n",
      "22656/23208 [============================>.] - ETA: 0s - loss: 0.2949 - accuracy: 0.8394\n",
      "Epoch 00833: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.2947 - accuracy: 0.8397 - val_loss: 0.6122 - val_accuracy: 0.8189\n",
      "Epoch 834/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.2918 - accuracy: 0.8413\n",
      "Epoch 00834: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.2921 - accuracy: 0.8415 - val_loss: 0.6317 - val_accuracy: 0.8162\n",
      "Epoch 835/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.2949 - accuracy: 0.8419\n",
      "Epoch 00835: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.2948 - accuracy: 0.8420 - val_loss: 0.6437 - val_accuracy: 0.8147\n",
      "Epoch 836/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.2933 - accuracy: 0.8408\n",
      "Epoch 00836: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2930 - accuracy: 0.8408 - val_loss: 0.6052 - val_accuracy: 0.8224\n",
      "Epoch 837/1000\n",
      "22688/23208 [============================>.] - ETA: 0s - loss: 0.2937 - accuracy: 0.8432\n",
      "Epoch 00837: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2940 - accuracy: 0.8429 - val_loss: 0.6284 - val_accuracy: 0.8189\n",
      "Epoch 838/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.2934 - accuracy: 0.8398\n",
      "Epoch 00838: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2944 - accuracy: 0.8397 - val_loss: 0.6329 - val_accuracy: 0.8193\n",
      "Epoch 839/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.2942 - accuracy: 0.8404\n",
      "Epoch 00839: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 99us/sample - loss: 0.2938 - accuracy: 0.8408 - val_loss: 0.6381 - val_accuracy: 0.8166\n",
      "Epoch 840/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2943 - accuracy: 0.8430\n",
      "Epoch 00840: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.2942 - accuracy: 0.8430 - val_loss: 0.6764 - val_accuracy: 0.8100\n",
      "Epoch 841/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2934 - accuracy: 0.8424\n",
      "Epoch 00841: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 98us/sample - loss: 0.2935 - accuracy: 0.8423 - val_loss: 0.6359 - val_accuracy: 0.8158\n",
      "Epoch 842/1000\n",
      "22656/23208 [============================>.] - ETA: 0s - loss: 0.2939 - accuracy: 0.8414\n",
      "Epoch 00842: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.2939 - accuracy: 0.8418 - val_loss: 0.6360 - val_accuracy: 0.8174\n",
      "Epoch 843/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.2945 - accuracy: 0.8409\n",
      "Epoch 00843: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 99us/sample - loss: 0.2945 - accuracy: 0.8408 - val_loss: 0.6263 - val_accuracy: 0.8139\n",
      "Epoch 844/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.2921 - accuracy: 0.8421\n",
      "Epoch 00844: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.2916 - accuracy: 0.8426 - val_loss: 0.6636 - val_accuracy: 0.8162\n",
      "Epoch 845/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.2930 - accuracy: 0.8421\n",
      "Epoch 00845: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2931 - accuracy: 0.8419 - val_loss: 0.6575 - val_accuracy: 0.8197\n",
      "Epoch 846/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2940 - accuracy: 0.8409\n",
      "Epoch 00846: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2938 - accuracy: 0.8410 - val_loss: 0.6114 - val_accuracy: 0.8139\n",
      "Epoch 847/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2954 - accuracy: 0.8403\n",
      "Epoch 00847: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2951 - accuracy: 0.8407 - val_loss: 0.6408 - val_accuracy: 0.8104\n",
      "Epoch 848/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2913 - accuracy: 0.8421\n",
      "Epoch 00848: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.2910 - accuracy: 0.8422 - val_loss: 0.6341 - val_accuracy: 0.8147\n",
      "Epoch 849/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.2923 - accuracy: 0.8419\n",
      "Epoch 00849: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2935 - accuracy: 0.8419 - val_loss: 0.6749 - val_accuracy: 0.8057\n",
      "Epoch 850/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.2927 - accuracy: 0.8417\n",
      "Epoch 00850: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.2927 - accuracy: 0.8416 - val_loss: 0.6312 - val_accuracy: 0.8170\n",
      "Epoch 851/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.2923 - accuracy: 0.8423\n",
      "Epoch 00851: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.2924 - accuracy: 0.8419 - val_loss: 0.6770 - val_accuracy: 0.8131\n",
      "Epoch 852/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.2924 - accuracy: 0.8401\n",
      "Epoch 00852: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 98us/sample - loss: 0.2924 - accuracy: 0.8401 - val_loss: 0.6660 - val_accuracy: 0.8158\n",
      "Epoch 853/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2960 - accuracy: 0.8397\n",
      "Epoch 00853: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.2960 - accuracy: 0.8395 - val_loss: 0.6340 - val_accuracy: 0.8174\n",
      "Epoch 854/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.2941 - accuracy: 0.8417\n",
      "Epoch 00854: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2943 - accuracy: 0.8414 - val_loss: 0.6700 - val_accuracy: 0.8147\n",
      "Epoch 855/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2930 - accuracy: 0.8410\n",
      "Epoch 00855: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2922 - accuracy: 0.8415 - val_loss: 0.6668 - val_accuracy: 0.8139\n",
      "Epoch 856/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.2963 - accuracy: 0.8388\n",
      "Epoch 00856: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2962 - accuracy: 0.8387 - val_loss: 0.6003 - val_accuracy: 0.8131\n",
      "Epoch 857/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.2958 - accuracy: 0.8411\n",
      "Epoch 00857: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2954 - accuracy: 0.8414 - val_loss: 0.6331 - val_accuracy: 0.8131\n",
      "Epoch 858/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.2921 - accuracy: 0.8420\n",
      "Epoch 00858: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2918 - accuracy: 0.8420 - val_loss: 0.6152 - val_accuracy: 0.8162\n",
      "Epoch 859/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2914 - accuracy: 0.8416\n",
      "Epoch 00859: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2911 - accuracy: 0.8418 - val_loss: 0.6526 - val_accuracy: 0.8174\n",
      "Epoch 860/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2918 - accuracy: 0.8405\n",
      "Epoch 00860: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2917 - accuracy: 0.8407 - val_loss: 0.6365 - val_accuracy: 0.8174\n",
      "Epoch 861/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.2921 - accuracy: 0.8424\n",
      "Epoch 00861: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2921 - accuracy: 0.8423 - val_loss: 0.6688 - val_accuracy: 0.8123\n",
      "Epoch 862/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.2932 - accuracy: 0.8400\n",
      "Epoch 00862: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2935 - accuracy: 0.8399 - val_loss: 0.6629 - val_accuracy: 0.8123\n",
      "Epoch 863/1000\n",
      "22688/23208 [============================>.] - ETA: 0s - loss: 0.2926 - accuracy: 0.8416\n",
      "Epoch 00863: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.2933 - accuracy: 0.8413 - val_loss: 0.6276 - val_accuracy: 0.8139\n",
      "Epoch 864/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.2929 - accuracy: 0.8414\n",
      "Epoch 00864: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.2930 - accuracy: 0.8408 - val_loss: 0.6738 - val_accuracy: 0.8131\n",
      "Epoch 865/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.2959 - accuracy: 0.8385\n",
      "Epoch 00865: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2957 - accuracy: 0.8386 - val_loss: 0.6345 - val_accuracy: 0.8123\n",
      "Epoch 866/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2942 - accuracy: 0.8393\n",
      "Epoch 00866: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2942 - accuracy: 0.8394 - val_loss: 0.6331 - val_accuracy: 0.8143\n",
      "Epoch 867/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.2938 - accuracy: 0.8425\n",
      "Epoch 00867: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 99us/sample - loss: 0.2937 - accuracy: 0.8423 - val_loss: 0.6492 - val_accuracy: 0.8178\n",
      "Epoch 868/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2911 - accuracy: 0.8428\n",
      "Epoch 00868: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 99us/sample - loss: 0.2913 - accuracy: 0.8428 - val_loss: 0.6676 - val_accuracy: 0.8127\n",
      "Epoch 869/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.2945 - accuracy: 0.8408\n",
      "Epoch 00869: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2945 - accuracy: 0.8406 - val_loss: 0.6617 - val_accuracy: 0.8131\n",
      "Epoch 870/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2937 - accuracy: 0.8403\n",
      "Epoch 00870: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2939 - accuracy: 0.8403 - val_loss: 0.6382 - val_accuracy: 0.8154\n",
      "Epoch 871/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.2905 - accuracy: 0.8411\n",
      "Epoch 00871: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2914 - accuracy: 0.8410 - val_loss: 0.6314 - val_accuracy: 0.8127\n",
      "Epoch 872/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.2922 - accuracy: 0.8424\n",
      "Epoch 00872: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2926 - accuracy: 0.8423 - val_loss: 0.6774 - val_accuracy: 0.8139\n",
      "Epoch 873/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2951 - accuracy: 0.8392\n",
      "Epoch 00873: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2948 - accuracy: 0.8393 - val_loss: 0.6665 - val_accuracy: 0.8104\n",
      "Epoch 874/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.2951 - accuracy: 0.8403\n",
      "Epoch 00874: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2947 - accuracy: 0.8405 - val_loss: 0.6795 - val_accuracy: 0.8147\n",
      "Epoch 875/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2924 - accuracy: 0.8407\n",
      "Epoch 00875: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.2924 - accuracy: 0.8406 - val_loss: 0.6795 - val_accuracy: 0.8119\n",
      "Epoch 876/1000\n",
      "22688/23208 [============================>.] - ETA: 0s - loss: 0.2980 - accuracy: 0.8400\n",
      "Epoch 00876: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2979 - accuracy: 0.8401 - val_loss: 0.6079 - val_accuracy: 0.8162\n",
      "Epoch 877/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2930 - accuracy: 0.8399\n",
      "Epoch 00877: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2927 - accuracy: 0.8401 - val_loss: 0.6840 - val_accuracy: 0.8104\n",
      "Epoch 878/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.2918 - accuracy: 0.8410\n",
      "Epoch 00878: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2917 - accuracy: 0.8413 - val_loss: 0.6953 - val_accuracy: 0.8131\n",
      "Epoch 879/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2940 - accuracy: 0.8425\n",
      "Epoch 00879: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2939 - accuracy: 0.8423 - val_loss: 0.6474 - val_accuracy: 0.8178\n",
      "Epoch 880/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2949 - accuracy: 0.8411\n",
      "Epoch 00880: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.2948 - accuracy: 0.8410 - val_loss: 0.6628 - val_accuracy: 0.8112\n",
      "Epoch 881/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.2925 - accuracy: 0.8424\n",
      "Epoch 00881: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.2925 - accuracy: 0.8423 - val_loss: 0.6339 - val_accuracy: 0.8185\n",
      "Epoch 882/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.2939 - accuracy: 0.8416\n",
      "Epoch 00882: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 98us/sample - loss: 0.2938 - accuracy: 0.8415 - val_loss: 0.6424 - val_accuracy: 0.8193\n",
      "Epoch 883/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.2925 - accuracy: 0.8414\n",
      "Epoch 00883: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2926 - accuracy: 0.8414 - val_loss: 0.6811 - val_accuracy: 0.8150\n",
      "Epoch 884/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2927 - accuracy: 0.8411\n",
      "Epoch 00884: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2924 - accuracy: 0.8413 - val_loss: 0.6654 - val_accuracy: 0.8162\n",
      "Epoch 885/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2941 - accuracy: 0.8418\n",
      "Epoch 00885: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2939 - accuracy: 0.8417 - val_loss: 0.6607 - val_accuracy: 0.8150\n",
      "Epoch 886/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.2956 - accuracy: 0.8404 ETA: 0s - l\n",
      "Epoch 00886: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2956 - accuracy: 0.8405 - val_loss: 0.6192 - val_accuracy: 0.8127\n",
      "Epoch 887/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.2931 - accuracy: 0.8410\n",
      "Epoch 00887: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2927 - accuracy: 0.8411 - val_loss: 0.6485 - val_accuracy: 0.8147\n",
      "Epoch 888/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2930 - accuracy: 0.8413\n",
      "Epoch 00888: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2928 - accuracy: 0.8412 - val_loss: 0.6551 - val_accuracy: 0.8123\n",
      "Epoch 889/1000\n",
      "22688/23208 [============================>.] - ETA: 0s - loss: 0.2959 - accuracy: 0.8418\n",
      "Epoch 00889: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.2955 - accuracy: 0.8418 - val_loss: 0.6325 - val_accuracy: 0.8119\n",
      "Epoch 890/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2936 - accuracy: 0.8413\n",
      "Epoch 00890: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2937 - accuracy: 0.8411 - val_loss: 0.6679 - val_accuracy: 0.8170\n",
      "Epoch 891/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2959 - accuracy: 0.8405\n",
      "Epoch 00891: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2952 - accuracy: 0.8409 - val_loss: 0.6586 - val_accuracy: 0.8116\n",
      "Epoch 892/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.2936 - accuracy: 0.8410\n",
      "Epoch 00892: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2934 - accuracy: 0.8412 - val_loss: 0.6320 - val_accuracy: 0.8131\n",
      "Epoch 893/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.2939 - accuracy: 0.8392\n",
      "Epoch 00893: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.2940 - accuracy: 0.8391 - val_loss: 0.6680 - val_accuracy: 0.8147\n",
      "Epoch 894/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.2953 - accuracy: 0.8409\n",
      "Epoch 00894: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2953 - accuracy: 0.8410 - val_loss: 0.6468 - val_accuracy: 0.8108\n",
      "Epoch 895/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.2955 - accuracy: 0.8406\n",
      "Epoch 00895: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2958 - accuracy: 0.8405 - val_loss: 0.6536 - val_accuracy: 0.8116\n",
      "Epoch 896/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2941 - accuracy: 0.8380\n",
      "Epoch 00896: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2941 - accuracy: 0.8381 - val_loss: 0.6655 - val_accuracy: 0.8174\n",
      "Epoch 897/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2949 - accuracy: 0.8409\n",
      "Epoch 00897: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 99us/sample - loss: 0.2950 - accuracy: 0.8411 - val_loss: 0.6440 - val_accuracy: 0.8139\n",
      "Epoch 898/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.2939 - accuracy: 0.8399\n",
      "Epoch 00898: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2935 - accuracy: 0.8400 - val_loss: 0.6588 - val_accuracy: 0.8112\n",
      "Epoch 899/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2927 - accuracy: 0.8406\n",
      "Epoch 00899: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2927 - accuracy: 0.8407 - val_loss: 0.6375 - val_accuracy: 0.8158\n",
      "Epoch 900/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.2949 - accuracy: 0.8397\n",
      "Epoch 00900: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2946 - accuracy: 0.8399 - val_loss: 0.6339 - val_accuracy: 0.8162\n",
      "Epoch 901/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2934 - accuracy: 0.8407\n",
      "Epoch 00901: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2935 - accuracy: 0.8404 - val_loss: 0.6472 - val_accuracy: 0.8127\n",
      "Epoch 902/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.2964 - accuracy: 0.8411\n",
      "Epoch 00902: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2969 - accuracy: 0.8408 - val_loss: 0.5993 - val_accuracy: 0.8123\n",
      "Epoch 903/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.2943 - accuracy: 0.8418\n",
      "Epoch 00903: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2944 - accuracy: 0.8417 - val_loss: 0.6487 - val_accuracy: 0.8077\n",
      "Epoch 904/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2913 - accuracy: 0.8410\n",
      "Epoch 00904: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2912 - accuracy: 0.8412 - val_loss: 0.6560 - val_accuracy: 0.8174\n",
      "Epoch 905/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2937 - accuracy: 0.8417\n",
      "Epoch 00905: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2936 - accuracy: 0.8416 - val_loss: 0.6638 - val_accuracy: 0.8119\n",
      "Epoch 906/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.2943 - accuracy: 0.8398\n",
      "Epoch 00906: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2942 - accuracy: 0.8399 - val_loss: 0.6334 - val_accuracy: 0.8135\n",
      "Epoch 907/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.2934 - accuracy: 0.8408\n",
      "Epoch 00907: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2939 - accuracy: 0.8405 - val_loss: 0.6504 - val_accuracy: 0.8015\n",
      "Epoch 908/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2929 - accuracy: 0.8410\n",
      "Epoch 00908: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2927 - accuracy: 0.8413 - val_loss: 0.6537 - val_accuracy: 0.7953\n",
      "Epoch 909/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2935 - accuracy: 0.8400\n",
      "Epoch 00909: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2934 - accuracy: 0.8401 - val_loss: 0.6797 - val_accuracy: 0.8127\n",
      "Epoch 910/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.2943 - accuracy: 0.8404\n",
      "Epoch 00910: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2939 - accuracy: 0.8406 - val_loss: 0.5965 - val_accuracy: 0.8096\n",
      "Epoch 911/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2961 - accuracy: 0.8404\n",
      "Epoch 00911: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.2958 - accuracy: 0.8405 - val_loss: 0.6234 - val_accuracy: 0.8119\n",
      "Epoch 912/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2933 - accuracy: 0.8420\n",
      "Epoch 00912: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2932 - accuracy: 0.8420 - val_loss: 0.6411 - val_accuracy: 0.8154\n",
      "Epoch 913/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2909 - accuracy: 0.8409\n",
      "Epoch 00913: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2908 - accuracy: 0.8410 - val_loss: 0.6657 - val_accuracy: 0.8197\n",
      "Epoch 914/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2924 - accuracy: 0.8430\n",
      "Epoch 00914: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2925 - accuracy: 0.8429 - val_loss: 0.6439 - val_accuracy: 0.8085\n",
      "Epoch 915/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2933 - accuracy: 0.8402\n",
      "Epoch 00915: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2930 - accuracy: 0.8404 - val_loss: 0.6564 - val_accuracy: 0.8166\n",
      "Epoch 916/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.2934 - accuracy: 0.8422\n",
      "Epoch 00916: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2932 - accuracy: 0.8422 - val_loss: 0.6363 - val_accuracy: 0.8170\n",
      "Epoch 917/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2938 - accuracy: 0.8433\n",
      "Epoch 00917: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2936 - accuracy: 0.8433 - val_loss: 0.6521 - val_accuracy: 0.8139\n",
      "Epoch 918/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.2966 - accuracy: 0.8405\n",
      "Epoch 00918: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.2965 - accuracy: 0.8404 - val_loss: 0.6127 - val_accuracy: 0.8193\n",
      "Epoch 919/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2963 - accuracy: 0.8414\n",
      "Epoch 00919: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2966 - accuracy: 0.8414 - val_loss: 0.6219 - val_accuracy: 0.8154\n",
      "Epoch 920/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.2955 - accuracy: 0.8409\n",
      "Epoch 00920: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 99us/sample - loss: 0.2950 - accuracy: 0.8413 - val_loss: 0.6374 - val_accuracy: 0.8181\n",
      "Epoch 921/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.2932 - accuracy: 0.8424\n",
      "Epoch 00921: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2932 - accuracy: 0.8426 - val_loss: 0.6567 - val_accuracy: 0.8135\n",
      "Epoch 922/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2942 - accuracy: 0.8407\n",
      "Epoch 00922: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2933 - accuracy: 0.8412 - val_loss: 0.6522 - val_accuracy: 0.8147\n",
      "Epoch 923/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2939 - accuracy: 0.8418\n",
      "Epoch 00923: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2944 - accuracy: 0.8413 - val_loss: 0.6193 - val_accuracy: 0.8185\n",
      "Epoch 924/1000\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.2929 - accuracy: 0.8421\n",
      "Epoch 00924: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2928 - accuracy: 0.8423 - val_loss: 0.6546 - val_accuracy: 0.8220\n",
      "Epoch 925/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2927 - accuracy: 0.8410\n",
      "Epoch 00925: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.2923 - accuracy: 0.8412 - val_loss: 0.6584 - val_accuracy: 0.8147\n",
      "Epoch 926/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.2941 - accuracy: 0.8405\n",
      "Epoch 00926: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.2938 - accuracy: 0.8410 - val_loss: 0.6481 - val_accuracy: 0.8150\n",
      "Epoch 927/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.2934 - accuracy: 0.8404\n",
      "Epoch 00927: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2932 - accuracy: 0.8405 - val_loss: 0.6410 - val_accuracy: 0.8154\n",
      "Epoch 928/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.2940 - accuracy: 0.8403\n",
      "Epoch 00928: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2936 - accuracy: 0.8407 - val_loss: 0.6618 - val_accuracy: 0.8131\n",
      "Epoch 929/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2940 - accuracy: 0.8413\n",
      "Epoch 00929: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2949 - accuracy: 0.8413 - val_loss: 0.6960 - val_accuracy: 0.8139\n",
      "Epoch 930/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.2954 - accuracy: 0.8385\n",
      "Epoch 00930: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2955 - accuracy: 0.8386 - val_loss: 0.6732 - val_accuracy: 0.8112\n",
      "Epoch 931/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2934 - accuracy: 0.8422\n",
      "Epoch 00931: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2937 - accuracy: 0.8420 - val_loss: 0.7010 - val_accuracy: 0.8092\n",
      "Epoch 932/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.2912 - accuracy: 0.8411\n",
      "Epoch 00932: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2911 - accuracy: 0.8411 - val_loss: 0.7138 - val_accuracy: 0.8158\n",
      "Epoch 933/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2964 - accuracy: 0.8397\n",
      "Epoch 00933: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.2961 - accuracy: 0.8398 - val_loss: 0.6363 - val_accuracy: 0.8104\n",
      "Epoch 934/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.2925 - accuracy: 0.8414\n",
      "Epoch 00934: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2924 - accuracy: 0.8417 - val_loss: 0.6775 - val_accuracy: 0.8096\n",
      "Epoch 935/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2927 - accuracy: 0.8414\n",
      "Epoch 00935: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 99us/sample - loss: 0.2926 - accuracy: 0.8416 - val_loss: 0.6746 - val_accuracy: 0.8143\n",
      "Epoch 936/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.2947 - accuracy: 0.8412\n",
      "Epoch 00936: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.2943 - accuracy: 0.8412 - val_loss: 0.6542 - val_accuracy: 0.8170\n",
      "Epoch 937/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2943 - accuracy: 0.8404\n",
      "Epoch 00937: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.2942 - accuracy: 0.8403 - val_loss: 0.6541 - val_accuracy: 0.8104\n",
      "Epoch 938/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.2913 - accuracy: 0.8417\n",
      "Epoch 00938: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2916 - accuracy: 0.8418 - val_loss: 0.6717 - val_accuracy: 0.8116\n",
      "Epoch 939/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.2927 - accuracy: 0.8427\n",
      "Epoch 00939: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2931 - accuracy: 0.8425 - val_loss: 0.6538 - val_accuracy: 0.8127\n",
      "Epoch 940/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.2951 - accuracy: 0.8403\n",
      "Epoch 00940: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2949 - accuracy: 0.8406 - val_loss: 0.6178 - val_accuracy: 0.8127\n",
      "Epoch 941/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.2951 - accuracy: 0.8390\n",
      "Epoch 00941: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.2951 - accuracy: 0.8391 - val_loss: 0.6447 - val_accuracy: 0.8150\n",
      "Epoch 942/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2950 - accuracy: 0.8405\n",
      "Epoch 00942: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.2950 - accuracy: 0.8405 - val_loss: 0.6658 - val_accuracy: 0.8150\n",
      "Epoch 943/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2925 - accuracy: 0.8415\n",
      "Epoch 00943: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2925 - accuracy: 0.8414 - val_loss: 0.6778 - val_accuracy: 0.8150\n",
      "Epoch 944/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.2959 - accuracy: 0.8400\n",
      "Epoch 00944: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.2959 - accuracy: 0.8401 - val_loss: 0.6400 - val_accuracy: 0.8123\n",
      "Epoch 945/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2919 - accuracy: 0.8419\n",
      "Epoch 00945: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2920 - accuracy: 0.8417 - val_loss: 0.6615 - val_accuracy: 0.8116\n",
      "Epoch 946/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.2919 - accuracy: 0.8410\n",
      "Epoch 00946: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2920 - accuracy: 0.8408 - val_loss: 0.6508 - val_accuracy: 0.8131\n",
      "Epoch 947/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.2937 - accuracy: 0.8404\n",
      "Epoch 00947: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2936 - accuracy: 0.8406 - val_loss: 0.6690 - val_accuracy: 0.8085\n",
      "Epoch 948/1000\n",
      "22688/23208 [============================>.] - ETA: 0s - loss: 0.2940 - accuracy: 0.8412\n",
      "Epoch 00948: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.2941 - accuracy: 0.8411 - val_loss: 0.6325 - val_accuracy: 0.8127\n",
      "Epoch 949/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.2923 - accuracy: 0.8397\n",
      "Epoch 00949: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 99us/sample - loss: 0.2923 - accuracy: 0.8395 - val_loss: 0.6326 - val_accuracy: 0.8123\n",
      "Epoch 950/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.2957 - accuracy: 0.8386\n",
      "Epoch 00950: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 112us/sample - loss: 0.2952 - accuracy: 0.8390 - val_loss: 0.6284 - val_accuracy: 0.8077\n",
      "Epoch 951/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.2959 - accuracy: 0.8410\n",
      "Epoch 00951: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 121us/sample - loss: 0.2952 - accuracy: 0.8413 - val_loss: 0.6537 - val_accuracy: 0.8162\n",
      "Epoch 952/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2932 - accuracy: 0.8427\n",
      "Epoch 00952: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2931 - accuracy: 0.8427 - val_loss: 0.6629 - val_accuracy: 0.8166\n",
      "Epoch 953/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.2934 - accuracy: 0.8435\n",
      "Epoch 00953: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2944 - accuracy: 0.8431 - val_loss: 0.6552 - val_accuracy: 0.8170\n",
      "Epoch 954/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.2936 - accuracy: 0.8414\n",
      "Epoch 00954: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2936 - accuracy: 0.8414 - val_loss: 0.6376 - val_accuracy: 0.8143\n",
      "Epoch 955/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.2929 - accuracy: 0.8417\n",
      "Epoch 00955: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2929 - accuracy: 0.8419 - val_loss: 0.6260 - val_accuracy: 0.8174\n",
      "Epoch 956/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2957 - accuracy: 0.8397\n",
      "Epoch 00956: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2964 - accuracy: 0.8396 - val_loss: 0.6451 - val_accuracy: 0.8135\n",
      "Epoch 957/1000\n",
      "23072/23208 [============================>.] - ETA: 0s - loss: 0.2919 - accuracy: 0.8414\n",
      "Epoch 00957: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 122us/sample - loss: 0.2920 - accuracy: 0.8415 - val_loss: 0.6527 - val_accuracy: 0.8139\n",
      "Epoch 958/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2934 - accuracy: 0.8427\n",
      "Epoch 00958: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 138us/sample - loss: 0.2935 - accuracy: 0.8426 - val_loss: 0.6578 - val_accuracy: 0.8112\n",
      "Epoch 959/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2925 - accuracy: 0.8397\n",
      "Epoch 00959: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 107us/sample - loss: 0.2930 - accuracy: 0.8395 - val_loss: 0.6698 - val_accuracy: 0.8193\n",
      "Epoch 960/1000\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.2928 - accuracy: 0.8424\n",
      "Epoch 00960: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2931 - accuracy: 0.8420 - val_loss: 0.6406 - val_accuracy: 0.8150\n",
      "Epoch 961/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.2963 - accuracy: 0.8397\n",
      "Epoch 00961: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2966 - accuracy: 0.8393 - val_loss: 0.6415 - val_accuracy: 0.8104\n",
      "Epoch 962/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.2939 - accuracy: 0.8416\n",
      "Epoch 00962: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2939 - accuracy: 0.8413 - val_loss: 0.6777 - val_accuracy: 0.8154\n",
      "Epoch 963/1000\n",
      "22688/23208 [============================>.] - ETA: 0s - loss: 0.2926 - accuracy: 0.8403\n",
      "Epoch 00963: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2938 - accuracy: 0.8399 - val_loss: 0.6388 - val_accuracy: 0.8181\n",
      "Epoch 964/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.2943 - accuracy: 0.8397\n",
      "Epoch 00964: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2942 - accuracy: 0.8397 - val_loss: 0.6763 - val_accuracy: 0.8143\n",
      "Epoch 965/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.2943 - accuracy: 0.8411\n",
      "Epoch 00965: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2944 - accuracy: 0.8414 - val_loss: 0.6529 - val_accuracy: 0.8147\n",
      "Epoch 966/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.2936 - accuracy: 0.8415\n",
      "Epoch 00966: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2928 - accuracy: 0.8422 - val_loss: 0.6701 - val_accuracy: 0.8158\n",
      "Epoch 967/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2958 - accuracy: 0.8393\n",
      "Epoch 00967: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2962 - accuracy: 0.8393 - val_loss: 0.6562 - val_accuracy: 0.8166\n",
      "Epoch 968/1000\n",
      "22816/23208 [============================>.] - ETA: 0s - loss: 0.2978 - accuracy: 0.8413\n",
      "Epoch 00968: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2974 - accuracy: 0.8415 - val_loss: 0.6327 - val_accuracy: 0.8178\n",
      "Epoch 969/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.2952 - accuracy: 0.8406\n",
      "Epoch 00969: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2951 - accuracy: 0.8407 - val_loss: 0.6527 - val_accuracy: 0.8158\n",
      "Epoch 970/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2937 - accuracy: 0.8422\n",
      "Epoch 00970: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2938 - accuracy: 0.8422 - val_loss: 0.6777 - val_accuracy: 0.8050\n",
      "Epoch 971/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.2934 - accuracy: 0.8412\n",
      "Epoch 00971: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2937 - accuracy: 0.8411 - val_loss: 0.6453 - val_accuracy: 0.8178\n",
      "Epoch 972/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2956 - accuracy: 0.8411\n",
      "Epoch 00972: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2956 - accuracy: 0.8411 - val_loss: 0.6387 - val_accuracy: 0.8150\n",
      "Epoch 973/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.2928 - accuracy: 0.8417\n",
      "Epoch 00973: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2930 - accuracy: 0.8414 - val_loss: 0.6323 - val_accuracy: 0.8197\n",
      "Epoch 974/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.2952 - accuracy: 0.8400\n",
      "Epoch 00974: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2952 - accuracy: 0.8400 - val_loss: 0.6420 - val_accuracy: 0.8154\n",
      "Epoch 975/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2925 - accuracy: 0.8416\n",
      "Epoch 00975: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2931 - accuracy: 0.8411 - val_loss: 0.6482 - val_accuracy: 0.8162\n",
      "Epoch 976/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.2936 - accuracy: 0.8398\n",
      "Epoch 00976: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2934 - accuracy: 0.8400 - val_loss: 0.6287 - val_accuracy: 0.8166\n",
      "Epoch 977/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.2944 - accuracy: 0.8417\n",
      "Epoch 00977: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2945 - accuracy: 0.8416 - val_loss: 0.6493 - val_accuracy: 0.8116\n",
      "Epoch 978/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.2909 - accuracy: 0.8412\n",
      "Epoch 00978: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.2915 - accuracy: 0.8410 - val_loss: 0.6520 - val_accuracy: 0.8143\n",
      "Epoch 979/1000\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.2930 - accuracy: 0.8405\n",
      "Epoch 00979: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.2929 - accuracy: 0.8405 - val_loss: 0.6692 - val_accuracy: 0.8178\n",
      "Epoch 980/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.2951 - accuracy: 0.8405\n",
      "Epoch 00980: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2950 - accuracy: 0.8407 - val_loss: 0.6615 - val_accuracy: 0.8162\n",
      "Epoch 981/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2948 - accuracy: 0.8429\n",
      "Epoch 00981: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2950 - accuracy: 0.8427 - val_loss: 0.6386 - val_accuracy: 0.8131\n",
      "Epoch 982/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2918 - accuracy: 0.8434\n",
      "Epoch 00982: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2921 - accuracy: 0.8432 - val_loss: 0.6426 - val_accuracy: 0.8150\n",
      "Epoch 983/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.2928 - accuracy: 0.8418\n",
      "Epoch 00983: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2928 - accuracy: 0.8418 - val_loss: 0.6690 - val_accuracy: 0.8127\n",
      "Epoch 984/1000\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.2941 - accuracy: 0.8416\n",
      "Epoch 00984: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2937 - accuracy: 0.8417 - val_loss: 0.6371 - val_accuracy: 0.8162\n",
      "Epoch 985/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2934 - accuracy: 0.8409\n",
      "Epoch 00985: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2938 - accuracy: 0.8409 - val_loss: 0.6875 - val_accuracy: 0.8178\n",
      "Epoch 986/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.2942 - accuracy: 0.8423\n",
      "Epoch 00986: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2942 - accuracy: 0.8423 - val_loss: 0.7115 - val_accuracy: 0.8158\n",
      "Epoch 987/1000\n",
      "23136/23208 [============================>.] - ETA: 0s - loss: 0.2936 - accuracy: 0.8422\n",
      "Epoch 00987: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2936 - accuracy: 0.8422 - val_loss: 0.6402 - val_accuracy: 0.8181\n",
      "Epoch 988/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.2942 - accuracy: 0.8415\n",
      "Epoch 00988: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2942 - accuracy: 0.8412 - val_loss: 0.6806 - val_accuracy: 0.8150\n",
      "Epoch 989/1000\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.2951 - accuracy: 0.8417\n",
      "Epoch 00989: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.2958 - accuracy: 0.8416 - val_loss: 0.6431 - val_accuracy: 0.8150\n",
      "Epoch 990/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2959 - accuracy: 0.8404\n",
      "Epoch 00990: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2956 - accuracy: 0.8405 - val_loss: 0.6598 - val_accuracy: 0.8178\n",
      "Epoch 991/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.2906 - accuracy: 0.8418\n",
      "Epoch 00991: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2905 - accuracy: 0.8416 - val_loss: 0.6592 - val_accuracy: 0.8127\n",
      "Epoch 992/1000\n",
      "23008/23208 [============================>.] - ETA: 0s - loss: 0.2930 - accuracy: 0.8418\n",
      "Epoch 00992: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 120us/sample - loss: 0.2932 - accuracy: 0.8416 - val_loss: 0.6738 - val_accuracy: 0.8147\n",
      "Epoch 993/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2940 - accuracy: 0.8417\n",
      "Epoch 00993: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 3s 134us/sample - loss: 0.2936 - accuracy: 0.8418 - val_loss: 0.6419 - val_accuracy: 0.8170\n",
      "Epoch 994/1000\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.2956 - accuracy: 0.8396\n",
      "Epoch 00994: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.2956 - accuracy: 0.8397 - val_loss: 0.6010 - val_accuracy: 0.8123\n",
      "Epoch 995/1000\n",
      "22944/23208 [============================>.] - ETA: 0s - loss: 0.2944 - accuracy: 0.8412\n",
      "Epoch 00995: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2943 - accuracy: 0.8412 - val_loss: 0.6550 - val_accuracy: 0.8181\n",
      "Epoch 996/1000\n",
      "22752/23208 [============================>.] - ETA: 0s - loss: 0.2933 - accuracy: 0.8404\n",
      "Epoch 00996: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2932 - accuracy: 0.8404 - val_loss: 0.6220 - val_accuracy: 0.8135\n",
      "Epoch 997/1000\n",
      "22880/23208 [============================>.] - ETA: 0s - loss: 0.2956 - accuracy: 0.8411\n",
      "Epoch 00997: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.2951 - accuracy: 0.8416 - val_loss: 0.6397 - val_accuracy: 0.8185\n",
      "Epoch 998/1000\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.2946 - accuracy: 0.8395\n",
      "Epoch 00998: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.2943 - accuracy: 0.8397 - val_loss: 0.6442 - val_accuracy: 0.8119\n",
      "Epoch 999/1000\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.2940 - accuracy: 0.8397\n",
      "Epoch 00999: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.2939 - accuracy: 0.8398 - val_loss: 0.6424 - val_accuracy: 0.8123\n",
      "Epoch 1000/1000\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.2939 - accuracy: 0.8393\n",
      "Epoch 01000: val_accuracy did not improve from 0.82513\n",
      "23208/23208 [==============================] - 2s 107us/sample - loss: 0.2939 - accuracy: 0.8393 - val_loss: 0.6328 - val_accuracy: 0.8088\n"
     ]
    }
   ],
   "source": [
    "# First trail\n",
    "cd=os.getcwd()\n",
    "# HyperParameters:\n",
    "#Batch_size=32\n",
    "#4 Hidden Layers with (264,128,64,32) neurons\n",
    "#Dropout_Rate: 0.1 applied in three layers\n",
    "#Learning_rate: 0.001\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "filepath=cd\n",
    "checkpoint = ModelCheckpoint(\"weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "# Fit the model\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "model=tf.keras.models.Sequential([ \n",
    "                                   tf.keras.layers.Dense(63,input_shape=(63,),activation='relu'),\n",
    "    tf.keras.layers.Dense(264,activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(128,activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(64,activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "   tf.keras.layers.Dense(32,activation='relu'),\n",
    "                                  \n",
    "                             \n",
    "                             tf.keras.layers.Dense(2,activation='softmax')\n",
    "])\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "y_train_=to_categorical(y_train)\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')\n",
    "history=model.fit(X_train,y_train_,epochs=1000,validation_split=0.1,callbacks=callbacks_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x180fc8a7748>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['train','val'])\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['train','val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 23208 samples, validate on 2579 samples\n",
      "Epoch 1/100\n",
      "23201/23208 [============================>.] - ETA: 0s - loss: 0.4309 - accuracy: 0.7791\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.80186, saving model to weights-improvement-01-0.80.hdf5\n",
      "23208/23208 [==============================] - 74s 3ms/sample - loss: 0.4310 - accuracy: 0.7791 - val_loss: 0.3947 - val_accuracy: 0.8019\n",
      "Epoch 2/100\n",
      "23202/23208 [============================>.] - ETA: 0s - loss: 0.4075 - accuracy: 0.7997\n",
      "Epoch 00002: val_accuracy improved from 0.80186 to 0.80302, saving model to weights-improvement-02-0.80.hdf5\n",
      "23208/23208 [==============================] - 95s 4ms/sample - loss: 0.4075 - accuracy: 0.7996 - val_loss: 0.3903 - val_accuracy: 0.8030\n",
      "Epoch 3/100\n",
      "23204/23208 [============================>.] - ETA: 0s - loss: 0.4022 - accuracy: 0.7996\n",
      "Epoch 00003: val_accuracy improved from 0.80302 to 0.80535, saving model to weights-improvement-03-0.81.hdf5\n",
      "23208/23208 [==============================] - 84s 4ms/sample - loss: 0.4022 - accuracy: 0.7996 - val_loss: 0.3822 - val_accuracy: 0.8054\n",
      "Epoch 4/100\n",
      "23193/23208 [============================>.] - ETA: 0s - loss: 0.3989 - accuracy: 0.8024\n",
      "Epoch 00004: val_accuracy did not improve from 0.80535\n",
      "23208/23208 [==============================] - 87s 4ms/sample - loss: 0.3990 - accuracy: 0.8022 - val_loss: 0.3774 - val_accuracy: 0.8054\n",
      "Epoch 5/100\n",
      "23192/23208 [============================>.] - ETA: 0s - loss: 0.3987 - accuracy: 0.8012\n",
      "Epoch 00005: val_accuracy improved from 0.80535 to 0.80807, saving model to weights-improvement-05-0.81.hdf5\n",
      "23208/23208 [==============================] - 75s 3ms/sample - loss: 0.3986 - accuracy: 0.8013 - val_loss: 0.3807 - val_accuracy: 0.8081\n",
      "Epoch 6/100\n",
      "23207/23208 [============================>.] - ETA: 0s - loss: 0.4001 - accuracy: 0.8027\n",
      "Epoch 00006: val_accuracy did not improve from 0.80807\n",
      "23208/23208 [==============================] - 89s 4ms/sample - loss: 0.4001 - accuracy: 0.8027 - val_loss: 0.3886 - val_accuracy: 0.8054\n",
      "Epoch 7/100\n",
      "23193/23208 [============================>.] - ETA: 0s - loss: 0.3983 - accuracy: 0.7993\n",
      "Epoch 00007: val_accuracy did not improve from 0.80807\n",
      "23208/23208 [==============================] - 75s 3ms/sample - loss: 0.3982 - accuracy: 0.7994 - val_loss: 0.3847 - val_accuracy: 0.8077\n",
      "Epoch 8/100\n",
      "23203/23208 [============================>.] - ETA: 0s - loss: 0.4017 - accuracy: 0.8011\n",
      "Epoch 00008: val_accuracy did not improve from 0.80807\n",
      "23208/23208 [==============================] - 76s 3ms/sample - loss: 0.4017 - accuracy: 0.8011 - val_loss: 0.3854 - val_accuracy: 0.8061\n",
      "Epoch 9/100\n",
      "23192/23208 [============================>.] - ETA: 0s - loss: 0.4112 - accuracy: 0.8024\n",
      "Epoch 00009: val_accuracy did not improve from 0.80807\n",
      "23208/23208 [==============================] - 73s 3ms/sample - loss: 0.4113 - accuracy: 0.8023 - val_loss: 0.3954 - val_accuracy: 0.7968\n",
      "Epoch 10/100\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.4071 - accuracy: 0.8022\n",
      "Epoch 00010: val_accuracy did not improve from 0.80807\n",
      "23208/23208 [==============================] - 75s 3ms/sample - loss: 0.4075 - accuracy: 0.8021 - val_loss: 0.4324 - val_accuracy: 0.8065\n",
      "Epoch 11/100\n",
      "23203/23208 [============================>.] - ETA: 0s - loss: 0.4256 - accuracy: 0.8019\n",
      "Epoch 00011: val_accuracy did not improve from 0.80807\n",
      "23208/23208 [==============================] - 75s 3ms/sample - loss: 0.4257 - accuracy: 0.8019 - val_loss: 0.3859 - val_accuracy: 0.8065\n",
      "Epoch 12/100\n",
      "23197/23208 [============================>.] - ETA: 0s - loss: 0.4056 - accuracy: 0.8039\n",
      "Epoch 00012: val_accuracy did not improve from 0.80807\n",
      "23208/23208 [==============================] - 72s 3ms/sample - loss: 0.4055 - accuracy: 0.8039 - val_loss: 0.3938 - val_accuracy: 0.8069\n",
      "Epoch 13/100\n",
      "23194/23208 [============================>.] - ETA: 0s - loss: 0.4176 - accuracy: 0.8012\n",
      "Epoch 00013: val_accuracy improved from 0.80807 to 0.80962, saving model to weights-improvement-13-0.81.hdf5\n",
      "23208/23208 [==============================] - 80s 3ms/sample - loss: 0.4175 - accuracy: 0.8013 - val_loss: 0.3970 - val_accuracy: 0.8096\n",
      "Epoch 14/100\n",
      "23194/23208 [============================>.] - ETA: 0s - loss: 0.4065 - accuracy: 0.8028\n",
      "Epoch 00014: val_accuracy did not improve from 0.80962\n",
      "23208/23208 [==============================] - 84s 4ms/sample - loss: 0.4065 - accuracy: 0.8028 - val_loss: 0.3970 - val_accuracy: 0.8085\n",
      "Epoch 15/100\n",
      "23202/23208 [============================>.] - ETA: 0s - loss: 0.4107 - accuracy: 0.8039\n",
      "Epoch 00015: val_accuracy improved from 0.80962 to 0.81078, saving model to weights-improvement-15-0.81.hdf5\n",
      "23208/23208 [==============================] - 82s 4ms/sample - loss: 0.4107 - accuracy: 0.8039 - val_loss: 0.3926 - val_accuracy: 0.8108\n",
      "Epoch 16/100\n",
      "23192/23208 [============================>.] - ETA: 0s - loss: 0.4050 - accuracy: 0.8039\n",
      "Epoch 00016: val_accuracy did not improve from 0.81078\n",
      "23208/23208 [==============================] - 85s 4ms/sample - loss: 0.4050 - accuracy: 0.8039 - val_loss: 0.4170 - val_accuracy: 0.8100\n",
      "Epoch 17/100\n",
      "23202/23208 [============================>.] - ETA: 0s - loss: 0.4190 - accuracy: 0.8029\n",
      "Epoch 00017: val_accuracy did not improve from 0.81078\n",
      "23208/23208 [==============================] - 73s 3ms/sample - loss: 0.4190 - accuracy: 0.8028 - val_loss: 0.3987 - val_accuracy: 0.8088\n",
      "Epoch 18/100\n",
      "23207/23208 [============================>.] - ETA: 0s - loss: 0.4220 - accuracy: 0.8032\n",
      "Epoch 00018: val_accuracy did not improve from 0.81078\n",
      "23208/23208 [==============================] - 80s 3ms/sample - loss: 0.4220 - accuracy: 0.8031 - val_loss: 0.3976 - val_accuracy: 0.8061\n",
      "Epoch 19/100\n",
      "23205/23208 [============================>.] - ETA: 0s - loss: 0.4039 - accuracy: 0.8041\n",
      "Epoch 00019: val_accuracy did not improve from 0.81078\n",
      "23208/23208 [==============================] - 69s 3ms/sample - loss: 0.4039 - accuracy: 0.8041 - val_loss: 0.4020 - val_accuracy: 0.7960\n",
      "Epoch 20/100\n",
      "23205/23208 [============================>.] - ETA: 0s - loss: 0.4134 - accuracy: 0.8042\n",
      "Epoch 00020: val_accuracy did not improve from 0.81078\n",
      "23208/23208 [==============================] - 70s 3ms/sample - loss: 0.4135 - accuracy: 0.8042 - val_loss: 0.3853 - val_accuracy: 0.8034\n",
      "Epoch 21/100\n",
      "23198/23208 [============================>.] - ETA: 0s - loss: 0.4119 - accuracy: 0.8051\n",
      "Epoch 00021: val_accuracy did not improve from 0.81078\n",
      "23208/23208 [==============================] - 76s 3ms/sample - loss: 0.4118 - accuracy: 0.8051 - val_loss: 0.3933 - val_accuracy: 0.8042\n",
      "Epoch 22/100\n",
      "23205/23208 [============================>.] - ETA: 0s - loss: 0.4200 - accuracy: 0.8029\n",
      "Epoch 00022: val_accuracy did not improve from 0.81078\n",
      "23208/23208 [==============================] - 73s 3ms/sample - loss: 0.4200 - accuracy: 0.8029 - val_loss: 0.4055 - val_accuracy: 0.8050\n",
      "Epoch 23/100\n",
      "23206/23208 [============================>.] - ETA: 0s - loss: 0.4106 - accuracy: 0.8032\n",
      "Epoch 00023: val_accuracy did not improve from 0.81078\n",
      "23208/23208 [==============================] - 71s 3ms/sample - loss: 0.4106 - accuracy: 0.8033 - val_loss: 0.4392 - val_accuracy: 0.7895\n",
      "Epoch 24/100\n",
      "23199/23208 [============================>.] - ETA: 0s - loss: 0.4201 - accuracy: 0.8005\n",
      "Epoch 00024: val_accuracy did not improve from 0.81078\n",
      "23208/23208 [==============================] - 72s 3ms/sample - loss: 0.4201 - accuracy: 0.8005 - val_loss: 0.3997 - val_accuracy: 0.8003\n",
      "Epoch 25/100\n",
      "23197/23208 [============================>.] - ETA: 0s - loss: 0.4145 - accuracy: 0.8028\n",
      "Epoch 00025: val_accuracy did not improve from 0.81078\n",
      "23208/23208 [==============================] - 76s 3ms/sample - loss: 0.4145 - accuracy: 0.8028 - val_loss: 0.3964 - val_accuracy: 0.8050\n",
      "Epoch 26/100\n",
      "23207/23208 [============================>.] - ETA: 0s - loss: 0.4276 - accuracy: 0.8065\n",
      "Epoch 00026: val_accuracy did not improve from 0.81078\n",
      "23208/23208 [==============================] - 72s 3ms/sample - loss: 0.4275 - accuracy: 0.8065 - val_loss: 0.4112 - val_accuracy: 0.8104\n",
      "Epoch 27/100\n",
      "23197/23208 [============================>.] - ETA: 0s - loss: 0.4172 - accuracy: 0.8060\n",
      "Epoch 00027: val_accuracy did not improve from 0.81078\n",
      "23208/23208 [==============================] - 81s 3ms/sample - loss: 0.4172 - accuracy: 0.8060 - val_loss: 0.4074 - val_accuracy: 0.8085\n",
      "Epoch 28/100\n",
      "23202/23208 [============================>.] - ETA: 0s - loss: 0.4568 - accuracy: 0.8032\n",
      "Epoch 00028: val_accuracy did not improve from 0.81078\n",
      "23208/23208 [==============================] - 82s 4ms/sample - loss: 0.4568 - accuracy: 0.8032 - val_loss: 0.4071 - val_accuracy: 0.8088\n",
      "Epoch 29/100\n",
      "23197/23208 [============================>.] - ETA: 0s - loss: 0.4277 - accuracy: 0.8036\n",
      "Epoch 00029: val_accuracy did not improve from 0.81078\n",
      "23208/23208 [==============================] - 94s 4ms/sample - loss: 0.4276 - accuracy: 0.8036 - val_loss: 0.4148 - val_accuracy: 0.8104\n",
      "Epoch 30/100\n",
      "23206/23208 [============================>.] - ETA: 0s - loss: 0.4481 - accuracy: 0.8048\n",
      "Epoch 00030: val_accuracy did not improve from 0.81078\n",
      "23208/23208 [==============================] - 94s 4ms/sample - loss: 0.4481 - accuracy: 0.8048 - val_loss: 0.4315 - val_accuracy: 0.7953\n",
      "Epoch 31/100\n",
      "23205/23208 [============================>.] - ETA: 0s - loss: 0.4432 - accuracy: 0.8028\n",
      "Epoch 00031: val_accuracy did not improve from 0.81078\n",
      "23208/23208 [==============================] - 90s 4ms/sample - loss: 0.4432 - accuracy: 0.8028 - val_loss: 0.4339 - val_accuracy: 0.7980\n",
      "Epoch 32/100\n",
      "23192/23208 [============================>.] - ETA: 0s - loss: 0.4658 - accuracy: 0.8026\n",
      "Epoch 00032: val_accuracy did not improve from 0.81078\n",
      "23208/23208 [==============================] - 87s 4ms/sample - loss: 0.4656 - accuracy: 0.8027 - val_loss: 0.4286 - val_accuracy: 0.8100\n",
      "Epoch 33/100\n",
      "23201/23208 [============================>.] - ETA: 0s - loss: 0.4841 - accuracy: 0.8014\n",
      "Epoch 00033: val_accuracy did not improve from 0.81078\n",
      "23208/23208 [==============================] - 71s 3ms/sample - loss: 0.4840 - accuracy: 0.8014 - val_loss: 0.5083 - val_accuracy: 0.7941\n",
      "Epoch 34/100\n",
      "23192/23208 [============================>.] - ETA: 0s - loss: 0.5515 - accuracy: 0.7999\n",
      "Epoch 00034: val_accuracy did not improve from 0.81078\n",
      "23208/23208 [==============================] - 69s 3ms/sample - loss: 0.5513 - accuracy: 0.7999 - val_loss: 0.5011 - val_accuracy: 0.8104\n",
      "Epoch 35/100\n",
      "23207/23208 [============================>.] - ETA: 0s - loss: 0.6038 - accuracy: 0.8031\n",
      "Epoch 00035: val_accuracy did not improve from 0.81078\n",
      "23208/23208 [==============================] - 70s 3ms/sample - loss: 0.6038 - accuracy: 0.8030 - val_loss: 0.5743 - val_accuracy: 0.8007\n",
      "Epoch 36/100\n",
      "23195/23208 [============================>.] - ETA: 0s - loss: 0.6447 - accuracy: 0.7973\n",
      "Epoch 00036: val_accuracy did not improve from 0.81078\n",
      "23208/23208 [==============================] - 72s 3ms/sample - loss: 0.6444 - accuracy: 0.7974 - val_loss: 0.5276 - val_accuracy: 0.8100\n",
      "Epoch 37/100\n",
      "23189/23208 [============================>.] - ETA: 0s - loss: 0.6064 - accuracy: 0.7984\n",
      "Epoch 00037: val_accuracy did not improve from 0.81078\n",
      "23208/23208 [==============================] - 69s 3ms/sample - loss: 0.6063 - accuracy: 0.7984 - val_loss: 0.4991 - val_accuracy: 0.8038\n",
      "Epoch 38/100\n",
      "23192/23208 [============================>.] - ETA: 0s - loss: 0.5973 - accuracy: 0.7986\n",
      "Epoch 00038: val_accuracy did not improve from 0.81078\n",
      "23208/23208 [==============================] - 69s 3ms/sample - loss: 0.5970 - accuracy: 0.7987 - val_loss: 0.4754 - val_accuracy: 0.8046\n",
      "Epoch 39/100\n",
      "23191/23208 [============================>.] - ETA: 0s - loss: 0.6318 - accuracy: 0.7962\n",
      "Epoch 00039: val_accuracy did not improve from 0.81078\n",
      "23208/23208 [==============================] - 69s 3ms/sample - loss: 0.6336 - accuracy: 0.7962 - val_loss: 0.5509 - val_accuracy: 0.8038\n",
      "Epoch 40/100\n",
      "23197/23208 [============================>.] - ETA: 0s - loss: 0.6296 - accuracy: 0.7996\n",
      "Epoch 00040: val_accuracy did not improve from 0.81078\n",
      "23208/23208 [==============================] - 69s 3ms/sample - loss: 0.6301 - accuracy: 0.7996 - val_loss: 0.5981 - val_accuracy: 0.8054\n",
      "Epoch 41/100\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.6316 - accuracy: 0.8009\n",
      "Epoch 00041: val_accuracy did not improve from 0.81078\n",
      "23208/23208 [==============================] - 69s 3ms/sample - loss: 0.6315 - accuracy: 0.8009 - val_loss: 0.4784 - val_accuracy: 0.8085\n",
      "Epoch 42/100\n",
      "23207/23208 [============================>.] - ETA: 0s - loss: 0.6038 - accuracy: 0.7986\n",
      "Epoch 00042: val_accuracy did not improve from 0.81078\n",
      "23208/23208 [==============================] - 72s 3ms/sample - loss: 0.6037 - accuracy: 0.7986 - val_loss: 0.4919 - val_accuracy: 0.8057\n",
      "Epoch 43/100\n",
      "23198/23208 [============================>.] - ETA: 0s - loss: 0.6163 - accuracy: 0.7956\n",
      "Epoch 00043: val_accuracy did not improve from 0.81078\n",
      "23208/23208 [==============================] - 72s 3ms/sample - loss: 0.6161 - accuracy: 0.7957 - val_loss: 0.4993 - val_accuracy: 0.7953\n",
      "Epoch 44/100\n",
      "23201/23208 [============================>.] - ETA: 0s - loss: 0.5911 - accuracy: 0.8003\n",
      "Epoch 00044: val_accuracy improved from 0.81078 to 0.81388, saving model to weights-improvement-44-0.81.hdf5\n",
      "23208/23208 [==============================] - 72s 3ms/sample - loss: 0.5909 - accuracy: 0.8004 - val_loss: 0.4860 - val_accuracy: 0.8139\n",
      "Epoch 45/100\n",
      "23207/23208 [============================>.] - ETA: 0s - loss: 0.5895 - accuracy: 0.8010\n",
      "Epoch 00045: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 72s 3ms/sample - loss: 0.5895 - accuracy: 0.8010 - val_loss: 0.5034 - val_accuracy: 0.8073\n",
      "Epoch 46/100\n",
      "23203/23208 [============================>.] - ETA: 0s - loss: 0.6017 - accuracy: 0.8013\n",
      "Epoch 00046: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 72s 3ms/sample - loss: 0.6017 - accuracy: 0.8013 - val_loss: 0.5398 - val_accuracy: 0.8034\n",
      "Epoch 47/100\n",
      "23199/23208 [============================>.] - ETA: 0s - loss: 0.6116 - accuracy: 0.8007\n",
      "Epoch 00047: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 72s 3ms/sample - loss: 0.6114 - accuracy: 0.8007 - val_loss: 0.5301 - val_accuracy: 0.8077\n",
      "Epoch 48/100\n",
      "23193/23208 [============================>.] - ETA: 0s - loss: 0.8605 - accuracy: 0.7859\n",
      "Epoch 00048: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 72s 3ms/sample - loss: 0.8602 - accuracy: 0.7859 - val_loss: 0.5302 - val_accuracy: 0.8011\n",
      "Epoch 49/100\n",
      "23206/23208 [============================>.] - ETA: 0s - loss: 0.8652 - accuracy: 0.7829\n",
      "Epoch 00049: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 71s 3ms/sample - loss: 0.8651 - accuracy: 0.7830 - val_loss: 0.5056 - val_accuracy: 0.8061\n",
      "Epoch 50/100\n",
      "23207/23208 [============================>.] - ETA: 0s - loss: 0.6908 - accuracy: 0.7972\n",
      "Epoch 00050: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 71s 3ms/sample - loss: 0.6908 - accuracy: 0.7972 - val_loss: 0.5152 - val_accuracy: 0.8050\n",
      "Epoch 51/100\n",
      "23195/23208 [============================>.] - ETA: 0s - loss: 0.6008 - accuracy: 0.8003\n",
      "Epoch 00051: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 72s 3ms/sample - loss: 0.6006 - accuracy: 0.8003 - val_loss: 0.5095 - val_accuracy: 0.8092\n",
      "Epoch 52/100\n",
      "23195/23208 [============================>.] - ETA: 0s - loss: 0.6100 - accuracy: 0.8017\n",
      "Epoch 00052: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 72s 3ms/sample - loss: 0.6099 - accuracy: 0.8017 - val_loss: 0.5647 - val_accuracy: 0.8108\n",
      "Epoch 53/100\n",
      "23207/23208 [============================>.] - ETA: 0s - loss: 0.6207 - accuracy: 0.8023\n",
      "Epoch 00053: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 73s 3ms/sample - loss: 0.6207 - accuracy: 0.8023 - val_loss: 0.5179 - val_accuracy: 0.8030\n",
      "Epoch 54/100\n",
      "23190/23208 [============================>.] - ETA: 0s - loss: 0.6105 - accuracy: 0.8012\n",
      "Epoch 00054: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 74s 3ms/sample - loss: 0.6104 - accuracy: 0.8011 - val_loss: 0.5385 - val_accuracy: 0.7968\n",
      "Epoch 55/100\n",
      "23201/23208 [============================>.] - ETA: 0s - loss: 0.6300 - accuracy: 0.8013\n",
      "Epoch 00055: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 70s 3ms/sample - loss: 0.6298 - accuracy: 0.8013 - val_loss: 0.5764 - val_accuracy: 0.8104\n",
      "Epoch 56/100\n",
      "23192/23208 [============================>.] - ETA: 0s - loss: 0.7117 - accuracy: 0.7970\n",
      "Epoch 00056: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 72s 3ms/sample - loss: 0.7113 - accuracy: 0.7971 - val_loss: 0.5579 - val_accuracy: 0.8061\n",
      "Epoch 57/100\n",
      "23193/23208 [============================>.] - ETA: 0s - loss: 2.2380 - accuracy: 0.6995\n",
      "Epoch 00057: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 71s 3ms/sample - loss: 2.2367 - accuracy: 0.6997 - val_loss: 0.5093 - val_accuracy: 0.8019\n",
      "Epoch 58/100\n",
      "23191/23208 [============================>.] - ETA: 0s - loss: 0.6538 - accuracy: 0.7990\n",
      "Epoch 00058: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 71s 3ms/sample - loss: 0.6536 - accuracy: 0.7989 - val_loss: 0.5327 - val_accuracy: 0.8046\n",
      "Epoch 59/100\n",
      "23196/23208 [============================>.] - ETA: 0s - loss: 0.6405 - accuracy: 0.7950\n",
      "Epoch 00059: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 70s 3ms/sample - loss: 0.6417 - accuracy: 0.7949 - val_loss: 0.5534 - val_accuracy: 0.8015\n",
      "Epoch 60/100\n",
      "23202/23208 [============================>.] - ETA: 0s - loss: 0.6295 - accuracy: 0.7911 ETA\n",
      "Epoch 00060: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 71s 3ms/sample - loss: 0.6294 - accuracy: 0.7910 - val_loss: 0.5423 - val_accuracy: 0.7941\n",
      "Epoch 61/100\n",
      "23199/23208 [============================>.] - ETA: 0s - loss: 0.7072 - accuracy: 0.7919\n",
      "Epoch 00061: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 71s 3ms/sample - loss: 0.7071 - accuracy: 0.7918 - val_loss: 0.5197 - val_accuracy: 0.8065\n",
      "Epoch 62/100\n",
      "23205/23208 [============================>.] - ETA: 0s - loss: 0.6566 - accuracy: 0.7944\n",
      "Epoch 00062: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 71s 3ms/sample - loss: 0.6566 - accuracy: 0.7945 - val_loss: 0.5871 - val_accuracy: 0.8054\n",
      "Epoch 63/100\n",
      "23198/23208 [============================>.] - ETA: 0s - loss: 0.6578 - accuracy: 0.7989\n",
      "Epoch 00063: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 71s 3ms/sample - loss: 0.6576 - accuracy: 0.7990 - val_loss: 0.5847 - val_accuracy: 0.7910\n",
      "Epoch 64/100\n",
      "23192/23208 [============================>.] - ETA: 0s - loss: 0.6755 - accuracy: 0.7982\n",
      "Epoch 00064: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 71s 3ms/sample - loss: 0.6761 - accuracy: 0.7981 - val_loss: 0.6061 - val_accuracy: 0.7914\n",
      "Epoch 65/100\n",
      "23192/23208 [============================>.] - ETA: 0s - loss: 0.6612 - accuracy: 0.7947\n",
      "Epoch 00065: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 71s 3ms/sample - loss: 0.6610 - accuracy: 0.7948 - val_loss: 0.5468 - val_accuracy: 0.8050\n",
      "Epoch 66/100\n",
      "23194/23208 [============================>.] - ETA: 0s - loss: 0.6419 - accuracy: 0.8005\n",
      "Epoch 00066: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 70s 3ms/sample - loss: 0.6425 - accuracy: 0.8005 - val_loss: 0.5756 - val_accuracy: 0.8054\n",
      "Epoch 67/100\n",
      "23192/23208 [============================>.] - ETA: 0s - loss: 0.6607 - accuracy: 0.8019\n",
      "Epoch 00067: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 70s 3ms/sample - loss: 0.6605 - accuracy: 0.8018 - val_loss: 0.5617 - val_accuracy: 0.8046\n",
      "Epoch 68/100\n",
      "23191/23208 [============================>.] - ETA: 0s - loss: 0.6894 - accuracy: 0.7975\n",
      "Epoch 00068: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 71s 3ms/sample - loss: 0.6899 - accuracy: 0.7975 - val_loss: 0.6990 - val_accuracy: 0.8019\n",
      "Epoch 69/100\n",
      "23203/23208 [============================>.] - ETA: 0s - loss: 0.7538 - accuracy: 0.7949\n",
      "Epoch 00069: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 70s 3ms/sample - loss: 0.7538 - accuracy: 0.7949 - val_loss: 0.5781 - val_accuracy: 0.8050\n",
      "Epoch 70/100\n",
      "23194/23208 [============================>.] - ETA: 0s - loss: 0.6423 - accuracy: 0.8034\n",
      "Epoch 00070: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 70s 3ms/sample - loss: 0.6427 - accuracy: 0.8035 - val_loss: 0.5453 - val_accuracy: 0.8034\n",
      "Epoch 71/100\n",
      "23197/23208 [============================>.] - ETA: 0s - loss: 0.6157 - accuracy: 0.8018\n",
      "Epoch 00071: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 71s 3ms/sample - loss: 0.6156 - accuracy: 0.8018 - val_loss: 0.5610 - val_accuracy: 0.8057\n",
      "Epoch 72/100\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.6850 - accuracy: 0.7968\n",
      "Epoch 00072: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 71s 3ms/sample - loss: 0.6849 - accuracy: 0.7968 - val_loss: 0.5429 - val_accuracy: 0.8050\n",
      "Epoch 73/100\n",
      "23191/23208 [============================>.] - ETA: 0s - loss: 0.6318 - accuracy: 0.7994\n",
      "Epoch 00073: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 71s 3ms/sample - loss: 0.6316 - accuracy: 0.7995 - val_loss: 0.5469 - val_accuracy: 0.8038\n",
      "Epoch 74/100\n",
      "23199/23208 [============================>.] - ETA: 0s - loss: 0.6400 - accuracy: 0.7994\n",
      "Epoch 00074: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 76s 3ms/sample - loss: 0.6399 - accuracy: 0.7993 - val_loss: 0.5463 - val_accuracy: 0.7922\n",
      "Epoch 75/100\n",
      "23206/23208 [============================>.] - ETA: 0s - loss: 0.6285 - accuracy: 0.8011\n",
      "Epoch 00075: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 161s 7ms/sample - loss: 0.6285 - accuracy: 0.8011 - val_loss: 0.5390 - val_accuracy: 0.8022\n",
      "Epoch 76/100\n",
      "23205/23208 [============================>.] - ETA: 0s - loss: 0.6091 - accuracy: 0.8037\n",
      "Epoch 00076: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 163s 7ms/sample - loss: 0.6091 - accuracy: 0.8037 - val_loss: 0.5381 - val_accuracy: 0.8073\n",
      "Epoch 77/100\n",
      "23205/23208 [============================>.] - ETA: 0s - loss: 0.7767 - accuracy: 0.7867\n",
      "Epoch 00077: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 158s 7ms/sample - loss: 0.7767 - accuracy: 0.7867 - val_loss: 0.5216 - val_accuracy: 0.8131\n",
      "Epoch 78/100\n",
      "23207/23208 [============================>.] - ETA: 0s - loss: 0.6365 - accuracy: 0.7971\n",
      "Epoch 00078: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 159s 7ms/sample - loss: 0.6365 - accuracy: 0.7971 - val_loss: 0.5379 - val_accuracy: 0.7972\n",
      "Epoch 79/100\n",
      "23207/23208 [============================>.] - ETA: 0s - loss: 0.6320 - accuracy: 0.7979\n",
      "Epoch 00079: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 160s 7ms/sample - loss: 0.6320 - accuracy: 0.7980 - val_loss: 0.5327 - val_accuracy: 0.8042\n",
      "Epoch 80/100\n",
      "23198/23208 [============================>.] - ETA: 0s - loss: 0.7160 - accuracy: 0.7954\n",
      "Epoch 00080: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 158s 7ms/sample - loss: 0.7165 - accuracy: 0.7954 - val_loss: 0.5611 - val_accuracy: 0.8003\n",
      "Epoch 81/100\n",
      "23206/23208 [============================>.] - ETA: 0s - loss: 0.8446 - accuracy: 0.7865\n",
      "Epoch 00081: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 164s 7ms/sample - loss: 0.8445 - accuracy: 0.7865 - val_loss: 0.5768 - val_accuracy: 0.8042\n",
      "Epoch 82/100\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.6848 - accuracy: 0.7982\n",
      "Epoch 00082: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 162s 7ms/sample - loss: 0.6848 - accuracy: 0.7981 - val_loss: 0.6683 - val_accuracy: 0.7949\n",
      "Epoch 83/100\n",
      "23202/23208 [============================>.] - ETA: 0s - loss: 0.6863 - accuracy: 0.7915\n",
      "Epoch 00083: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 159s 7ms/sample - loss: 0.6863 - accuracy: 0.7915 - val_loss: 0.5827 - val_accuracy: 0.7937\n",
      "Epoch 84/100\n",
      "23197/23208 [============================>.] - ETA: 0s - loss: 0.7429 - accuracy: 0.7938\n",
      "Epoch 00084: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 162s 7ms/sample - loss: 0.7427 - accuracy: 0.7938 - val_loss: 0.6264 - val_accuracy: 0.7906\n",
      "Epoch 85/100\n",
      "23203/23208 [============================>.] - ETA: 0s - loss: 0.9207 - accuracy: 0.7829\n",
      "Epoch 00085: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 162s 7ms/sample - loss: 0.9212 - accuracy: 0.7829 - val_loss: 0.5939 - val_accuracy: 0.8112\n",
      "Epoch 86/100\n",
      "23203/23208 [============================>.] - ETA: 0s - loss: 0.6319 - accuracy: 0.7968\n",
      "Epoch 00086: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 160s 7ms/sample - loss: 0.6319 - accuracy: 0.7968 - val_loss: 0.5598 - val_accuracy: 0.8073\n",
      "Epoch 87/100\n",
      "23207/23208 [============================>.] - ETA: 0s - loss: 0.6656 - accuracy: 0.7920\n",
      "Epoch 00087: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 159s 7ms/sample - loss: 0.6656 - accuracy: 0.7921 - val_loss: 0.5573 - val_accuracy: 0.7984\n",
      "Epoch 88/100\n",
      "23201/23208 [============================>.] - ETA: 0s - loss: 0.6602 - accuracy: 0.7934\n",
      "Epoch 00088: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 159s 7ms/sample - loss: 0.6601 - accuracy: 0.7934 - val_loss: 0.6061 - val_accuracy: 0.8011\n",
      "Epoch 89/100\n",
      "23205/23208 [============================>.] - ETA: 0s - loss: 0.6582 - accuracy: 0.7969\n",
      "Epoch 00089: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 160s 7ms/sample - loss: 0.6581 - accuracy: 0.7968 - val_loss: 0.5556 - val_accuracy: 0.7972\n",
      "Epoch 90/100\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.8094 - accuracy: 0.7908\n",
      "Epoch 00090: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 107s 5ms/sample - loss: 0.8093 - accuracy: 0.7908 - val_loss: 0.5558 - val_accuracy: 0.7918\n",
      "Epoch 91/100\n",
      "23204/23208 [============================>.] - ETA: 0s - loss: 0.6554 - accuracy: 0.7979\n",
      "Epoch 00091: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 72s 3ms/sample - loss: 0.6560 - accuracy: 0.7978 - val_loss: 0.5805 - val_accuracy: 0.8077\n",
      "Epoch 92/100\n",
      "23197/23208 [============================>.] - ETA: 0s - loss: 0.6655 - accuracy: 0.8010\n",
      "Epoch 00092: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 73s 3ms/sample - loss: 0.6653 - accuracy: 0.8010 - val_loss: 0.5774 - val_accuracy: 0.7988\n",
      "Epoch 93/100\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.7066 - accuracy: 0.7961\n",
      "Epoch 00093: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 77s 3ms/sample - loss: 0.7065 - accuracy: 0.7961 - val_loss: 0.6065 - val_accuracy: 0.8038\n",
      "Epoch 94/100\n",
      "23199/23208 [============================>.] - ETA: 0s - loss: 0.8327 - accuracy: 0.7892\n",
      "Epoch 00094: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 73s 3ms/sample - loss: 0.8325 - accuracy: 0.7893 - val_loss: 0.6186 - val_accuracy: 0.7999\n",
      "Epoch 95/100\n",
      "23202/23208 [============================>.] - ETA: 0s - loss: 0.6643 - accuracy: 0.7971\n",
      "Epoch 00095: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 73s 3ms/sample - loss: 0.6643 - accuracy: 0.7971 - val_loss: 0.6209 - val_accuracy: 0.8057\n",
      "Epoch 96/100\n",
      "23192/23208 [============================>.] - ETA: 0s - loss: 0.6569 - accuracy: 0.7969\n",
      "Epoch 00096: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 70s 3ms/sample - loss: 0.6567 - accuracy: 0.7968 - val_loss: 0.5587 - val_accuracy: 0.8069\n",
      "Epoch 97/100\n",
      "23200/23208 [============================>.] - ETA: 0s - loss: 0.6357 - accuracy: 0.7997\n",
      "Epoch 00097: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 76s 3ms/sample - loss: 0.6356 - accuracy: 0.7997 - val_loss: 0.5629 - val_accuracy: 0.7995\n",
      "Epoch 98/100\n",
      "23206/23208 [============================>.] - ETA: 0s - loss: 0.6501 - accuracy: 0.8000\n",
      "Epoch 00098: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 76s 3ms/sample - loss: 0.6501 - accuracy: 0.8000 - val_loss: 0.6153 - val_accuracy: 0.7910\n",
      "Epoch 99/100\n",
      "23207/23208 [============================>.] - ETA: 0s - loss: 0.6658 - accuracy: 0.7972\n",
      "Epoch 00099: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 79s 3ms/sample - loss: 0.6658 - accuracy: 0.7971 - val_loss: 0.6149 - val_accuracy: 0.8030\n",
      "Epoch 100/100\n",
      "23190/23208 [============================>.] - ETA: 0s - loss: 0.6720 - accuracy: 0.8015\n",
      "Epoch 00100: val_accuracy did not improve from 0.81388\n",
      "23208/23208 [==============================] - 72s 3ms/sample - loss: 0.6718 - accuracy: 0.8015 - val_loss: 0.6278 - val_accuracy: 0.8038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1e77c120f98>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Trial 2\n",
    "cd=os.getcwd()\n",
    "# HyperParameters:\n",
    "#Batch_size=1\n",
    "#4 Hidden Layers with (264,128,64,32) neurons\n",
    "#Dropout_Rate: 0.1 applied in three layers\n",
    "#Learning_rate: 0.001\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "filepath=cd\n",
    "checkpoint = ModelCheckpoint(\"weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "# Fit the model\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "model=tf.keras.models.Sequential([ \n",
    "                                   tf.keras.layers.Dense(63,input_shape=(63,),activation='relu'),\n",
    "    tf.keras.layers.Dense(264,activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(128,activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(64,activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "   tf.keras.layers.Dense(32,activation='relu'),\n",
    "                                  \n",
    "                             \n",
    "                             tf.keras.layers.Dense(2,activation='softmax')\n",
    "])\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "y_train_=to_categorical(y_train)\n",
    "# lets train the best model for 100 epochs\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')\n",
    "history=model.fit(X_train,y_train_,epochs=100,validation_split=0.1,callbacks=callbacks_list,batch_size=1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['train','val'])\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['train','val'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 23208 samples, validate on 2579 samples\n",
      "Epoch 1/100\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.4374 - accuracy: 0.7649\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.78907, saving model to weights-improvement-01-0.79.hdf5\n",
      "23208/23208 [==============================] - 4s 166us/sample - loss: 0.4372 - accuracy: 0.7652 - val_loss: 0.3942 - val_accuracy: 0.7891\n",
      "Epoch 2/100\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.4061 - accuracy: 0.7899\n",
      "Epoch 00002: val_accuracy improved from 0.78907 to 0.80070, saving model to weights-improvement-02-0.80.hdf5\n",
      "23208/23208 [==============================] - 3s 136us/sample - loss: 0.4061 - accuracy: 0.7897 - val_loss: 0.3807 - val_accuracy: 0.8007\n",
      "Epoch 3/100\n",
      "22592/23208 [============================>.] - ETA: 0s - loss: 0.3894 - accuracy: 0.7963\n",
      "Epoch 00003: val_accuracy improved from 0.80070 to 0.80496, saving model to weights-improvement-03-0.80.hdf5\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.3898 - accuracy: 0.7964 - val_loss: 0.3776 - val_accuracy: 0.8050\n",
      "Epoch 4/100\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3803 - accuracy: 0.8011\n",
      "Epoch 00004: val_accuracy did not improve from 0.80496\n",
      "23208/23208 [==============================] - 2s 101us/sample - loss: 0.3802 - accuracy: 0.8011 - val_loss: 0.3783 - val_accuracy: 0.8050\n",
      "Epoch 5/100\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3783 - accuracy: 0.8026\n",
      "Epoch 00005: val_accuracy did not improve from 0.80496\n",
      "23208/23208 [==============================] - 3s 120us/sample - loss: 0.3782 - accuracy: 0.8027 - val_loss: 0.3838 - val_accuracy: 0.7991\n",
      "Epoch 6/100\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.3739 - accuracy: 0.8042\n",
      "Epoch 00006: val_accuracy improved from 0.80496 to 0.80574, saving model to weights-improvement-06-0.81.hdf5\n",
      "23208/23208 [==============================] - 3s 115us/sample - loss: 0.3740 - accuracy: 0.8037 - val_loss: 0.3840 - val_accuracy: 0.8057\n",
      "Epoch 7/100\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3723 - accuracy: 0.8051\n",
      "Epoch 00007: val_accuracy improved from 0.80574 to 0.80729, saving model to weights-improvement-07-0.81.hdf5\n",
      "23208/23208 [==============================] - 3s 125us/sample - loss: 0.3726 - accuracy: 0.8051 - val_loss: 0.3750 - val_accuracy: 0.8073\n",
      "Epoch 8/100\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3714 - accuracy: 0.8055\n",
      "Epoch 00008: val_accuracy improved from 0.80729 to 0.80807, saving model to weights-improvement-08-0.81.hdf5\n",
      "23208/23208 [==============================] - 3s 119us/sample - loss: 0.3714 - accuracy: 0.8055 - val_loss: 0.3783 - val_accuracy: 0.8081\n",
      "Epoch 9/100\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.3688 - accuracy: 0.8064\n",
      "Epoch 00009: val_accuracy improved from 0.80807 to 0.81194, saving model to weights-improvement-09-0.81.hdf5\n",
      "23208/23208 [==============================] - 3s 112us/sample - loss: 0.3686 - accuracy: 0.8064 - val_loss: 0.3745 - val_accuracy: 0.8119\n",
      "Epoch 10/100\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.3671 - accuracy: 0.8067\n",
      "Epoch 00010: val_accuracy did not improve from 0.81194\n",
      "23208/23208 [==============================] - 3s 119us/sample - loss: 0.3672 - accuracy: 0.8064 - val_loss: 0.3813 - val_accuracy: 0.8061\n",
      "Epoch 11/100\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.3661 - accuracy: 0.8062\n",
      "Epoch 00011: val_accuracy did not improve from 0.81194\n",
      "23208/23208 [==============================] - 3s 150us/sample - loss: 0.3666 - accuracy: 0.8060 - val_loss: 0.3807 - val_accuracy: 0.8046\n",
      "Epoch 12/100\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3640 - accuracy: 0.8070\n",
      "Epoch 00012: val_accuracy did not improve from 0.81194\n",
      "23208/23208 [==============================] - 3s 148us/sample - loss: 0.3640 - accuracy: 0.8068 - val_loss: 0.3754 - val_accuracy: 0.8069\n",
      "Epoch 13/100\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.3627 - accuracy: 0.8076\n",
      "Epoch 00013: val_accuracy did not improve from 0.81194\n",
      "23208/23208 [==============================] - 3s 116us/sample - loss: 0.3628 - accuracy: 0.8075 - val_loss: 0.3896 - val_accuracy: 0.8042\n",
      "Epoch 14/100\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.3616 - accuracy: 0.8084\n",
      "Epoch 00014: val_accuracy did not improve from 0.81194\n",
      "23208/23208 [==============================] - 2s 98us/sample - loss: 0.3613 - accuracy: 0.8085 - val_loss: 0.3824 - val_accuracy: 0.8050\n",
      "Epoch 15/100\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3620 - accuracy: 0.8086\n",
      "Epoch 00015: val_accuracy did not improve from 0.81194\n",
      "23208/23208 [==============================] - 2s 108us/sample - loss: 0.3620 - accuracy: 0.8084 - val_loss: 0.3810 - val_accuracy: 0.8077\n",
      "Epoch 16/100\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.3583 - accuracy: 0.8102\n",
      "Epoch 00016: val_accuracy did not improve from 0.81194\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.3587 - accuracy: 0.8096 - val_loss: 0.3841 - val_accuracy: 0.8026\n",
      "Epoch 17/100\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3577 - accuracy: 0.8085\n",
      "Epoch 00017: val_accuracy did not improve from 0.81194\n",
      "23208/23208 [==============================] - 3s 112us/sample - loss: 0.3579 - accuracy: 0.8084 - val_loss: 0.3849 - val_accuracy: 0.8057\n",
      "Epoch 18/100\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.3544 - accuracy: 0.8115\n",
      "Epoch 00018: val_accuracy improved from 0.81194 to 0.81582, saving model to weights-improvement-18-0.82.hdf5\n",
      "23208/23208 [==============================] - 3s 108us/sample - loss: 0.3546 - accuracy: 0.8112 - val_loss: 0.3831 - val_accuracy: 0.8158\n",
      "Epoch 19/100\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.3560 - accuracy: 0.8102\n",
      "Epoch 00019: val_accuracy did not improve from 0.81582\n",
      "23208/23208 [==============================] - 2s 97us/sample - loss: 0.3552 - accuracy: 0.8109 - val_loss: 0.3915 - val_accuracy: 0.8077\n",
      "Epoch 20/100\n",
      "22656/23208 [============================>.] - ETA: 0s - loss: 0.3534 - accuracy: 0.8106\n",
      "Epoch 00020: val_accuracy did not improve from 0.81582\n",
      "23208/23208 [==============================] - 2s 92us/sample - loss: 0.3530 - accuracy: 0.8111 - val_loss: 0.3954 - val_accuracy: 0.8038\n",
      "Epoch 21/100\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3543 - accuracy: 0.8087\n",
      "Epoch 00021: val_accuracy did not improve from 0.81582\n",
      "23208/23208 [==============================] - 2s 92us/sample - loss: 0.3536 - accuracy: 0.8095 - val_loss: 0.4062 - val_accuracy: 0.8065\n",
      "Epoch 22/100\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.3516 - accuracy: 0.8118\n",
      "Epoch 00022: val_accuracy did not improve from 0.81582\n",
      "23208/23208 [==============================] - 2s 95us/sample - loss: 0.3522 - accuracy: 0.8114 - val_loss: 0.3966 - val_accuracy: 0.8065\n",
      "Epoch 23/100\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3516 - accuracy: 0.8131\n",
      "Epoch 00023: val_accuracy improved from 0.81582 to 0.81660, saving model to weights-improvement-23-0.82.hdf5\n",
      "23208/23208 [==============================] - 3s 125us/sample - loss: 0.3514 - accuracy: 0.8134 - val_loss: 0.3912 - val_accuracy: 0.8166\n",
      "Epoch 24/100\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.3521 - accuracy: 0.8116\n",
      "Epoch 00024: val_accuracy did not improve from 0.81660\n",
      "23208/23208 [==============================] - 3s 121us/sample - loss: 0.3515 - accuracy: 0.8120 - val_loss: 0.4012 - val_accuracy: 0.8116\n",
      "Epoch 25/100\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.3509 - accuracy: 0.8131\n",
      "Epoch 00025: val_accuracy did not improve from 0.81660\n",
      "23208/23208 [==============================] - 3s 113us/sample - loss: 0.3506 - accuracy: 0.8132 - val_loss: 0.3973 - val_accuracy: 0.8143\n",
      "Epoch 26/100\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.3489 - accuracy: 0.8142\n",
      "Epoch 00026: val_accuracy did not improve from 0.81660\n",
      "23208/23208 [==============================] - 2s 93us/sample - loss: 0.3488 - accuracy: 0.8140 - val_loss: 0.3902 - val_accuracy: 0.8077\n",
      "Epoch 27/100\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.3485 - accuracy: 0.8158\n",
      "Epoch 00027: val_accuracy improved from 0.81660 to 0.81853, saving model to weights-improvement-27-0.82.hdf5\n",
      "23208/23208 [==============================] - 2s 96us/sample - loss: 0.3485 - accuracy: 0.8158 - val_loss: 0.3981 - val_accuracy: 0.8185\n",
      "Epoch 28/100\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3464 - accuracy: 0.8181\n",
      "Epoch 00028: val_accuracy did not improve from 0.81853\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.3463 - accuracy: 0.8182 - val_loss: 0.4221 - val_accuracy: 0.8046\n",
      "Epoch 29/100\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3449 - accuracy: 0.8194\n",
      "Epoch 00029: val_accuracy did not improve from 0.81853\n",
      "23208/23208 [==============================] - 3s 110us/sample - loss: 0.3450 - accuracy: 0.8193 - val_loss: 0.4142 - val_accuracy: 0.8065\n",
      "Epoch 30/100\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3464 - accuracy: 0.8152\n",
      "Epoch 00030: val_accuracy did not improve from 0.81853\n",
      "23208/23208 [==============================] - 3s 118us/sample - loss: 0.3463 - accuracy: 0.8152 - val_loss: 0.4043 - val_accuracy: 0.8015\n",
      "Epoch 31/100\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.3456 - accuracy: 0.8170\n",
      "Epoch 00031: val_accuracy did not improve from 0.81853\n",
      "23208/23208 [==============================] - 3s 113us/sample - loss: 0.3456 - accuracy: 0.8172 - val_loss: 0.4109 - val_accuracy: 0.8116\n",
      "Epoch 32/100\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3451 - accuracy: 0.8173\n",
      "Epoch 00032: val_accuracy did not improve from 0.81853\n",
      "23208/23208 [==============================] - 2s 99us/sample - loss: 0.3451 - accuracy: 0.8173 - val_loss: 0.4076 - val_accuracy: 0.8104\n",
      "Epoch 33/100\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.3439 - accuracy: 0.8172\n",
      "Epoch 00033: val_accuracy did not improve from 0.81853\n",
      "23208/23208 [==============================] - 2s 93us/sample - loss: 0.3440 - accuracy: 0.8174 - val_loss: 0.4236 - val_accuracy: 0.8057\n",
      "Epoch 34/100\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.3443 - accuracy: 0.8180\n",
      "Epoch 00034: val_accuracy did not improve from 0.81853\n",
      "23208/23208 [==============================] - 2s 92us/sample - loss: 0.3443 - accuracy: 0.8183 - val_loss: 0.4060 - val_accuracy: 0.8143\n",
      "Epoch 35/100\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3425 - accuracy: 0.8201\n",
      "Epoch 00035: val_accuracy did not improve from 0.81853\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.3424 - accuracy: 0.8201 - val_loss: 0.4145 - val_accuracy: 0.8123\n",
      "Epoch 36/100\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.3418 - accuracy: 0.8199\n",
      "Epoch 00036: val_accuracy did not improve from 0.81853\n",
      "23208/23208 [==============================] - 3s 120us/sample - loss: 0.3431 - accuracy: 0.8189 - val_loss: 0.4216 - val_accuracy: 0.8042\n",
      "Epoch 37/100\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.3413 - accuracy: 0.8202\n",
      "Epoch 00037: val_accuracy did not improve from 0.81853\n",
      "23208/23208 [==============================] - 3s 122us/sample - loss: 0.3414 - accuracy: 0.8201 - val_loss: 0.4299 - val_accuracy: 0.8061\n",
      "Epoch 38/100\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3392 - accuracy: 0.8174\n",
      "Epoch 00038: val_accuracy did not improve from 0.81853\n",
      "23208/23208 [==============================] - 2s 99us/sample - loss: 0.3396 - accuracy: 0.8173 - val_loss: 0.4247 - val_accuracy: 0.8139\n",
      "Epoch 39/100\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.3406 - accuracy: 0.8205\n",
      "Epoch 00039: val_accuracy did not improve from 0.81853\n",
      "23208/23208 [==============================] - 2s 95us/sample - loss: 0.3407 - accuracy: 0.8204 - val_loss: 0.4292 - val_accuracy: 0.8166\n",
      "Epoch 40/100\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.3408 - accuracy: 0.8202\n",
      "Epoch 00040: val_accuracy did not improve from 0.81853\n",
      "23208/23208 [==============================] - 2s 96us/sample - loss: 0.3408 - accuracy: 0.8202 - val_loss: 0.4336 - val_accuracy: 0.8077\n",
      "Epoch 41/100\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.3422 - accuracy: 0.8213\n",
      "Epoch 00041: val_accuracy did not improve from 0.81853\n",
      "23208/23208 [==============================] - 2s 107us/sample - loss: 0.3415 - accuracy: 0.8219 - val_loss: 0.4411 - val_accuracy: 0.8150\n",
      "Epoch 42/100\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3390 - accuracy: 0.8238\n",
      "Epoch 00042: val_accuracy did not improve from 0.81853\n",
      "23208/23208 [==============================] - 2s 107us/sample - loss: 0.3396 - accuracy: 0.8232 - val_loss: 0.4288 - val_accuracy: 0.8154\n",
      "Epoch 43/100\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.3399 - accuracy: 0.8226\n",
      "Epoch 00043: val_accuracy did not improve from 0.81853\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.3399 - accuracy: 0.8227 - val_loss: 0.4329 - val_accuracy: 0.8174\n",
      "Epoch 44/100\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3383 - accuracy: 0.8221\n",
      "Epoch 00044: val_accuracy improved from 0.81853 to 0.82047, saving model to weights-improvement-44-0.82.hdf5\n",
      "23208/23208 [==============================] - 3s 108us/sample - loss: 0.3383 - accuracy: 0.8220 - val_loss: 0.4394 - val_accuracy: 0.8205\n",
      "Epoch 45/100\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3369 - accuracy: 0.8204\n",
      "Epoch 00045: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 2s 97us/sample - loss: 0.3367 - accuracy: 0.8205 - val_loss: 0.4320 - val_accuracy: 0.8119\n",
      "Epoch 46/100\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.3382 - accuracy: 0.8207\n",
      "Epoch 00046: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.3371 - accuracy: 0.8217 - val_loss: 0.4585 - val_accuracy: 0.8185\n",
      "Epoch 47/100\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3387 - accuracy: 0.8238\n",
      "Epoch 00047: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 2s 92us/sample - loss: 0.3387 - accuracy: 0.8235 - val_loss: 0.4245 - val_accuracy: 0.8166\n",
      "Epoch 48/100\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3372 - accuracy: 0.8214\n",
      "Epoch 00048: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 2s 95us/sample - loss: 0.3369 - accuracy: 0.8216 - val_loss: 0.4533 - val_accuracy: 0.8143\n",
      "Epoch 49/100\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3368 - accuracy: 0.8208\n",
      "Epoch 00049: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 2s 97us/sample - loss: 0.3371 - accuracy: 0.8206 - val_loss: 0.4547 - val_accuracy: 0.8150\n",
      "Epoch 50/100\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.3357 - accuracy: 0.8232\n",
      "Epoch 00050: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 3s 109us/sample - loss: 0.3360 - accuracy: 0.8229 - val_loss: 0.4345 - val_accuracy: 0.8042\n",
      "Epoch 51/100\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.3343 - accuracy: 0.8258\n",
      "Epoch 00051: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 3s 121us/sample - loss: 0.3346 - accuracy: 0.8258 - val_loss: 0.4652 - val_accuracy: 0.8150\n",
      "Epoch 52/100\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.3353 - accuracy: 0.8216\n",
      "Epoch 00052: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 2s 97us/sample - loss: 0.3352 - accuracy: 0.8216 - val_loss: 0.4530 - val_accuracy: 0.8143\n",
      "Epoch 53/100\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3347 - accuracy: 0.8247\n",
      "Epoch 00053: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.3346 - accuracy: 0.8247 - val_loss: 0.4564 - val_accuracy: 0.8147\n",
      "Epoch 54/100\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.3335 - accuracy: 0.8232\n",
      "Epoch 00054: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.3338 - accuracy: 0.8227 - val_loss: 0.4488 - val_accuracy: 0.8205\n",
      "Epoch 55/100\n",
      "22592/23208 [============================>.] - ETA: 0s - loss: 0.3319 - accuracy: 0.8243\n",
      "Epoch 00055: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 2s 96us/sample - loss: 0.3325 - accuracy: 0.8240 - val_loss: 0.4469 - val_accuracy: 0.8189\n",
      "Epoch 56/100\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.3329 - accuracy: 0.8233\n",
      "Epoch 00056: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.3333 - accuracy: 0.8233 - val_loss: 0.4656 - val_accuracy: 0.8162\n",
      "Epoch 57/100\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.3347 - accuracy: 0.8263\n",
      "Epoch 00057: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 2s 105us/sample - loss: 0.3354 - accuracy: 0.8260 - val_loss: 0.4724 - val_accuracy: 0.8201\n",
      "Epoch 58/100\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3345 - accuracy: 0.8220\n",
      "Epoch 00058: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 2s 96us/sample - loss: 0.3347 - accuracy: 0.8219 - val_loss: 0.4685 - val_accuracy: 0.8162\n",
      "Epoch 59/100\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.3304 - accuracy: 0.8254\n",
      "Epoch 00059: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 2s 99us/sample - loss: 0.3304 - accuracy: 0.8252 - val_loss: 0.4828 - val_accuracy: 0.8127\n",
      "Epoch 60/100\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.3330 - accuracy: 0.8245\n",
      "Epoch 00060: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.3330 - accuracy: 0.8246 - val_loss: 0.4661 - val_accuracy: 0.8170\n",
      "Epoch 61/100\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3351 - accuracy: 0.8229\n",
      "Epoch 00061: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 2s 98us/sample - loss: 0.3351 - accuracy: 0.8229 - val_loss: 0.4623 - val_accuracy: 0.8143\n",
      "Epoch 62/100\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.3299 - accuracy: 0.8259\n",
      "Epoch 00062: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 2s 97us/sample - loss: 0.3300 - accuracy: 0.8259 - val_loss: 0.4709 - val_accuracy: 0.8154\n",
      "Epoch 63/100\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3297 - accuracy: 0.8280\n",
      "Epoch 00063: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 2s 95us/sample - loss: 0.3297 - accuracy: 0.8280 - val_loss: 0.4607 - val_accuracy: 0.8185\n",
      "Epoch 64/100\n",
      "22656/23208 [============================>.] - ETA: 0s - loss: 0.3304 - accuracy: 0.8245\n",
      "Epoch 00064: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 2s 94us/sample - loss: 0.3299 - accuracy: 0.8241 - val_loss: 0.4902 - val_accuracy: 0.8147\n",
      "Epoch 65/100\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3318 - accuracy: 0.8252\n",
      "Epoch 00065: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 2s 95us/sample - loss: 0.3316 - accuracy: 0.8253 - val_loss: 0.4657 - val_accuracy: 0.8147\n",
      "Epoch 66/100\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.3290 - accuracy: 0.8251\n",
      "Epoch 00066: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 2s 102us/sample - loss: 0.3297 - accuracy: 0.8245 - val_loss: 0.4691 - val_accuracy: 0.8143\n",
      "Epoch 67/100\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.3315 - accuracy: 0.8260\n",
      "Epoch 00067: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 3s 114us/sample - loss: 0.3313 - accuracy: 0.8260 - val_loss: 0.4665 - val_accuracy: 0.8174\n",
      "Epoch 68/100\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3292 - accuracy: 0.8249\n",
      "Epoch 00068: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 2s 98us/sample - loss: 0.3291 - accuracy: 0.8247 - val_loss: 0.4711 - val_accuracy: 0.8170\n",
      "Epoch 69/100\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.3293 - accuracy: 0.8258\n",
      "Epoch 00069: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 3s 125us/sample - loss: 0.3295 - accuracy: 0.8254 - val_loss: 0.4801 - val_accuracy: 0.8096\n",
      "Epoch 70/100\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.3277 - accuracy: 0.8283\n",
      "Epoch 00070: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 3s 142us/sample - loss: 0.3280 - accuracy: 0.8282 - val_loss: 0.4895 - val_accuracy: 0.8116\n",
      "Epoch 71/100\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.3282 - accuracy: 0.8271\n",
      "Epoch 00071: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 3s 121us/sample - loss: 0.3284 - accuracy: 0.8270 - val_loss: 0.4875 - val_accuracy: 0.7991\n",
      "Epoch 72/100\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3272 - accuracy: 0.8248\n",
      "Epoch 00072: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 2s 107us/sample - loss: 0.3271 - accuracy: 0.8251 - val_loss: 0.4787 - val_accuracy: 0.8154\n",
      "Epoch 73/100\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.3264 - accuracy: 0.8276\n",
      "Epoch 00073: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.3265 - accuracy: 0.8276 - val_loss: 0.4734 - val_accuracy: 0.8123\n",
      "Epoch 74/100\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3278 - accuracy: 0.8256\n",
      "Epoch 00074: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 2s 96us/sample - loss: 0.3273 - accuracy: 0.8259 - val_loss: 0.4739 - val_accuracy: 0.8158\n",
      "Epoch 75/100\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.3268 - accuracy: 0.8272\n",
      "Epoch 00075: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.3268 - accuracy: 0.8276 - val_loss: 0.4862 - val_accuracy: 0.8061\n",
      "Epoch 76/100\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3266 - accuracy: 0.8262\n",
      "Epoch 00076: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 3s 109us/sample - loss: 0.3268 - accuracy: 0.8263 - val_loss: 0.4872 - val_accuracy: 0.8135\n",
      "Epoch 77/100\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3247 - accuracy: 0.8292\n",
      "Epoch 00077: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 3s 137us/sample - loss: 0.3246 - accuracy: 0.8290 - val_loss: 0.4951 - val_accuracy: 0.8096\n",
      "Epoch 78/100\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.3261 - accuracy: 0.8295\n",
      "Epoch 00078: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 3s 133us/sample - loss: 0.3261 - accuracy: 0.8294 - val_loss: 0.4903 - val_accuracy: 0.8042\n",
      "Epoch 79/100\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.3264 - accuracy: 0.8270\n",
      "Epoch 00079: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 3s 138us/sample - loss: 0.3264 - accuracy: 0.8269 - val_loss: 0.4847 - val_accuracy: 0.8166\n",
      "Epoch 80/100\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3244 - accuracy: 0.8280\n",
      "Epoch 00080: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 3s 130us/sample - loss: 0.3244 - accuracy: 0.8280 - val_loss: 0.4933 - val_accuracy: 0.8092\n",
      "Epoch 81/100\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3240 - accuracy: 0.8273\n",
      "Epoch 00081: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 3s 149us/sample - loss: 0.3239 - accuracy: 0.8273 - val_loss: 0.4995 - val_accuracy: 0.8081\n",
      "Epoch 82/100\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3245 - accuracy: 0.8280\n",
      "Epoch 00082: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 3s 130us/sample - loss: 0.3245 - accuracy: 0.8279 - val_loss: 0.4828 - val_accuracy: 0.8061\n",
      "Epoch 83/100\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.3250 - accuracy: 0.8259\n",
      "Epoch 00083: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.3247 - accuracy: 0.8260 - val_loss: 0.4902 - val_accuracy: 0.8147\n",
      "Epoch 84/100\n",
      "22592/23208 [============================>.] - ETA: 0s - loss: 0.3246 - accuracy: 0.8287\n",
      "Epoch 00084: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 3s 108us/sample - loss: 0.3241 - accuracy: 0.8290 - val_loss: 0.4764 - val_accuracy: 0.8135\n",
      "Epoch 85/100\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3216 - accuracy: 0.8293\n",
      "Epoch 00085: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 2s 104us/sample - loss: 0.3218 - accuracy: 0.8291 - val_loss: 0.5023 - val_accuracy: 0.8061\n",
      "Epoch 86/100\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3238 - accuracy: 0.8297\n",
      "Epoch 00086: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.3239 - accuracy: 0.8298 - val_loss: 0.4895 - val_accuracy: 0.8108\n",
      "Epoch 87/100\n",
      "22976/23208 [============================>.] - ETA: 0s - loss: 0.3252 - accuracy: 0.8266\n",
      "Epoch 00087: val_accuracy did not improve from 0.82047\n",
      "23208/23208 [==============================] - 2s 92us/sample - loss: 0.3244 - accuracy: 0.8272 - val_loss: 0.4906 - val_accuracy: 0.8127\n",
      "Epoch 88/100\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.3218 - accuracy: 0.8289\n",
      "Epoch 00088: val_accuracy improved from 0.82047 to 0.82202, saving model to weights-improvement-88-0.82.hdf5\n",
      "23208/23208 [==============================] - 3s 111us/sample - loss: 0.3221 - accuracy: 0.8286 - val_loss: 0.5025 - val_accuracy: 0.8220\n",
      "Epoch 89/100\n",
      "23104/23208 [============================>.] - ETA: 0s - loss: 0.3228 - accuracy: 0.8290\n",
      "Epoch 00089: val_accuracy did not improve from 0.82202\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.3229 - accuracy: 0.8290 - val_loss: 0.5076 - val_accuracy: 0.8119\n",
      "Epoch 90/100\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.3221 - accuracy: 0.8273\n",
      "Epoch 00090: val_accuracy did not improve from 0.82202\n",
      "23208/23208 [==============================] - 2s 97us/sample - loss: 0.3222 - accuracy: 0.8274 - val_loss: 0.4947 - val_accuracy: 0.8181\n",
      "Epoch 91/100\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3218 - accuracy: 0.8280\n",
      "Epoch 00091: val_accuracy did not improve from 0.82202\n",
      "23208/23208 [==============================] - 3s 110us/sample - loss: 0.3217 - accuracy: 0.8280 - val_loss: 0.5223 - val_accuracy: 0.8150\n",
      "Epoch 92/100\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.3233 - accuracy: 0.8279\n",
      "Epoch 00092: val_accuracy did not improve from 0.82202\n",
      "23208/23208 [==============================] - 3s 118us/sample - loss: 0.3231 - accuracy: 0.8280 - val_loss: 0.4972 - val_accuracy: 0.8166\n",
      "Epoch 93/100\n",
      "23040/23208 [============================>.] - ETA: 0s - loss: 0.3223 - accuracy: 0.8276\n",
      "Epoch 00093: val_accuracy did not improve from 0.82202\n",
      "23208/23208 [==============================] - 2s 95us/sample - loss: 0.3217 - accuracy: 0.8282 - val_loss: 0.4970 - val_accuracy: 0.8162\n",
      "Epoch 94/100\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.3210 - accuracy: 0.8284\n",
      "Epoch 00094: val_accuracy did not improve from 0.82202\n",
      "23208/23208 [==============================] - 2s 100us/sample - loss: 0.3211 - accuracy: 0.8283 - val_loss: 0.5110 - val_accuracy: 0.8205\n",
      "Epoch 95/100\n",
      "23168/23208 [============================>.] - ETA: 0s - loss: 0.3214 - accuracy: 0.8299\n",
      "Epoch 00095: val_accuracy did not improve from 0.82202\n",
      "23208/23208 [==============================] - 2s 106us/sample - loss: 0.3215 - accuracy: 0.8298 - val_loss: 0.5102 - val_accuracy: 0.8147\n",
      "Epoch 96/100\n",
      "22848/23208 [============================>.] - ETA: 0s - loss: 0.3207 - accuracy: 0.8270\n",
      "Epoch 00096: val_accuracy did not improve from 0.82202\n",
      "23208/23208 [==============================] - 3s 116us/sample - loss: 0.3206 - accuracy: 0.8273 - val_loss: 0.5015 - val_accuracy: 0.8158\n",
      "Epoch 97/100\n",
      "22784/23208 [============================>.] - ETA: 0s - loss: 0.3196 - accuracy: 0.8292\n",
      "Epoch 00097: val_accuracy did not improve from 0.82202\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.3205 - accuracy: 0.8288 - val_loss: 0.5134 - val_accuracy: 0.8112\n",
      "Epoch 98/100\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3188 - accuracy: 0.8277\n",
      "Epoch 00098: val_accuracy did not improve from 0.82202\n",
      "23208/23208 [==============================] - 2s 103us/sample - loss: 0.3189 - accuracy: 0.8277 - val_loss: 0.5111 - val_accuracy: 0.8158\n",
      "Epoch 99/100\n",
      "22912/23208 [============================>.] - ETA: 0s - loss: 0.3208 - accuracy: 0.8273\n",
      "Epoch 00099: val_accuracy did not improve from 0.82202\n",
      "23208/23208 [==============================] - 3s 108us/sample - loss: 0.3209 - accuracy: 0.8271 - val_loss: 0.5172 - val_accuracy: 0.8050\n",
      "Epoch 100/100\n",
      "22720/23208 [============================>.] - ETA: 0s - loss: 0.3217 - accuracy: 0.8291\n",
      "Epoch 00100: val_accuracy did not improve from 0.82202\n",
      "23208/23208 [==============================] - 3s 115us/sample - loss: 0.3212 - accuracy: 0.8296 - val_loss: 0.5106 - val_accuracy: 0.8147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1e77acddc88>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Trial 3\n",
    "cd=os.getcwd()\n",
    "# HyperParameters:\n",
    "#Batch_size=64\n",
    "#4 Hidden Layers with (264,128,64,32) neurons\n",
    "#Dropout_Rate: 0.1 applied in three layers\n",
    "#Learning_rate: 0.001\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "filepath=cd\n",
    "checkpoint = ModelCheckpoint(\"weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "# Fit the model\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "model=tf.keras.models.Sequential([ \n",
    "                                   tf.keras.layers.Dense(63,input_shape=(63,),activation='relu'),\n",
    "    tf.keras.layers.Dense(264,activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(128,activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(64,activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "   tf.keras.layers.Dense(32,activation='relu'),\n",
    "                                  \n",
    "                             \n",
    "                             tf.keras.layers.Dense(2,activation='softmax')\n",
    "])\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "y_train_=to_categorical(y_train)\n",
    "# lets train the best model for 100 epochs\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')\n",
    "history=model.fit(X_train,y_train_,epochs=100,validation_split=0.1,callbacks=callbacks_list,batch_size=64)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['train','val'])\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['train','val'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
